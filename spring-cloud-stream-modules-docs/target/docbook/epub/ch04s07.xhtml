<?xml version="1.0" encoding="UTF-8" standalone="no"?><!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis" xmlns:svg="http://www.w3.org/2000/svg"><head><title>Hadoop (HDFS) (hdfs)</title><link rel="stylesheet" type="text/css" href="docbook-epub.css"/><meta name="generator" content="DocBook XSL Stylesheets V1.78.1"/><link rel="prev" href="ch04s06.xhtml" title="Gemfire (gemfire)"/><link rel="next" href="ch04s08.xhtml" title="JDBC (jdbc)"/></head><body><header/><section class="section" title="Hadoop (HDFS) (hdfs)" epub:type="subchapter" id="spring-cloud-stream-modules-hdfs"><div class="titlepage"><div><div><h2 class="title" style="clear: both">Hadoop (HDFS) (<code class="literal">hdfs</code>)</h2></div></div></div><p>If you do not have Hadoop installed, you can install Hadoop as described in our <a class="link" href="Hadoop-Installation.xml#installing-hadoop" target="_top">separate guide</a>.</p><p>Once Hadoop is up and running, you can then use the <code class="literal">hdfs</code> sink when creating a <a class="link" href="Streams.xml#streams" target="_top">stream</a></p><pre class="literallayout">dataflow:&gt; stream create --name myhdfsstream1 --definition "time | hdfs" --deploy</pre><p>In the above example, we’ve scheduled <code class="literal">time</code> source to automatically send ticks to <code class="literal">hdfs</code> once in every second. If you wait a little while for data to accumuluate you can then list can then list the files in the hadoop filesystem using the shell’s built in hadoop fs commands.  Before making any access to HDFS in the shell you first need to configure the shell to point to your name node.  This is done using the <code class="literal">hadoop config</code> command.</p><pre class="literallayout">dataflow:&gt;hadoop config fs --namenode hdfs://localhost:8020</pre><p>In this example the hdfs protocol is used but you may also use the webhdfs protocol.  Listing the contents in the output directory (named by default after the stream name) is done by issuing the following command.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream1
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup          0 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt.tmp</pre><p>While the file is being written to it will have the <code class="literal">tmp</code> suffix.  When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the <code class="literal">tmp</code> suffix.  There are several options to control the in use file file naming options.  These are <code class="literal">--inUsePrefix</code> and <code class="literal">--inUseSuffix</code> set the file name prefix and suffix respectfully.</p><p>When you destroy a stream</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream1</pre><p>and list the stream directory again, in use file suffix doesn’t exist anymore.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream1
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup        380 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt</pre><p>To list the list the contents of a file directly from a shell execute the hadoop cat command.</p><pre class="literallayout">dataflow:&gt; hadoop fs cat /xd/myhdfsstream1/myhdfsstream1-0.txt
2013-12-18 18:10:07
2013-12-18 18:10:08
2013-12-18 18:10:09
...</pre><p>In the above examples we didn’t yet go through why the file was written in a specific directory and why it was named in this specific way. Default location of a file is defined as <code class="literal">/xd/&lt;stream name&gt;/&lt;stream name&gt;-&lt;rolling part&gt;.txt</code>. These can be changed using options <code class="literal">--directory</code> and <code class="literal">--fileName</code> respectively. Example is shown below.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream2 --definition "time | hdfs --directory=/xd/tmp --fileName=data" --deploy
dataflow:&gt;stream destroy --name myhdfsstream2
dataflow:&gt;hadoop fs ls /xd/tmp
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup        120 2013-12-18 18:31 /xd/tmp/data-0.txt</pre><p>It is also possible to control the size of a files written into HDFS. The <code class="literal">--rollover</code> option can be used to control when file currently being written is rolled over and a new file opened by providing the rollover size in bytes, kilobytes, megatypes, gigabytes, and terabytes.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream3 --definition "time | hdfs --rollover=100" --deploy
dataflow:&gt;stream destroy --name myhdfsstream3
dataflow:&gt;hadoop fs ls /xd/myhdfsstream3
Found 3 items
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-0.txt
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-1.txt
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-2.txt</pre><p>Shortcuts to specify sizes other than bytes are written as <code class="literal">--rollover=64M</code>, <code class="literal">--rollover=512G</code> or <code class="literal">--rollover=1T</code>.</p><p>The stream can also be compressed during the write operation. Example of this is shown below.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream4 --definition "time | hdfs --codec=gzip" --deploy
dataflow:&gt;stream destroy --name myhdfsstream4
dataflow:&gt;hadoop fs ls /xd/myhdfsstream4
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup         80 2013-12-18 18:48 /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip</pre><p>From a native os shell we can use hadoop’s fs commands and pipe data into gunzip.</p><pre class="literallayout"># bin/hadoop fs -cat /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip | gunzip
2013-12-18 18:48:10
2013-12-18 18:48:11
...</pre><p>Often a stream of data may not have a high enough rate to roll over files frequently, leaving the file in an opened state.  This prevents users from reading a consistent set of data when running mapreduce jobs.  While one can alleviate this problem by using a small rollover value, a better way is to use the <code class="literal">idleTimeout</code>  option that will automatically close the file if there was no writes during the specified period of time.   This feature is also useful in cases where burst of data is written into a stream and you’d like that data to become visible in HDFS.</p><div class="note" title="Note" epub:type="notice"><h3 class="title">Note</h3><p>The <code class="literal">idleTimeout</code> value should not exceed the timeout values set on the Hadoop cluster. These are typically configured using the <code class="literal">dfs.socket.timeout</code> and/or <code class="literal">dfs.datanode.socket.write.timeout</code> properties in the <code class="literal">hdfs-site.xml</code> configuration file.</p></div><pre class="literallayout">dataflow:&gt; stream create --name myhdfsstream5 --definition "http --server.port=8000 | hdfs --rollover=20 --idleTimeout=10000" --deploy</pre><p>In the above example we changed a source to <code class="literal">http</code> order to control what we write into a <code class="literal">hdfs</code> sink. We defined a small rollover size and a timeout of 10 seconds. Now we can simply post data into this stream via source end point using a below command.</p><pre class="literallayout">dataflow:&gt; http post --target http://localhost:8000 --data "hello"</pre><p>If we repeat the command very quickly and then wait for the timeout we should be able to see that some files are closed before rollover size was met and some were simply rolled because of a rollover size.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream5
Found 4 items
-rw-r--r--   3 jvalkealahti supergroup         12 2013-12-18 19:02 /xd/myhdfsstream5/myhdfsstream5-0.txt
-rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-1.txt
-rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-2.txt
-rw-r--r--   3 jvalkealahti supergroup         18 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-3.txt</pre><p>Files can be automatically partitioned using a <code class="literal">partitionPath</code> expression. If we create a stream with <code class="literal">idleTimeout</code> and <code class="literal">partitionPath</code> with simple format <code class="literal">yyyy/MM/dd/HH/mm</code> we should see writes ending into its own files within every minute boundary.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream6 --definition "time|hdfs --idleTimeout=10000 --partitionPath=dateFormat('yyyy/MM/dd/HH/mm')" --deploy</pre><p>Let a stream run for a short period of time and list files.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls --recursive true --dir /xd/myhdfsstream6
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42
-rw-r--r--   3 jvalkealahti supergroup        140 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42/myhdfsstream6-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43
-rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43/myhdfsstream6-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44
-rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44/myhdfsstream6-0.txt</pre><p>Partitioning can also be based on defined lists. In a below example we simulate feeding data by using a <code class="literal">time</code> and a <code class="literal">transform</code> elements. Data passed to <code class="literal">hdfs</code> sink has a content <code class="literal">APP0:foobar</code>, <code class="literal">APP1:foobar</code>, <code class="literal">APP2:foobar</code> or <code class="literal">APP3:foobar</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream7 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*3)+':foobar'\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),list(payload.split(':')[0],{{'0TO1','APP0','APP1'},{'2TO3','APP2','APP3'}}))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream7
Destroyed stream 'myhdfsstream7'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list
-rw-r--r--   3 jvalkealahti supergroup        108 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list
-rw-r--r--   3 jvalkealahti supergroup        180 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list/myhdfsstream7-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
APP1:foobar
APP1:foobar
APP0:foobar
APP0:foobar
APP1:foobar</pre><p>Partitioning can also be based on defined ranges. In a below example we simulate feeding data by using a <code class="literal">time</code> and a <code class="literal">transform</code> elements. Data passed to <code class="literal">hdfs</code> sink has a content ranging from <code class="literal">APP0</code> to <code class="literal">APP15</code>. We simple parse the number part and use it to do a partition with ranges <code class="literal">{3,5,10}</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream8 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*15)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),range(T(Integer).parseInt(payload.substring(3)),{3,5,10}))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream8
Destroyed stream 'myhdfsstream8'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range
-rw-r--r--   3 jvalkealahti supergroup         16 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range
-rw-r--r--   3 jvalkealahti supergroup         35 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range
-rw-r--r--   3 jvalkealahti supergroup          5 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
APP3
APP3
APP1
APP0
APP1
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
APP4
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
APP6
APP15
APP7</pre><p>Partition using a <code class="literal">dateFormat</code> can be based on content itself. This is a good use case if old log files needs to be processed where partitioning should happen based on timestamp of a log entry. We create a fake log data with a simple date string ranging from <code class="literal">1970-01-10</code> to <code class="literal">1970-01-13</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream9 --definition "time | transform --expression=\"'1970-01-'+1+T(Math).round(T(Math).random()*3)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH',payload,'yyyy-MM-DD'))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files. If you see the partition paths, those are based on year 1970, not present year.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream9
Destroyed stream 'myhdfsstream9'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/10
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00
-rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/11
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00
-rw-r--r--   3 jvalkealahti supergroup         99 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/12
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00
-rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/13
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00
-rw-r--r--   3 jvalkealahti supergroup         55 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00/myhdfsstream9-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
1970-01-10
1970-01-10
1970-01-10
1970-01-10</pre><section class="section" title="Options" epub:type="division" id="_options_21"><div class="titlepage"><div><div><h3 class="title">Options</h3></div></div></div><p>The <span class="strong"><strong>hdfs</strong></span> sink has the following options:</p><div class="variablelist" epub:type="list"><dl class="variablelist"><dt><span class="term">closeTimeout</span></dt><dd>timeout in ms, regardless of activity, after which file will be automatically closed <span class="strong"><strong>(long, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">codec</span></dt><dd>compression codec alias name (gzip, snappy, bzip2, lzo, or slzo) <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">directory</span></dt><dd>where to output the files in the Hadoop FileSystem <span class="strong"><strong>(String, default: <code class="literal">/tmp/hdfs-sink</code>)</strong></span></dd><dt><span class="term">fileExtension</span></dt><dd>the base filename extension to use for the created files <span class="strong"><strong>(String, default: <code class="literal">txt</code>)</strong></span></dd><dt><span class="term">fileName</span></dt><dd>the base filename to use for the created files <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">fileOpenAttempts</span></dt><dd>maximum number of file open attempts to find a path <span class="strong"><strong>(int, default: <code class="literal">10</code>)</strong></span></dd><dt><span class="term">fileUuid</span></dt><dd>whether file name should contain uuid <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">fsUri</span></dt><dd>the URI to use to access the Hadoop FileSystem <span class="strong"><strong>(String, default: <code class="literal">${spring.hadoop.fsUri}</code>)</strong></span></dd><dt><span class="term">idleTimeout</span></dt><dd>inactivity timeout in ms after which file will be automatically closed <span class="strong"><strong>(long, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">inUsePrefix</span></dt><dd>prefix for files currently being written <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">inUseSuffix</span></dt><dd>suffix for files currently being written <span class="strong"><strong>(String, default: <code class="literal">.tmp</code>)</strong></span></dd><dt><span class="term">overwrite</span></dt><dd>whether writer is allowed to overwrite files in Hadoop FileSystem <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">partitionPath</span></dt><dd>a SpEL expression defining the partition path <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">rollover</span></dt><dd>threshold in bytes when file will be automatically rolled over <span class="strong"><strong>(String, default: <code class="literal">1G</code>)</strong></span></dd></dl></div><div class="note" title="Note" epub:type="notice"><h3 class="title">Note</h3><p>In the context of the <code class="literal">fileOpenAttempts</code> option, attempt is either one rollover request or failed stream open request for a path (if another writer came up with a same path and already opened it).</p></div></section><section class="section" title="Partition Path Expression" epub:type="division" id="_partition_path_expression"><div class="titlepage"><div><div><h3 class="title">Partition Path Expression</h3></div></div></div><p>SpEL expression is evaluated against a Spring Messaging <code class="literal">Message</code> passed internally into a HDFS writer. This allows expression to use <code class="literal">headers</code> and <code class="literal">payload</code> from that message. While you could do a custom processing within a stream and add custom headers, <code class="literal">timestamp</code> is always going to be there. Data to be written is then available in a <code class="literal">payload</code>.</p><section class="section" title="Accessing Properties" epub:type="division" id="_accessing_properties"><div class="titlepage"><div><div><h4 class="title">Accessing Properties</h4></div></div></div><p>Using a <code class="literal">payload</code> simply returns whatever is currently being written. Access to headers is via <code class="literal">headers</code> property. Any other property is automatically resolved from headers if found. For example <code class="literal">headers.timestamp</code> is equivalent to <code class="literal">timestamp</code>.</p></section><section class="section" title="Custom Methods" epub:type="division" id="_custom_methods"><div class="titlepage"><div><div><h4 class="title">Custom Methods</h4></div></div></div><p>Addition to a normal SpEL functionality, few custom methods has been added to make it easier to build partition paths. These custom methods can be used to work with a normal partition concepts like <code class="literal">date formatting</code>, <code class="literal">lists</code>, <code class="literal">ranges</code> and <code class="literal">hashes</code>.</p><section class="section" title="path" epub:type="division" id="_path"><div class="titlepage"><div><div><h5 class="title">path</h5></div></div></div><pre class="programlisting">path(String... paths)</pre><p>Concatenates paths together with a delimiter <code class="literal">/</code>. This method can be used to make the expression less verbose than using a native SpEL functionality to combine path parts together. To create a path <code class="literal">part1/part2</code>, expression <code class="literal">'part1' + '/' + 'part2'</code> is equivalent to <code class="literal">path('part1','part2')</code>.</p><div class="variablelist" title="Parameters" epub:type="list"><div class="variablelist-title">Parameters</div><dl class="variablelist"><dt><span class="term">paths</span></dt><dd>Any number of path parts</dd></dl></div><p title="Return Value"><span class="formalpara-title">Return Value. </span>Concatenated value of paths delimited with <code class="literal">/</code>.</p></section><section class="section" title="dateFormat" epub:type="division" id="_dateformat"><div class="titlepage"><div><div><h5 class="title">dateFormat</h5></div></div></div><pre class="programlisting">dateFormat(String pattern)
dateFormat(String pattern, Long epoch)
dateFormat(String pattern, Date date)
dateFormat(String pattern, String datestring)
dateFormat(String pattern, String datestring, String dateformat)</pre><p>Creates a path using date formatting. Internally this method delegates into <code class="literal">SimpleDateFormat</code> and needs a <code class="literal">Date</code> and a <code class="literal">pattern</code>. On default if no parameter used for conversion is given, <code class="literal">timestamp</code> is expected. Effectively <code class="literal">dateFormat('yyyy')</code> equals to <code class="literal">dateFormat('yyyy', timestamp)</code> or <code class="literal">dateFormat('yyyy', headers.timestamp)</code>.</p><p>Method signature with three parameters can be used to create a custom <code class="literal">Date</code> object which is then passed to <code class="literal">SimpleDateFormat</code> conversion using a <code class="literal">dateformat</code> pattern. This is useful in use cases where partition should be based on a date or time string found from a payload content itself. Default <code class="literal">dateformat</code> pattern if omitted is <code class="literal">yyyy-MM-dd</code>.</p><div class="variablelist" title="Parameters" epub:type="list"><div class="variablelist-title">Parameters</div><dl class="variablelist"><dt><span class="term">pattern</span></dt><dd>Pattern compatible with <code class="literal">SimpleDateFormat</code> to produce a final output.</dd><dt><span class="term">epoch</span></dt><dd>Timestamp as <code class="literal">Long</code> which is converted into a <code class="literal">Date</code>.</dd><dt><span class="term">date</span></dt><dd>A <code class="literal">Date</code> to be formatted.</dd><dt><span class="term">dateformat</span></dt><dd>Secondary pattern to convert <code class="literal">datestring</code> into a <code class="literal">Date</code>.</dd><dt><span class="term">datestring</span></dt><dd><code class="literal">Date</code> as a <code class="literal">String</code></dd></dl></div><p title="Return Value"><span class="formalpara-title">Return Value. </span>A path part representation which can be a simple file or directory name or a directory structure.</p></section><section class="section" title="list" epub:type="division" id="_list"><div class="titlepage"><div><div><h5 class="title">list</h5></div></div></div><pre class="programlisting">list(Object source, List&lt;List&lt;Object&gt;&gt; lists)</pre><p>Creates a partition path part by matching a <code class="literal">source</code> against a lists denoted by <code class="literal">lists</code>.</p><p>Lets assume that data is being written and it’s possible to extrace an <code class="literal">appid</code> either from headers or payload. We can automatically do a list based partition by using a partition method <code class="literal">list(headers.appid,{{'1TO3','APP1','APP2','APP3'},{'4TO6','APP4','APP5','APP6'}})</code>. This method would create three partitions, <code class="literal">1TO3_list</code>, <code class="literal">4TO6_list</code> and <code class="literal">list</code>. Latter is used if no match is found from partition lists passed to <code class="literal">lists</code>.</p><div class="variablelist" title="Parameters" epub:type="list"><div class="variablelist-title">Parameters</div><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> to be matched against <code class="literal">lists</code>.</dd><dt><span class="term">lists</span></dt><dd>A definition of list of lists.</dd></dl></div><p title="Return Value"><span class="formalpara-title">Return Value. </span>A path part prefixed with a matched key i.e. <code class="literal">XXX_list</code> or <code class="literal">list</code> if no match.</p></section><section class="section" title="range" epub:type="division" id="_range"><div class="titlepage"><div><div><h5 class="title">range</h5></div></div></div><pre class="programlisting">range(Object source, List&lt;Object&gt; list)</pre><p>Creates a partition path part by matching a <code class="literal">source</code> against a list denoted by <code class="literal">list</code> using a simple binary search.</p><p>The partition method takes a <code class="literal">source</code> as first argument and <code class="literal">list</code> as a second argument. Behind the scenes this is using jvm’s <code class="literal">binarySearch</code> which works on an <code class="literal">Object</code> level so we can pass in anything. Remember that meaningful range match only works if passed in <code class="literal">Object</code> and types in list are of same type like <code class="literal">Integer</code>. Range is defined by a binarySearch itself so mostly it is to match against an upper bound except the last range in a list. Having a list of <code class="literal">{1000,3000,5000}</code> means that everything above 3000 will be matched with 5000. If that is an issue then simply adding <code class="literal">Integer.MAX_VALUE</code> as last range would overflow everything above 5000 into a new partition. Created partitions would then be <code class="literal">1000_range</code>, <code class="literal">3000_range</code> and <code class="literal">5000_range</code>.</p><div class="variablelist" title="Parameters" epub:type="list"><div class="variablelist-title">Parameters</div><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> to be matched against <code class="literal">list</code>.</dd><dt><span class="term">list</span></dt><dd>A definition of list.</dd></dl></div><p title="Return Value"><span class="formalpara-title">Return Value. </span>A path part prefixed with a matched key i.e. <code class="literal">XXX_range</code>.</p></section><section class="section" title="hash" epub:type="division" id="_hash"><div class="titlepage"><div><div><h5 class="title">hash</h5></div></div></div><pre class="programlisting">hash(Object source, int bucketcount)</pre><p>Creates a partition path part by calculating hashkey using <code class="literal">source`s</code> <code class="literal">hashCode</code> and <code class="literal">bucketcount</code>. Using a partition method <code class="literal">hash(timestamp,2)</code> would then create partitions named <code class="literal">0_hash</code>, <code class="literal">1_hash</code> and <code class="literal">2_hash</code>. Number suffixed with <code class="literal">_hash</code> is simply calculated using <code class="literal">Object.hashCode() % bucketcount</code>.</p><div class="variablelist" title="Parameters" epub:type="list"><div class="variablelist-title">Parameters</div><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> which <code class="literal">hashCode</code> will be used.</dd><dt><span class="term">bucketcount</span></dt><dd>A number of buckets</dd></dl></div><p title="Return Value"><span class="formalpara-title">Return Value. </span>A path part prefixed with a hash key i.e. <code class="literal">XXX_hash</code>.</p></section></section></section></section><footer/></body></html>