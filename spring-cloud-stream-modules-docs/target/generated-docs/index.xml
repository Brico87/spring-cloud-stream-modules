<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc maxdepth="4"?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Spring Cloud Stream Reference Guide</title>
<date>2016-03-30</date>
<author>
<personname>
<firstname>Sabby Anandan, Artem Bilan, Marius Bogoevici, Eric Bottard, Mark Fisher, Ilayaperumal Gopinathan, Gunnar Hillert, Mark Pollack, Patrick Peralta, Glenn Renfro, Gary Russell, Thomas Risberg, David Turanski, Janne Valkealahti</firstname>
</personname>
</author>
<authorinitials>S</authorinitials>
<productname>Spring Cloud Data Flow</productname>
<releaseinfo>1.0.0.BUILD-SNAPSHOT</releaseinfo>
<copyright>
	<year>2013-2016</year>
	<holder>Pivotal Software, Inc.</holder>
</copyright>
<legalnotice>
	<para>
		Copies of this document may be made for your own use and for distribution to
		others, provided that you do not charge any fee for such copies and further
		provided that each copy contains this Copyright Notice, whether distributed in
		print or electronically.
	</para>
</legalnotice>
</info>
<part xml:id="_reference_guide">
<title>Reference Guide</title>
<chapter xml:id="overview">
<title>Spring Cloud Stream Modules</title>
<simpara>This section goes into more detail about how you can work with Spring Cloud Stream Module as standalone applications or with Spring Cloud Data Flow.</simpara>
<section xml:id="_overview">
<title>Overview</title>
<simpara>TBD</simpara>
</section>
</chapter>
</part>
<part xml:id="_modules">
<title>Modules</title>
<chapter xml:id="sources">
<title>Sources</title>
<section xml:id="spring-cloud-stream-modules-file-source">
<title>File Source</title>
<simpara>This application polls a directory and sends new files or their contents to the output channel.
The file source provides the contents of a File as a byte array by default.
However, this can be customized using the <literal>--mode</literal> option:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">ref</emphasis> Provides a <literal>java.io.File</literal> reference</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">lines</emphasis> Will split files line-by-line and emit a new message for each line</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">contents</emphasis> The default. Provides the contents of a file as a byte array</simpara>
</listitem>
</itemizedlist>
<simpara>When using <literal>--mode=lines</literal>, you can also provide the additional option <literal>--withMarkers=true</literal>.
If set to <literal>true</literal>, the underlying <literal>FileSplitter</literal> will emit additional <emphasis>start-of-file</emphasis> and <emphasis>end-of-file</emphasis> marker messages before and after the actual data.
The payload of these 2 additional marker messages is of type <literal>FileSplitter.FileMarker</literal>. The option <literal>withMarkers</literal> defaults to <literal>false</literal> if not explicitly set.</simpara>
<section xml:id="_options">
<title>Options</title>
<simpara>The <emphasis role="strong">file</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>dir</term>
<listitem>
<simpara>the absolute path to the directory to monitor for files <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fixedDelay</term>
<listitem>
<simpara>the fixed delay polling interval specified in seconds <emphasis role="strong">(int, default: <literal>5</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>initialDelay</term>
<listitem>
<simpara>an initial delay when using a fixed delay trigger, expressed in TimeUnits (seconds by default) <emphasis role="strong">(int, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>maxMessages</term>
<listitem>
<simpara>the maximum messages per poll; -1 for unlimited <emphasis role="strong">(long, default: <literal>-1</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>mode</term>
<listitem>
<simpara>specifies how the file is being read. By default the content of a file is provided as byte array <emphasis role="strong">(FileReadingMode, default: <literal>contents</literal>, possible values: <literal>ref,lines,contents</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>pattern</term>
<listitem>
<simpara>a filter expression (Ant style) to accept only files that match the pattern <emphasis role="strong">(String, default: * )</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>preventDuplicates</term>
<listitem>
<simpara>whether to prevent the same file from being processed twice <emphasis role="strong">(boolean, default: <literal>true</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>timeUnit</term>
<listitem>
<simpara>the time unit for the fixed and initial delays <emphasis role="strong">(String, default: <literal>SECONDS</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>withMarkers</term>
<listitem>
<simpara>if true emits start of file/end of file marker messages before/after the data. Only valid with FileReadingMode 'lines' <emphasis role="strong">(Boolean, no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The <literal>ref</literal> option is useful in some cases in which the file contents are large and it would be more efficient to send the file path.</simpara>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-ftp-source">
<title>FTP Source</title>
<simpara>This source application supports transfer of files using the FTP protocol.
Files are transferred from the <literal>remote</literal> directory to the <literal>local</literal> directory where the app is deployed.
Messages emitted by the source are provided as a byte array by default. However, this can be
customized using the <literal>--mode</literal> option:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">ref</emphasis> Provides a <literal>java.io.File</literal> reference</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">lines</emphasis> Will split files line-by-line and emit a new message for each line</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">contents</emphasis> The default. Provides the contents of a file as a byte array</simpara>
</listitem>
</itemizedlist>
<simpara>When using <literal>--mode=lines</literal>, you can also provide the additional option <literal>--withMarkers=true</literal>.
If set to <literal>true</literal>, the underlying <literal>FileSplitter</literal> will emit additional <emphasis>start-of-file</emphasis> and <emphasis>end-of-file</emphasis> marker messages before and after the actual data.
The payload of these 2 additional marker messages is of type <literal>FileSplitter.FileMarker</literal>. The option <literal>withMarkers</literal> defaults to <literal>false</literal> if not explicitly set.</simpara>
<section xml:id="_options_2">
<title>Options</title>
<simpara>The <emphasis role="strong">ftp</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>autoCreateLocalDir</term>
<listitem>
<simpara>local directory must be auto created if it does not exist <emphasis role="strong">(boolean, default: <literal>true</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>clientMode</term>
<listitem>
<simpara>client mode to use : 2 for passive mode and 0 for active mode <emphasis role="strong">(int, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>deleteRemoteFiles</term>
<listitem>
<simpara>delete remote files after transfer <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>filenamePattern</term>
<listitem>
<simpara>simple filename pattern to apply to the filter <emphasis role="strong">(String, default: *)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fixedDelay</term>
<listitem>
<simpara>the rate at which to poll the remote directory <emphasis role="strong">(int, default: <literal>1</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>host</term>
<listitem>
<simpara>the host name for the FTP server <emphasis role="strong">(String, default: <literal>localhost</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>initialDelay</term>
<listitem>
<simpara>an initial delay when using a fixed delay trigger, expressed in TimeUnits (seconds by default) <emphasis role="strong">(int, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>localDir</term>
<listitem>
<simpara>set the local directory the remote files are transferred to <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>maxMessages</term>
<listitem>
<simpara>the maximum messages per poll; -1 for unlimited <emphasis role="strong">(long, default: <literal>-1</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>mode</term>
<listitem>
<simpara>specifies how the file is being read. By default the content of a file is provided as byte array <emphasis role="strong">(FileReadingMode, default: <literal>contents</literal>, possible values: <literal>ref,lines,contents</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>password</term>
<listitem>
<simpara>the password for the FTP connection <emphasis role="strong">(Password, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>port</term>
<listitem>
<simpara>the port for the FTP server <emphasis role="strong">(int, default: <literal>21</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>preserveTimestamp</term>
<listitem>
<simpara>whether to preserve the timestamp of files retrieved <emphasis role="strong">(boolean, default: <literal>true</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>remoteDir</term>
<listitem>
<simpara>the remote directory to transfer the files from <emphasis role="strong">(String, default: <literal>/</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>remoteFileSeparator</term>
<listitem>
<simpara>file separator to use on the remote side <emphasis role="strong">(String, default: <literal>/</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>timeUnit</term>
<listitem>
<simpara>the time unit for the fixed and initial delays <emphasis role="strong">(String, default: <literal>SECONDS</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>tmpFileSuffix</term>
<listitem>
<simpara>extension to use when downloading files <emphasis role="strong">(String, default: <literal>.tmp</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>username</term>
<listitem>
<simpara>the username for the FTP connection <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>withMarkers</term>
<listitem>
<simpara>if true emits start of file/end of file marker messages before/after the data. Only valid with FileReadingMode 'lines' <emphasis role="strong">(Boolean, no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-http-source">
<title>Http Source</title>
<simpara>A source module that listens for HTTP requests and emits the body as a message payload.
If the Content-Type matches <literal>text/*</literal> or <literal>application/json</literal>, the payload will be a String,
otherwise the payload will be a byte array.</simpara>
<section xml:id="_options_3">
<title>Options</title>
<simpara>The <emphasis role="strong">http</emphasis> source supports the following configuration properties:</simpara>
<variablelist>
<varlistentry>
<term>pathPattern</term>
<listitem>
<simpara>An Ant-Style pattern to determine which http requests will be captured <emphasis role="strong">(String, default: <literal>/</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-jdbc-source">
<title>JDBC Source</title>
<simpara>This source polls data from an RDBMS.
This source is fully based on the <literal>DataSourceAutoConfiguration</literal>, so refer to the
<link xlink:href="http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-sql.html">Spring Boot JDBC Support</link> for more
information.</simpara>
<section xml:id="_options_4">
<title>Options</title>
<simpara>The <emphasis role="strong">jdbc</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>query</term>
<listitem>
<simpara>the query to use to select data <emphasis role="strong">(String, no default, required)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>update</term>
<listitem>
<simpara>an SQL update statement to execute for marking polled messages as 'seen' <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>split</term>
<listitem>
<simpara>whether to split the SQL result as individual messages <emphasis role="strong">(boolean, default: <literal>true</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>$maxRowsPerPoll$$</term>
<listitem>
<simpara>max numbers of rows to process for each poll <emphasis role="strong">(int, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Also see the <link xlink:href="http://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html">Spring Boot Documentation</link>
for addition <literal>DataSource</literal> properties and <literal>TriggerProperties</literal> and <literal>MaxMessagesProperties</literal> for polling options.</simpara>
</section>
</section>
<section xml:id="jms">
<title>JMS</title>
<simpara>The "jms" source enables receiving messages from JMS.</simpara>
<section xml:id="_options_5">
<title>Options</title>
<simpara>The <emphasis role="strong">jms</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>spring.jms.listener.acknowledgeMode</term>
<listitem>
<simpara>the session acknowledge mode <emphasis role="strong">(String, default: <literal>AUTO</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>clientId</term>
<listitem>
<simpara>an identifier for the client, to be associated with a durable or shared topic subscription <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>destination</term>
<listitem>
<simpara>the destination name from which messages will be received <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>messageSelector</term>
<listitem>
<simpara>a message selector to be applied to messages <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>subscriptionDurable</term>
<listitem>
<simpara>when true, indicates the subscription to a topic is durable <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>subscriptionShared</term>
<listitem>
<simpara>when true, indicates the subscription to a topic is shared (JMS 2.0) <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.jms.pubSubDomain</term>
<listitem>
<simpara>when true, indicates that the destination is a topic <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>subscriptionName</term>
<listitem>
<simpara>a name that will be assigned to the topic subscription <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>sessionTransacted</term>
<listitem>
<simpara>True to enable transactions and use a `DefaultMessageListenerContainer`, false to select a
`SimpleMessageListenerContainer` <emphasis role="strong">(String, default: true)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.jms.listener.concurrency</term>
<listitem>
<simpara>The minimum number of consumer threads. *(Integer, default: 1)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.jms.listener.maxConcurrency</term>
<listitem>
<simpara>The maximum number of consumer threads. Only supported when `sessionTransacted ` is true *(Integer, default: 1)</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>Spring boot broker configuration is used; refer to the
<link xlink:href="http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#boot-features-jms">Spring Boot Documentation</link> for more information.
The <literal>spring.jms.*</literal> properties above are also handled by the boot JMS support.</simpara>
</note>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-load-generator">
<title>Load Generator (<literal>load-generator</literal>)</title>
<simpara>A source that sends generated data and dispatches it to the stream. This is to provide a method for users to identify the performance of Spring Cloud Data Flow in different environments and deployment types.</simpara>
<section xml:id="_options_6">
<title>Options</title>
<simpara>The <emphasis role="strong">load-generator</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>messageCount</term>
<listitem>
<simpara>the number of messages to send <emphasis role="strong">(Integer, default: <literal>100</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>messageSize</term>
<listitem>
<simpara>the size of message to send <emphasis role="strong">(Integer, <literal>1000</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>producers</term>
<listitem>
<simpara>the number of producers <emphasis role="strong">(Integer, <literal>1</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>outputType</term>
<listitem>
<simpara>how this module should emit messages it produces <emphasis role="strong">(MimeType, default: no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-source-rabbit">
<title>RabbitMQ</title>
<simpara>The "rabbit" source enables receiving messages from RabbitMQ.</simpara>
<simpara>The queue(s) must exist before the stream is deployed; they are not created automatically.
You can easily create a Queue using the RabbitMQ web UI.</simpara>
<section xml:id="_options_7">
<title>Options</title>
<simpara>The <emphasis role="strong">rabbit</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>enableRetry</term>
<listitem>
<simpara>enable retry; when retries are exhausted the message will be rejected; message disposition will depend on dead letter configuration <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>initialRetryInterval</term>
<listitem>
<simpara>initial interval between retries <emphasis role="strong">(int, default: <literal>1000</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>mappedRequestHeaders</term>
<listitem>
<simpara>request message header names to be mapped from the incoming message <emphasis role="strong">(String, default: <literal>STANDARD_REQUEST_HEADERS</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>maxAttempts</term>
<listitem>
<simpara>maximum delivery attempts <emphasis role="strong">(int, default: <literal>3</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>maxConcurrency</term>
<listitem>
<simpara>the maximum number of consumers <emphasis role="strong">(int, default: <literal>1</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>maxRetryInterval</term>
<listitem>
<simpara>maximum retry interval <emphasis role="strong">(int, default: <literal>30000</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>queues</term>
<listitem>
<simpara>the queue(s) from which messages will be received <emphasis role="strong">(String, default: no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>requeue</term>
<listitem>
<simpara>whether rejected messages will be requeued by default <emphasis role="strong">(boolean, default: <literal>true</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>retryMultiplier</term>
<listitem>
<simpara>retry interval multiplier <emphasis role="strong">(double, default: <literal>2.0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>transacted</term>
<listitem>
<simpara>true if the channel is to be transacted <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Also see the <link xlink:href="http://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html">Spring Boot Documentation</link>
for addition properties for the broker connections and listener properties.</simpara>
</section>
<section xml:id="rabbitSourceRetry">
<title>A Note About Retry</title>
<note>
<simpara>With the default <emphasis>ackMode</emphasis> (<emphasis role="strong">AUTO</emphasis>) and <emphasis>requeue</emphasis> (<emphasis role="strong">true</emphasis>) options, failed message deliveries will be retried
indefinitely.
Since there is not much processing in the rabbit source, the risk of failure in the source itself is small, unless
the downstream <literal>Binder</literal> is not connected for some reason.
Setting <emphasis>requeue</emphasis> to <emphasis role="strong">false</emphasis> will cause messages to be rejected on the first attempt (and possibly sent to a Dead Letter
Exchange/Queue if the broker is so configured).
The <emphasis>enableRetry</emphasis> option allows configuration of retry parameters such that a failed message delivery can be retried and
eventually discarded (or dead-lettered) when retries are exhausted.
The delivery thread is suspended during the retry interval(s).
Retry options are <emphasis>enableRetry</emphasis>, <emphasis>maxAttempts</emphasis>, <emphasis>initialRetryInterval</emphasis>, <emphasis>retryMultiplier</emphasis>, and <emphasis>maxRetryInterval</emphasis>.
Message deliveries failing with a <emphasis>MessageConversionException</emphasis> are never retried; the assumption being that if a message
could not be converted on the first attempt, subsequent attempts will also fail.
Such messages are discarded (or dead-lettered).</simpara>
</note>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-sftp">
<title>SFTP (<literal>sftp</literal>)</title>
<simpara>This source module supports transfer of files using the SFTP protocol.
Files are transferred from the <literal>remote</literal> directory to the <literal>local</literal> directory where the module is deployed.</simpara>
<simpara>Messages emitted by the source are provided as a byte array by default. However, this can be
customized using the <literal>--mode</literal> option:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">ref</emphasis> Provides a <literal>java.io.File</literal> reference</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">lines</emphasis> Will split files line-by-line and emit a new message for each line</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">contents</emphasis> The default. Provides the contents of a file as a byte array</simpara>
</listitem>
</itemizedlist>
<simpara>When using <literal>--mode=lines</literal>, you can also provide the additional option <literal>--withMarkers=true</literal>.
If set to <literal>true</literal>, the underlying <literal>FileSplitter</literal> will emit additional <emphasis>start-of-file</emphasis> and <emphasis>end-of-file</emphasis> marker messages before and after the actual data.
The payload of these 2 additional marker messages is of type <literal>FileSplitter.FileMarker</literal>. The option <literal>withMarkers</literal> defaults to <literal>false</literal> if not explicitly set.</simpara>
<section xml:id="_options_8">
<title>Options</title>
<simpara>The <emphasis role="strong">sftp</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>allowUnknownKeys</term>
<listitem>
<simpara>true to allow connecting to a host with an unknown or changed key <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>autoCreateLocalDir</term>
<listitem>
<simpara>if local directory must be auto created if it does not exist <emphasis role="strong">(boolean, default: <literal>true</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>deleteRemoteFiles</term>
<listitem>
<simpara>delete remote files after transfer <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fixedDelay</term>
<listitem>
<simpara>fixed delay in SECONDS to poll the remote directory <emphasis role="strong">(int, default: <literal>1</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>host</term>
<listitem>
<simpara>the remote host to connect to <emphasis role="strong">(String, default: <literal>localhost</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>initialDelay</term>
<listitem>
<simpara>an initial delay when using a fixed delay trigger, expressed in TimeUnits (seconds by default) <emphasis role="strong">(int, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>knownHostsExpression</term>
<listitem>
<simpara>a SpEL expression location of known hosts file; required if 'allowUnknownKeys' is false; examples: systemProperties["user.home"]+"/.ssh/known_hosts", "/foo/bar/known_hosts" <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>localDir</term>
<listitem>
<simpara>set the local directory the remote files are transferred to <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>maxMessages</term>
<listitem>
<simpara>the maximum messages per poll; -1 for unlimited <emphasis role="strong">(long, default: <literal>-1</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>mode</term>
<listitem>
<simpara>specifies how the file is being read. By default the content of a file is provided as byte array <emphasis role="strong">(FileReadingMode, default: <literal>contents</literal>, possible values: <literal>ref,lines,contents</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>passPhrase</term>
<listitem>
<simpara>the passphrase to use <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>password</term>
<listitem>
<simpara>the password for the provided user <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>pattern</term>
<listitem>
<simpara>simple filename pattern to apply to the filter <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>port</term>
<listitem>
<simpara>the remote port to connect to <emphasis role="strong">(int, default: <literal>22</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>privateKey</term>
<listitem>
<simpara>the private key location (a valid Spring Resource URL) <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>regexPattern</term>
<listitem>
<simpara>filename regex pattern to apply to the filter <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>remoteDir</term>
<listitem>
<simpara>the remote directory to transfer the files from <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>timeUnit</term>
<listitem>
<simpara>the time unit for the fixed and initial delays <emphasis role="strong">(String, default: <literal>SECONDS</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>tmpFileSuffix</term>
<listitem>
<simpara>extension to use when downloading files <emphasis role="strong">(String, default: <literal>.tmp</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>user</term>
<listitem>
<simpara>the username to use <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>withMarkers</term>
<listitem>
<simpara>if true emits start of file/end of file marker messages before/after the data. Only valid with FileReadingMode 'lines' <emphasis role="strong">(Boolean, no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-clound-stream-modules-source-syslog">
<title>SYSLOG</title>
<simpara>The syslog source receives SYSLOG packets over UDP, TCP, or both.
RFC3164 (BSD) and RFC5424 formats are supported.</simpara>
<section xml:id="_options_9">
<title>Options</title>
<simpara>The <emphasis role="strong">syslog</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>protocol</term>
<listitem>
<simpara>`udp`, `tcp`, or `both` <emphasis role="strong">(String, default <literal>tcp</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>rfc</term>
<listitem>
<simpara>`3164` or `5424` <emphasis role="strong">(String, default <literal>3164</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>port</term>
<listitem>
<simpara>the port on which to listen  <emphasis role="strong">(String, default <literal>1514</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>bufferSize</term>
<listitem>
<simpara>the maximum size allowed (TCP) <emphasis role="strong">(int, default <literal>2048</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>nio</term>
<listitem>
<simpara>`true` to use NIO - only recommended when supporting many connections <emphasis role="strong">(Boolean, default <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>reverseLookup</term>
<listitem>
<simpara>`true` to perform a reverse lookup on the remote IP address <emphasis role="strong">(Boolean, default <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>socketTimeout</term>
<listitem>
<simpara>the socket timeout <emphasis role="strong">(long, default <literal>none</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="_tcp">
<title>TCP</title>
<simpara>The <literal>tcp</literal> source acts as a server and allows a remote party to connect to Spring Cloud Data Flow and submit data over a raw tcp socket.</simpara>
<simpara>TCP is a streaming protocol and some mechanism is needed to frame messages on the wire. A number of decoders are
available, the default being 'CRLF' which is compatible with Telnet.</simpara>
<simpara>Messages produced by the TCP source module have a <literal>byte[]</literal> payload.</simpara>
<section xml:id="_options_10">
<title>Options</title>
<variablelist>
<varlistentry>
<term>bufferSize</term>
<listitem>
<simpara>the size of the buffer (bytes) to use when decoding <emphasis role="strong">(int, default: <literal>2048</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>decoder</term>
<listitem>
<simpara>the decoder to use when receiving messages <emphasis role="strong">(Encoding, default: <literal>CRLF</literal>, possible values: <literal>CRLF,LF,NULL,STXETX,RAW,L1,L2,L4</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>nio</term>
<listitem>
<simpara>whether or not to use NIO <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>port</term>
<listitem>
<simpara>the port on which to listen <emphasis role="strong">(int, default: <literal>1234</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>reverseLookup</term>
<listitem>
<simpara>perform a reverse DNS lookup on the remote IP Address <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>socketTimeout</term>
<listitem>
<simpara>the timeout (ms) before closing the socket when no data is received <emphasis role="strong">(int, default: <literal>120000</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>useDirectBuffers</term>
<listitem>
<simpara>whether or not to use direct buffers <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="_available_decoders">
<title>Available Decoders</title>
<variablelist>
<title>Text Data</title>
<varlistentry>
<term>CRLF (default)</term>
<listitem>
<simpara>text terminated by carriage return (0x0d) followed by line feed (0x0a)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>LF</term>
<listitem>
<simpara>text terminated by line feed (0x0a)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>NULL</term>
<listitem>
<simpara>text terminated by a null byte (0x00)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>STXETX</term>
<listitem>
<simpara>text preceded by an STX (0x02) and terminated by an ETX (0x03)</simpara>
</listitem>
</varlistentry>
</variablelist>
<variablelist>
<title>Text and Binary Data</title>
<varlistentry>
<term>RAW</term>
<listitem>
<simpara>no structure - the client indicates a complete message by closing the socket</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>L1</term>
<listitem>
<simpara>data preceded by a one byte (unsigned) length field (supports up to 255 bytes)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>L2</term>
<listitem>
<simpara>data preceded by a two byte (unsigned) length field (up to 2<superscript>16</superscript>-1 bytes)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>L4</term>
<listitem>
<simpara>data preceded by a four byte (signed) length field (up to 2<superscript>31</superscript>-1 bytes)</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-time">
<title>Time (<literal>time</literal>)</title>
<simpara>The time source will simply emit a String with the current time every so often.</simpara>
<section xml:id="_options_11">
<title>Options</title>
<simpara>The <emphasis role="strong">time</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>fixedDelay</term>
<listitem>
<simpara>time delay between messages, expressed in TimeUnits (seconds by default) <emphasis role="strong">(int, default: <literal>1</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>dateFormat</term>
<listitem>
<simpara>how to render the current time, using SimpleDateFormat <emphasis role="strong">(String, default: <literal>yyyy-MM-dd HH:mm:ss</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>initialDelay</term>
<listitem>
<simpara>an initial delay when using a fixed delay trigger, expressed in TimeUnits (seconds by default) <emphasis role="strong">(int, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>timeUnit</term>
<listitem>
<simpara>the time unit for the fixed and initial delays <emphasis role="strong">(String, default: <literal>SECONDS</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-twitterstream">
<title>Twitter Stream (<literal>twitterstream</literal>)</title>
<simpara>This source ingests data from Twitter&#8217;s <link xlink:href="https://dev.twitter.com/docs/streaming-apis/streams/public">streaming API v1.1</link>. It uses the <link xlink:href="https://dev.twitter.com/docs/streaming-apis/streams/public">sample and filter</link> stream endpoints rather than the full "firehose" which needs special access. The endpoint used will depend on the parameters you supply in the stream definition (some are specific to the filter endpoint).</simpara>
<simpara>You need to supply all keys and secrets (both consumer and accessToken) to authenticate for this source, so it is easiest if you just add these as the following environment variables: CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN and ACCESS_TOKEN_SECRET.</simpara>
<simpara>Stream creation is then straightforward:</simpara>
<literallayout class="monospaced">dataflow:&gt; stream create --name tweets --definition "twitterstream | log" --deploy</literallayout>
<section xml:id="_options_12">
<title>Options</title>
<simpara>The <emphasis role="strong">twitterstream</emphasis> source has the following options:</simpara>
<variablelist>
<varlistentry>
<term>accessToken</term>
<listitem>
<simpara>a valid OAuth access token <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>accessTokenSecret</term>
<listitem>
<simpara>an OAuth secret corresponding to the access token <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>consumerKey</term>
<listitem>
<simpara>a consumer key issued by twitter <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>consumerSecret</term>
<listitem>
<simpara>consumer secret corresponding to the consumer key <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>language</term>
<listitem>
<simpara>language code e.g. 'en' <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara><literal>twitterstream</literal> emit JSON in the <link xlink:href="https://dev.twitter.com/docs/platform-objects/tweets">native Twitter format</link>.</simpara>
</note>
</section>
</section>
</chapter>
<chapter xml:id="spring-cloud-stream-modules-processors">
<title>Processors</title>
<section xml:id="spring-cloud-stream-modules-filter">
<title>Filter (<literal>filter</literal>)</title>
<simpara>Use the filter module in a stream to determine whether a Message should be passed to the output channel.</simpara>
<simpara>The <emphasis role="strong">filter</emphasis> processor has the following options:</simpara>
<variablelist>
<varlistentry>
<term>expression</term>
<listitem>
<simpara>a SpEL expression used to transform messages <emphasis role="strong">(String, default: <literal>payload.toString()</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<section xml:id="_filter_with_spel_expression">
<title>Filter with SpEL expression</title>
<simpara>The simplest way to use the filter processor is to pass a SpEL expression when creating the stream. The expression should evaluate the message and return true or false.  For example:</simpara>
<literallayout class="monospaced">dataflow:&gt; stream create --name filtertest --definition "http --server.port=9000 | filter --expression=payload=='good' | log" --deploy</literallayout>
<simpara>This filter will only pass Messages to the log sink if the payload is the word "good". Try sending "good" to the HTTP endpoint and you should see it in the Spring Cloud Data Flow logs:</simpara>
<literallayout class="monospaced">dataflow:&gt; http post --target http://localhost:9000 --data "good"</literallayout>
<simpara>Alternatively, if you send the word "bad" (or anything else), you shouldn&#8217;t see the log entry.</simpara>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-groovy-filter">
<title>Groovy Filter (<literal>groovy-filter</literal>)</title>
<simpara>A Processor module that retains or discards messages according to a predicate, expressed as a Groovy script.</simpara>
<section xml:id="_options_13">
<title>Options</title>
<simpara>The <emphasis role="strong">groovy-filter</emphasis> processor has the following options:</simpara>
<variablelist>
<varlistentry>
<term>script</term>
<listitem>
<simpara>The script resource location <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>variables</term>
<listitem>
<simpara>Variable bindings as a comma delimited string of name-value pairs, e.g. 'foo=bar,baz=car' <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>variablesLocation</term>
<listitem>
<simpara>The location of a properties file containing custom script variable bindings <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-httpclient">
<title>Http Client (<literal>httpclient</literal>)</title>
<simpara>A processor module that makes requests to an HTTP resource and emits the response body as a message payload. This processor can be combined, e.g., with a time source module to periodically poll results from a HTTP resource.</simpara>
<section xml:id="_options_14">
<title>Options</title>
<simpara>The <emphasis role="strong">httpclient</emphasis> processor has the following options:</simpara>
<variablelist>
<varlistentry>
<term>url</term>
<listitem>
<simpara>The URL to issue an http request to, as a static value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>urlExpression</term>
<listitem>
<simpara>A SpEL expression against incoming message to determine the URL to use.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>httpMethod</term>
<listitem>
<simpara>The kind of http method to use.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>body</term>
<listitem>
<simpara>The (static) body of the request to use.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>bodyExpression</term>
<listitem>
<simpara>A SpEL expression against incoming message to derive the request body to use.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>headersExpression</term>
<listitem>
<simpara>A SpEL expression used to derive the http headers map to use.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>expectedResponseType</term>
<listitem>
<simpara>The type used to interpret the response.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>replyExpression</term>
<listitem>
<simpara>A SpEL expression used to compute the final result, applied against the whole http response.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-bridge">
<title>Bridge (<literal>bridge</literal>)</title>
<simpara>A Processor module that returns messages that is passed by connecting just the input and output channels.</simpara>
</section>
<section xml:id="spring-cloud-stream-modules-groovy-transform">
<title>Groovy Transform (<literal>groovy-transform</literal>)</title>
<simpara>A Processor module that transforms messages using a Groovy script.</simpara>
<section xml:id="_options_15">
<title>Options</title>
<simpara>The <emphasis role="strong">groovy-transform</emphasis> processor has the following options:</simpara>
<variablelist>
<varlistentry>
<term>script</term>
<listitem>
<simpara>The script resource location <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>variables</term>
<listitem>
<simpara>Variable bindings as a comma delimited string of name-value pairs, e.g. 'foo=bar,baz=car' <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>variablesLocation</term>
<listitem>
<simpara>The location of a properties file containing custom script variable bindings <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-transform">
<title>Transform (<literal>transform</literal>)</title>
<simpara>Use the transform module in a stream to convert a Message&#8217;s content or structure.</simpara>
<section xml:id="_options_16">
<title>Options</title>
<simpara>The <emphasis role="strong">transform</emphasis> processor has the following options:</simpara>
<variablelist>
<varlistentry>
<term>expression</term>
<listitem>
<simpara>a SpEL expression used to transform messages <emphasis role="strong">(String, default: <literal>payload.toString()</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="_transform_with_spel_expression">
<title>Transform with SpEL expression</title>
<simpara>The simplest way to use the transform processor is to pass a SpEL expression when creating the stream. The expression should return the modified message or payload.  For example:</simpara>
<literallayout class="monospaced">dataflow:&gt; stream create --name transformtest --definition "http --server.port=9003 | transform --expression=payload.toUpperCase() | log" --deploy</literallayout>
<simpara>This transform will convert all message payloads to upper case. If sending the word "foo" to the HTTP endpoint and you should see "FOO" in the Spring Cloud Data Flow logs:</simpara>
<literallayout class="monospaced">dataflow:&gt; http post --target http://localhost:9003 --data "foo"</literallayout>
<simpara>As part of the SpEL expression you can make use of the pre-registered JSON Path function.  The syntax is #jsonPath(payload,'&lt;json path expression&gt;')</simpara>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-splitter">
<title>Splitter</title>
<simpara>The splitter module builds upon the concept of the same name in Spring Integration and allows the splitting of a single
message into several distinct messages.</simpara>
<variablelist>
<varlistentry>
<term>expression</term>
<listitem>
<simpara>a SpEL expression which would typically evaluate to an array or collection <emphasis role="strong">(String, default: <literal>null</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>delimiters</term>
<listitem>
<simpara>A list of delimiters to tokenize a String payload ('expression' must be null) <emphasis role="strong">(String, default: <literal>null</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fileMarkers</term>
<listitem>
<simpara>Split File payloads, when true, START and END marker messages will be emitted, when false no markers are emitted <emphasis role="strong">(String, default: <literal>null</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>charset</term>
<listitem>
<simpara>Split File payloads using this charset to convert bytes to String <emphasis role="strong">(String, default: <literal>null</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>applySequence</term>
<listitem>
<simpara>Add correlation and sequence information to the message headers <emphasis role="strong">(String, default: <literal>true</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>When no <literal>expression</literal>, <literal>fileMarkers</literal>, or <literal>charset</literal> is provided, a <literal>DefaultMessageSplitter</literal> is configured with (optional) <literal>delimiters</literal>.
When <literal>fileMarkers</literal> or <literal>charset</literal> is provided, a <literal>FileSplitter</literal> is configured (you must provide either a <literal>fileMarkers</literal>
or <literal>charset</literal> to split files, which must be text-based - they are split into lines).
Otherwise, an <literal>ExpressionEvaluatingMessageSplitter</literal> is configured.</simpara>
<simpara>When splitting <literal>File</literal> payloads, the <literal>sequenceSize</literal> header is zero because the size cannot be determined at the beginning.</simpara>
<simpara><emphasis role="strong">Ambiguous properties are not allowed.</emphasis></simpara>
<section xml:id="_json_example">
<title>JSON Example</title>
<simpara>As part of the SpEL expression you can make use of the pre-registered JSON Path function. The syntax is
<literal>#jsonPath(payload, '&lt;json path expression&gt;')</literal>.</simpara>
<simpara>For example, consider the following JSON:</simpara>
<programlisting language="json" linenumbering="unnumbered">{ "store": {
    "book": [
        {
            "category": "reference",
            "author": "Nigel Rees",
            "title": "Sayings of the Century",
            "price": 8.95
        },
        {
            "category": "fiction",
            "author": "Evelyn Waugh",
            "title": "Sword of Honour",
            "price": 12.99
        },
        {
            "category": "fiction",
            "author": "Herman Melville",
            "title": "Moby Dick",
            "isbn": "0-553-21311-3",
            "price": 8.99
        },
        {
            "category": "fiction",
            "author": "J. R. R. Tolkien",
            "title": "The Lord of the Rings",
            "isbn": "0-395-19395-8",
            "price": 22.99
        }
    ],
    "bicycle": {
        "color": "red",
        "price": 19.95
    }
}}</programlisting>
<simpara>and an expression <literal>#jsonPath(payload, '$.store.book')</literal>; the result will be 4 messages, each with a <literal>Map</literal> payload
containing the properties of a single book.</simpara>
</section>
</section>
</chapter>
<chapter xml:id="spring-cloud-stream-modules-sinks">
<title>Sinks</title>
<section xml:id="spring-cloud-stream-modules-cassandra">
<title>Cassandra (<literal>cassandra</literal>)</title>
<simpara>The Cassandra sink writes into a Cassandra table.  Here is a simple example</simpara>
<literallayout class="monospaced">dataflow:&gt;stream create cassandrastream --definition "http --server.port=8888 --spring.cloud.stream.bindings.output.contentType='application/json' | cassandra --ingestQuery='insert into book (id, isbn, title, author) values (uuid(), ?, ?, ?)' --spring.cassandra.keyspace=clouddata" --deploy</literallayout>
<simpara>Create a keyspace and a <literal>book</literal> table in Cassandra using:</simpara>
<programlisting language="text" linenumbering="unnumbered">CREATE KEYSPACE clouddata WITH REPLICATION = { 'class' : 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1' } AND DURABLE_WRITES = true;
USE clouddata;
CREATE TABLE book  (
    id          uuid PRIMARY KEY,
    isbn        text,
    author      text,
    title       text
);</programlisting>
<simpara>You can then send data to this stream via</simpara>
<literallayout class="monospaced">dataflow:&gt;http post --contentType 'application/json' --data '{"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}' --target http://localhost:8888/
&gt; POST (application/json;charset=UTF-8) http://localhost:8888/ {"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}
&gt; 202 ACCEPTED</literallayout>
<simpara>and see the table contents using the CQL</simpara>
<literallayout class="monospaced">SELECT * FROM clouddata.book;</literallayout>
<section xml:id="_options_17">
<title>Options</title>
<simpara>The <emphasis role="strong">cassandra</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>compressionType</term>
<listitem>
<simpara>the compression to use for the transport <emphasis role="strong">(CompressionType, default: <literal>NONE</literal>, possible values: <literal>NONE,SNAPPY</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>consistencyLevel</term>
<listitem>
<simpara>the consistencyLevel option of WriteOptions <emphasis role="strong">(ConsistencyLevel, no default, possible values: <literal>ANY,ONE,TWO,THREE,QUOROM,LOCAL_QUOROM,EACH_QUOROM,ALL,LOCAL_ONE,SERIAL,LOCAL_SERIAL</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.cassandra.contactPoints</term>
<listitem>
<simpara>the comma-delimited string of the hosts to connect to Cassandra <emphasis role="strong">(String, default: <literal>localhost</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>entityBasePackages</term>
<listitem>
<simpara>the base packages to scan for entities annotated with Table annotations <emphasis role="strong">(String[], default: <literal>[]</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>ingestQuery</term>
<listitem>
<simpara>the ingest Cassandra query <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.cassandra.initScript</term>
<listitem>
<simpara>the path to file with CQL scripts (delimited by ';') to initialize keyspace schema <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.cassandra.keyspace</term>
<listitem>
<simpara>the keyspace name to connect to <emphasis role="strong">(String, default: <literal>&lt;stream name&gt;</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>metricsEnabled</term>
<listitem>
<simpara>enable/disable metrics collection for the created cluster <emphasis role="strong">(boolean, default: <literal>true</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.cassandra.password</term>
<listitem>
<simpara>the password for connection <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.cassandra.port</term>
<listitem>
<simpara>the port to use to connect to the Cassandra host <emphasis role="strong">(int, default: <literal>9042</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>queryType</term>
<listitem>
<simpara>the queryType for Cassandra Sink <emphasis role="strong">(Type, default: <literal>INSERT</literal>, possible values: <literal>INSERT,UPDATE,DELETE,STATEMENT</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>retryPolicy</term>
<listitem>
<simpara>the retryPolicy  option of WriteOptions <emphasis role="strong">(RetryPolicy, no default, possible values: <literal>DEFAULT,DOWNGRADING_CONSISTENCY,FALLTHROUGH,LOGGING</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>statementExpression</term>
<listitem>
<simpara>the expression in Cassandra query DSL style <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.cassandra.schemaAction</term>
<listitem>
<simpara>schema action to perform <emphasis role="strong">(SchemaAction, default: <literal>NONE</literal>, possible values: <literal>CREATE,NONE,RECREATE,RECREATE_DROP_UNUSED</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>ttl</term>
<listitem>
<simpara>the time-to-live option of WriteOptions <emphasis role="strong">(int, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>spring.cassandra.username</term>
<listitem>
<simpara>the username for connection <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-counter">
<title>Counter (<literal>counter</literal>)</title>
<simpara>A simple module that counts messages received, using Spring Boot metrics abstraction.</simpara>
<simpara>The <emphasis role="strong">counter</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>name</term>
<listitem>
<simpara>The name of the counter to increment. <emphasis role="strong">(String, default: <literal>counts</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>nameExpression</term>
<listitem>
<simpara>A SpEL expression (against the incoming Message) to derive the name of the counter to increment. <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>store</term>
<listitem>
<simpara>The name of a store used to store the counter. <emphasis role="strong">(String, default: <literal>memory</literal>, possible values: <literal>memory</literal>, <literal>redis</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="spring-cloud-stream-modules-field-value-counter">
<title>Field Value Counter (<literal>field-value-counter</literal>)</title>
<simpara>A field value counter is a Metric used for counting occurrences of unique values for a named field in a message payload. Spring Cloud Data Flow supports the following payload types out of the box:</simpara>
<itemizedlist>
<listitem>
<simpara>POJO (Java bean)</simpara>
</listitem>
<listitem>
<simpara>Tuple</simpara>
</listitem>
<listitem>
<simpara>JSON String</simpara>
</listitem>
</itemizedlist>
<simpara>For example suppose a message source produces a payload with a field named <emphasis>user</emphasis> :</simpara>
<programlisting language="java" linenumbering="unnumbered">class Foo {
   String user;
   public Foo(String user) {
       this.user = user;
   }
}</programlisting>
<simpara>If the stream source produces messages with the following objects:</simpara>
<programlisting language="java" linenumbering="unnumbered">   new Foo("fred")
   new Foo("sue")
   new Foo("dave")
   new Foo("sue")</programlisting>
<simpara>The field value counter on the field <emphasis>user</emphasis> will contain:</simpara>
<literallayout class="monospaced">fred:1, sue:2, dave:1</literallayout>
<simpara>Multi-value fields are also supported. For example, if a field contains a list, each value will be counted once:</simpara>
<literallayout class="monospaced">users:["dave","fred","sue"]
users:["sue","jon"]</literallayout>
<simpara>The field value counter on the field <emphasis>users</emphasis> will contain:</simpara>
<literallayout class="monospaced">dave:1, fred:1, sue:2, jon:1</literallayout>
<section xml:id="_options_18">
<title>Options</title>
<simpara>The <emphasis role="strong">field-value-counter</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>fieldName</term>
<listitem>
<simpara>the name of the field for which values are counted <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>name</term>
<listitem>
<simpara>the name of the metric to contribute to (will be created if necessary) <emphasis role="strong">(String, default: <literal>&lt;stream name&gt;</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>nameExpression</term>
<listitem>
<simpara>a SpEL expression to compute the name of the metric to contribute to <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-file-sink">
<title>File (<literal>file</literal>)</title>
<simpara>This module writes each message it receives to a file.</simpara>
<section xml:id="_options_19">
<title>Options</title>
<simpara>The <emphasis role="strong">file</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>binary</term>
<listitem>
<simpara>if false, will append a newline character at the end of each line <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>charset</term>
<listitem>
<simpara>the charset to use when writing a String payload <emphasis role="strong">(String, default: <literal>UTF-8</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>dir</term>
<listitem>
<simpara>the directory in which files will be created <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>dirExpression</term>
<listitem>
<simpara>spring expression used to define directory name <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>mode</term>
<listitem>
<simpara>what to do if the file already exists <emphasis role="strong">(Mode, default: <literal>APPEND</literal>, possible values: <literal>APPEND,REPLACE,FAIL,IGNORE</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>name</term>
<listitem>
<simpara>filename pattern to use <emphasis role="strong">(String, default: <literal>&lt;stream name&gt;</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>nameExpression</term>
<listitem>
<simpara>spring expression used to define filename <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>suffix</term>
<listitem>
<simpara>filename extension to use <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="ftp-sink">
<title>FTP Sink (<literal>ftp</literal>)</title>
<simpara>FTP sink is a simple option to push files to an FTP server from incoming messages.</simpara>
<simpara>It uses an <literal>ftp-outbound-adapter</literal>, therefore incoming messages could be either a <literal>java.io.File</literal> object, a <literal>String</literal> (content of the file)
or an array of <literal>bytes</literal> (file content as well).</simpara>
<simpara>To use this sink, you need a username and a password to login.</simpara>
<note>
<simpara>By default Spring Integration will use <literal>o.s.i.file.DefaultFileNameGenerator</literal> if none is specified. <literal>DefaultFileNameGenerator</literal> will determine the file name
based on the value of the <literal>file_name</literal> header (if it exists) in the <literal>MessageHeaders</literal>, or if the payload of the <literal>Message</literal> is already a <literal>java.io.File</literal>, then it will
use the original name of that file.</simpara>
</note>
</section>
<section xml:id="spring-cloud-stream-modules-gemfire-sink">
<title>Gemfire (<literal>gemfire</literal>)</title>
<simpara>A sink module that allows one to write message payloads to a Gemfire server.</simpara>
<section xml:id="_options_20">
<title>Options</title>
<simpara>The <emphasis role="strong">gemfire</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>hostAddresses</term>
<listitem>
<simpara>a comma separated list of [host]:[port] specifying either locator or server addresses for the client connection pool <emphasis role="strong">(String, <literal>localhost:10334</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>keyExpression</term>
<listitem>
<simpara>a SpEL expression which is evaluated to create a cache key <emphasis role="strong">(String, default: <literal>the value is currently the message payload'</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>port</term>
<listitem>
<simpara>port of the cache server or locator (if useLocator=true). May be a comma delimited list <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>regionName</term>
<listitem>
<simpara>name of the region to use when storing data <emphasis role="strong">(String, default: <literal>${spring.application.name}</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>connectType</term>
<listitem>
<simpara>'server' or 'locator' <emphasis role="strong">(String, default: <literal>locator</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-hdfs">
<title>Hadoop (HDFS) (<literal>hdfs</literal>)</title>
<simpara>If you do not have Hadoop installed, you can install Hadoop as described in our <link xlink:href="Hadoop-Installation.xml#installing-hadoop">separate guide</link>.</simpara>
<simpara>Once Hadoop is up and running, you can then use the <literal>hdfs</literal> sink when creating a <link xlink:href="Streams.xml#streams">stream</link></simpara>
<literallayout class="monospaced">dataflow:&gt; stream create --name myhdfsstream1 --definition "time | hdfs" --deploy</literallayout>
<simpara>In the above example, we&#8217;ve scheduled <literal>time</literal> source to automatically send ticks to <literal>hdfs</literal> once in every second. If you wait a little while for data to accumuluate you can then list can then list the files in the hadoop filesystem using the shell&#8217;s built in hadoop fs commands.  Before making any access to HDFS in the shell you first need to configure the shell to point to your name node.  This is done using the <literal>hadoop config</literal> command.</simpara>
<literallayout class="monospaced">dataflow:&gt;hadoop config fs --namenode hdfs://localhost:8020</literallayout>
<simpara>In this example the hdfs protocol is used but you may also use the webhdfs protocol.  Listing the contents in the output directory (named by default after the stream name) is done by issuing the following command.</simpara>
<literallayout class="monospaced">dataflow:&gt;hadoop fs ls /xd/myhdfsstream1
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup          0 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt.tmp</literallayout>
<simpara>While the file is being written to it will have the <literal>tmp</literal> suffix.  When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the <literal>tmp</literal> suffix.  There are several options to control the in use file file naming options.  These are <literal>--inUsePrefix</literal> and <literal>--inUseSuffix</literal> set the file name prefix and suffix respectfully.</simpara>
<simpara>When you destroy a stream</simpara>
<literallayout class="monospaced">dataflow:&gt;stream destroy --name myhdfsstream1</literallayout>
<simpara>and list the stream directory again, in use file suffix doesn&#8217;t exist anymore.</simpara>
<literallayout class="monospaced">dataflow:&gt;hadoop fs ls /xd/myhdfsstream1
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup        380 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt</literallayout>
<simpara>To list the list the contents of a file directly from a shell execute the hadoop cat command.</simpara>
<literallayout class="monospaced">dataflow:&gt; hadoop fs cat /xd/myhdfsstream1/myhdfsstream1-0.txt
2013-12-18 18:10:07
2013-12-18 18:10:08
2013-12-18 18:10:09
...</literallayout>
<simpara>In the above examples we didn&#8217;t yet go through why the file was written in a specific directory and why it was named in this specific way. Default location of a file is defined as <literal>/xd/&lt;stream name&gt;/&lt;stream name&gt;-&lt;rolling part&gt;.txt</literal>. These can be changed using options <literal>--directory</literal> and <literal>--fileName</literal> respectively. Example is shown below.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream create --name myhdfsstream2 --definition "time | hdfs --directory=/xd/tmp --fileName=data" --deploy
dataflow:&gt;stream destroy --name myhdfsstream2
dataflow:&gt;hadoop fs ls /xd/tmp
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup        120 2013-12-18 18:31 /xd/tmp/data-0.txt</literallayout>
<simpara>It is also possible to control the size of a files written into HDFS. The <literal>--rollover</literal> option can be used to control when file currently being written is rolled over and a new file opened by providing the rollover size in bytes, kilobytes, megatypes, gigabytes, and terabytes.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream create --name myhdfsstream3 --definition "time | hdfs --rollover=100" --deploy
dataflow:&gt;stream destroy --name myhdfsstream3
dataflow:&gt;hadoop fs ls /xd/myhdfsstream3
Found 3 items
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-0.txt
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-1.txt
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-2.txt</literallayout>
<simpara>Shortcuts to specify sizes other than bytes are written as <literal>--rollover=64M</literal>, <literal>--rollover=512G</literal> or <literal>--rollover=1T</literal>.</simpara>
<simpara>The stream can also be compressed during the write operation. Example of this is shown below.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream create --name myhdfsstream4 --definition "time | hdfs --codec=gzip" --deploy
dataflow:&gt;stream destroy --name myhdfsstream4
dataflow:&gt;hadoop fs ls /xd/myhdfsstream4
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup         80 2013-12-18 18:48 /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip</literallayout>
<simpara>From a native os shell we can use hadoop&#8217;s fs commands and pipe data into gunzip.</simpara>
<literallayout class="monospaced"># bin/hadoop fs -cat /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip | gunzip
2013-12-18 18:48:10
2013-12-18 18:48:11
...</literallayout>
<simpara>Often a stream of data may not have a high enough rate to roll over files frequently, leaving the file in an opened state.  This prevents users from reading a consistent set of data when running mapreduce jobs.  While one can alleviate this problem by using a small rollover value, a better way is to use the <literal>idleTimeout</literal>  option that will automatically close the file if there was no writes during the specified period of time.   This feature is also useful in cases where burst of data is written into a stream and you&#8217;d like that data to become visible in HDFS.</simpara>
<note>
<simpara>The <literal>idleTimeout</literal> value should not exceed the timeout values set on the Hadoop cluster. These are typically configured using the <literal>dfs.socket.timeout</literal> and/or <literal>dfs.datanode.socket.write.timeout</literal> properties in the <literal>hdfs-site.xml</literal> configuration file.</simpara>
</note>
<literallayout class="monospaced">dataflow:&gt; stream create --name myhdfsstream5 --definition "http --server.port=8000 | hdfs --rollover=20 --idleTimeout=10000" --deploy</literallayout>
<simpara>In the above example we changed a source to <literal>http</literal> order to control what we write into a <literal>hdfs</literal> sink. We defined a small rollover size and a timeout of 10 seconds. Now we can simply post data into this stream via source end point using a below command.</simpara>
<literallayout class="monospaced">dataflow:&gt; http post --target http://localhost:8000 --data "hello"</literallayout>
<simpara>If we repeat the command very quickly and then wait for the timeout we should be able to see that some files are closed before rollover size was met and some were simply rolled because of a rollover size.</simpara>
<literallayout class="monospaced">dataflow:&gt;hadoop fs ls /xd/myhdfsstream5
Found 4 items
-rw-r--r--   3 jvalkealahti supergroup         12 2013-12-18 19:02 /xd/myhdfsstream5/myhdfsstream5-0.txt
-rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-1.txt
-rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-2.txt
-rw-r--r--   3 jvalkealahti supergroup         18 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-3.txt</literallayout>
<simpara>Files can be automatically partitioned using a <literal>partitionPath</literal> expression. If we create a stream with <literal>idleTimeout</literal> and <literal>partitionPath</literal> with simple format <literal>yyyy/MM/dd/HH/mm</literal> we should see writes ending into its own files within every minute boundary.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream create --name myhdfsstream6 --definition "time|hdfs --idleTimeout=10000 --partitionPath=dateFormat('yyyy/MM/dd/HH/mm')" --deploy</literallayout>
<simpara>Let a stream run for a short period of time and list files.</simpara>
<literallayout class="monospaced">dataflow:&gt;hadoop fs ls --recursive true --dir /xd/myhdfsstream6
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42
-rw-r--r--   3 jvalkealahti supergroup        140 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42/myhdfsstream6-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43
-rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43/myhdfsstream6-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44
-rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44/myhdfsstream6-0.txt</literallayout>
<simpara>Partitioning can also be based on defined lists. In a below example we simulate feeding data by using a <literal>time</literal> and a <literal>transform</literal> elements. Data passed to <literal>hdfs</literal> sink has a content <literal>APP0:foobar</literal>, <literal>APP1:foobar</literal>, <literal>APP2:foobar</literal> or <literal>APP3:foobar</literal>.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream create --name myhdfsstream7 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*3)+':foobar'\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),list(payload.split(':')[0],{{'0TO1','APP0','APP1'},{'2TO3','APP2','APP3'}}))" --deploy</literallayout>
<simpara>Let the stream run few seconds, destroy it and check what got written in those partitioned files.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream destroy --name myhdfsstream7
Destroyed stream 'myhdfsstream7'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list
-rw-r--r--   3 jvalkealahti supergroup        108 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list
-rw-r--r--   3 jvalkealahti supergroup        180 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list/myhdfsstream7-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
APP1:foobar
APP1:foobar
APP0:foobar
APP0:foobar
APP1:foobar</literallayout>
<simpara>Partitioning can also be based on defined ranges. In a below example we simulate feeding data by using a <literal>time</literal> and a <literal>transform</literal> elements. Data passed to <literal>hdfs</literal> sink has a content ranging from <literal>APP0</literal> to <literal>APP15</literal>. We simple parse the number part and use it to do a partition with ranges <literal>{3,5,10}</literal>.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream create --name myhdfsstream8 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*15)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),range(T(Integer).parseInt(payload.substring(3)),{3,5,10}))" --deploy</literallayout>
<simpara>Let the stream run few seconds, destroy it and check what got written in those partitioned files.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream destroy --name myhdfsstream8
Destroyed stream 'myhdfsstream8'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range
-rw-r--r--   3 jvalkealahti supergroup         16 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range
-rw-r--r--   3 jvalkealahti supergroup         35 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range
-rw-r--r--   3 jvalkealahti supergroup          5 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
APP3
APP3
APP1
APP0
APP1
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
APP4
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
APP6
APP15
APP7</literallayout>
<simpara>Partition using a <literal>dateFormat</literal> can be based on content itself. This is a good use case if old log files needs to be processed where partitioning should happen based on timestamp of a log entry. We create a fake log data with a simple date string ranging from <literal>1970-01-10</literal> to <literal>1970-01-13</literal>.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream create --name myhdfsstream9 --definition "time | transform --expression=\"'1970-01-'+1+T(Math).round(T(Math).random()*3)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH',payload,'yyyy-MM-DD'))" --deploy</literallayout>
<simpara>Let the stream run few seconds, destroy it and check what got written in those partitioned files. If you see the partition paths, those are based on year 1970, not present year.</simpara>
<literallayout class="monospaced">dataflow:&gt;stream destroy --name myhdfsstream9
Destroyed stream 'myhdfsstream9'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/10
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00
-rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/11
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00
-rw-r--r--   3 jvalkealahti supergroup         99 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/12
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00
-rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/13
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00
-rw-r--r--   3 jvalkealahti supergroup         55 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00/myhdfsstream9-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
1970-01-10
1970-01-10
1970-01-10
1970-01-10</literallayout>
<section xml:id="_options_21">
<title>Options</title>
<simpara>The <emphasis role="strong">hdfs</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>closeTimeout</term>
<listitem>
<simpara>timeout in ms, regardless of activity, after which file will be automatically closed <emphasis role="strong">(long, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>codec</term>
<listitem>
<simpara>compression codec alias name (gzip, snappy, bzip2, lzo, or slzo) <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>directory</term>
<listitem>
<simpara>where to output the files in the Hadoop FileSystem <emphasis role="strong">(String, default: <literal>/tmp/hdfs-sink</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fileExtension</term>
<listitem>
<simpara>the base filename extension to use for the created files <emphasis role="strong">(String, default: <literal>txt</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fileName</term>
<listitem>
<simpara>the base filename to use for the created files <emphasis role="strong">(String, default: <literal>&lt;stream name&gt;</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fileOpenAttempts</term>
<listitem>
<simpara>maximum number of file open attempts to find a path <emphasis role="strong">(int, default: <literal>10</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fileUuid</term>
<listitem>
<simpara>whether file name should contain uuid <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>fsUri</term>
<listitem>
<simpara>the URI to use to access the Hadoop FileSystem <emphasis role="strong">(String, default: <literal>${spring.hadoop.fsUri}</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>idleTimeout</term>
<listitem>
<simpara>inactivity timeout in ms after which file will be automatically closed <emphasis role="strong">(long, default: <literal>0</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>inUsePrefix</term>
<listitem>
<simpara>prefix for files currently being written <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>inUseSuffix</term>
<listitem>
<simpara>suffix for files currently being written <emphasis role="strong">(String, default: <literal>.tmp</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>overwrite</term>
<listitem>
<simpara>whether writer is allowed to overwrite files in Hadoop FileSystem <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>partitionPath</term>
<listitem>
<simpara>a SpEL expression defining the partition path <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>rollover</term>
<listitem>
<simpara>threshold in bytes when file will be automatically rolled over <emphasis role="strong">(String, default: <literal>1G</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>In the context of the <literal>fileOpenAttempts</literal> option, attempt is either one rollover request or failed stream open request for a path (if another writer came up with a same path and already opened it).</simpara>
</note>
</section>
<section xml:id="_partition_path_expression">
<title>Partition Path Expression</title>
<simpara>SpEL expression is evaluated against a Spring Messaging <literal>Message</literal> passed internally into a HDFS writer. This allows expression to use <literal>headers</literal> and <literal>payload</literal> from that message. While you could do a custom processing within a stream and add custom headers, <literal>timestamp</literal> is always going to be there. Data to be written is then available in a <literal>payload</literal>.</simpara>
<section xml:id="_accessing_properties">
<title>Accessing Properties</title>
<simpara>Using a <literal>payload</literal> simply returns whatever is currently being written. Access to headers is via <literal>headers</literal> property. Any other property is automatically resolved from headers if found. For example <literal>headers.timestamp</literal> is equivalent to <literal>timestamp</literal>.</simpara>
</section>
<section xml:id="_custom_methods">
<title>Custom Methods</title>
<simpara>Addition to a normal SpEL functionality, few custom methods has been added to make it easier to build partition paths. These custom methods can be used to work with a normal partition concepts like <literal>date formatting</literal>, <literal>lists</literal>, <literal>ranges</literal> and <literal>hashes</literal>.</simpara>
<section xml:id="_path">
<title>path</title>
<programlisting language="text" linenumbering="unnumbered">path(String... paths)</programlisting>
<simpara>Concatenates paths together with a delimiter <literal>/</literal>. This method can be used to make the expression less verbose than using a native SpEL functionality to combine path parts together. To create a path <literal>part1/part2</literal>, expression <literal>'part1' + '/' + 'part2'</literal> is equivalent to <literal>path('part1','part2')</literal>.</simpara>
<variablelist>
<title>Parameters</title>
<varlistentry>
<term>paths</term>
<listitem>
<simpara>Any number of path parts</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Return Value</title>
<para>Concatenated value of paths delimited with <literal>/</literal>.</para>
</formalpara>
</section>
<section xml:id="_dateformat">
<title>dateFormat</title>
<programlisting language="text" linenumbering="unnumbered">dateFormat(String pattern)
dateFormat(String pattern, Long epoch)
dateFormat(String pattern, Date date)
dateFormat(String pattern, String datestring)
dateFormat(String pattern, String datestring, String dateformat)</programlisting>
<simpara>Creates a path using date formatting. Internally this method delegates into <literal>SimpleDateFormat</literal> and needs a <literal>Date</literal> and a <literal>pattern</literal>. On default if no parameter used for conversion is given, <literal>timestamp</literal> is expected. Effectively <literal>dateFormat('yyyy')</literal> equals to <literal>dateFormat('yyyy', timestamp)</literal> or <literal>dateFormat('yyyy', headers.timestamp)</literal>.</simpara>
<simpara>Method signature with three parameters can be used to create a custom <literal>Date</literal> object which is then passed to <literal>SimpleDateFormat</literal> conversion using a <literal>dateformat</literal> pattern. This is useful in use cases where partition should be based on a date or time string found from a payload content itself. Default <literal>dateformat</literal> pattern if omitted is <literal>yyyy-MM-dd</literal>.</simpara>
<variablelist>
<title>Parameters</title>
<varlistentry>
<term>pattern</term>
<listitem>
<simpara>Pattern compatible with <literal>SimpleDateFormat</literal> to produce a final output.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>epoch</term>
<listitem>
<simpara>Timestamp as <literal>Long</literal> which is converted into a <literal>Date</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>date</term>
<listitem>
<simpara>A <literal>Date</literal> to be formatted.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>dateformat</term>
<listitem>
<simpara>Secondary pattern to convert <literal>datestring</literal> into a <literal>Date</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>datestring</term>
<listitem>
<simpara><literal>Date</literal> as a <literal>String</literal></simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Return Value</title>
<para>A path part representation which can be a simple file or directory name or a directory structure.</para>
</formalpara>
</section>
<section xml:id="_list">
<title>list</title>
<programlisting language="text" linenumbering="unnumbered">list(Object source, List&lt;List&lt;Object&gt;&gt; lists)</programlisting>
<simpara>Creates a partition path part by matching a <literal>source</literal> against a lists denoted by <literal>lists</literal>.</simpara>
<simpara>Lets assume that data is being written and it&#8217;s possible to extrace an <literal>appid</literal> either from headers or payload. We can automatically do a list based partition by using a partition method <literal>list(headers.appid,{{'1TO3','APP1','APP2','APP3'},{'4TO6','APP4','APP5','APP6'}})</literal>. This method would create three partitions, <literal>1TO3_list</literal>, <literal>4TO6_list</literal> and <literal>list</literal>. Latter is used if no match is found from partition lists passed to <literal>lists</literal>.</simpara>
<variablelist>
<title>Parameters</title>
<varlistentry>
<term>source</term>
<listitem>
<simpara>An <literal>Object</literal> to be matched against <literal>lists</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>lists</term>
<listitem>
<simpara>A definition of list of lists.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Return Value</title>
<para>A path part prefixed with a matched key i.e. <literal>XXX_list</literal> or <literal>list</literal> if no match.</para>
</formalpara>
</section>
<section xml:id="_range">
<title>range</title>
<programlisting language="text" linenumbering="unnumbered">range(Object source, List&lt;Object&gt; list)</programlisting>
<simpara>Creates a partition path part by matching a <literal>source</literal> against a list denoted by <literal>list</literal> using a simple binary search.</simpara>
<simpara>The partition method takes a <literal>source</literal> as first argument and <literal>list</literal> as a second argument. Behind the scenes this is using jvm’s <literal>binarySearch</literal> which works on an <literal>Object</literal> level so we can pass in anything. Remember that meaningful range match only works if passed in <literal>Object</literal> and types in list are of same type like <literal>Integer</literal>. Range is defined by a binarySearch itself so mostly it is to match against an upper bound except the last range in a list. Having a list of <literal>{1000,3000,5000}</literal> means that everything above 3000 will be matched with 5000. If that is an issue then simply adding <literal>Integer.MAX_VALUE</literal> as last range would overflow everything above 5000 into a new partition. Created partitions would then be <literal>1000_range</literal>, <literal>3000_range</literal> and <literal>5000_range</literal>.</simpara>
<variablelist>
<title>Parameters</title>
<varlistentry>
<term>source</term>
<listitem>
<simpara>An <literal>Object</literal> to be matched against <literal>list</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>list</term>
<listitem>
<simpara>A definition of list.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Return Value</title>
<para>A path part prefixed with a matched key i.e. <literal>XXX_range</literal>.</para>
</formalpara>
</section>
<section xml:id="_hash">
<title>hash</title>
<programlisting language="text" linenumbering="unnumbered">hash(Object source, int bucketcount)</programlisting>
<simpara>Creates a partition path part by calculating hashkey using <literal>source`s</literal> <literal>hashCode</literal> and <literal>bucketcount</literal>. Using a partition method <literal>hash(timestamp,2)</literal> would then create partitions named <literal>0_hash</literal>, <literal>1_hash</literal> and <literal>2_hash</literal>. Number suffixed with <literal>_hash</literal> is simply calculated using <literal>Object.hashCode() % bucketcount</literal>.</simpara>
<variablelist>
<title>Parameters</title>
<varlistentry>
<term>source</term>
<listitem>
<simpara>An <literal>Object</literal> which <literal>hashCode</literal> will be used.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>bucketcount</term>
<listitem>
<simpara>A number of buckets</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Return Value</title>
<para>A path part prefixed with a hash key i.e. <literal>XXX_hash</literal>.</para>
</formalpara>
</section>
</section>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-jdbc">
<title>JDBC (<literal>jdbc</literal>)</title>
<simpara>A module that writes its incoming payload to an RDBMS using JDBC.</simpara>
<section xml:id="_options_22">
<title>Options</title>
<simpara>The <emphasis role="strong">jdbc</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>expression</term>
<listitem>
<simpara>a SpEL expression used to transform messages <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>tableName</term>
<listitem>
<simpara>String <emphasis role="strong">(String, default: <literal>&lt;stream name</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>columns</term>
<listitem>
<simpara>the names of the columns that shall receive data, as a set of column[:SpEL] mappings, also used at initialization time to issue the DDL <emphasis role="strong">(String, default: <literal>payload</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>initialize</term>
<listitem>
<simpara>String <emphasis role="strong">(Boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>batchSize</term>
<listitem>
<simpara>String <emphasis role="strong">(long, default: <literal>10000</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The module also uses Spring Boot&#8217;s <link xlink:href="http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-sql.html#boot-features-configure-datasource">DataSource support</link>
for configuring the database connection, so properties like <literal>spring.datasource.url</literal> <emphasis>etc.</emphasis> apply.</simpara>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-log">
<title>Log (<literal>log</literal>)</title>
<simpara>Probably the simplest option for a sink is just to log the data. The <literal>log</literal> sink uses the application logger to output the data for inspection. The log level is set to <literal>WARN</literal> and the logger name is created from the stream name. To create a stream using a <literal>log</literal> sink you would use a command like</simpara>
<literallayout class="monospaced">dataflow:&gt; stream create --name mylogstream --definition "http --server.port=8000 | log" --deploy</literallayout>
<simpara>You can then try adding some data. We&#8217;ve used the <literal>http</literal> source on port 8000 here, so run the following command to send a message</simpara>
<literallayout class="monospaced">dataflow:&gt; http post --target http://localhost:8000 --data "hello"</literallayout>
<simpara>and you should see the following output in the Spring Cloud Data Flow console.</simpara>
<literallayout class="monospaced">13/06/07 16:12:18 INFO Received: hello</literallayout>
</section>
<section xml:id="spring-cloud-stream-modules-sink-rabbitmq">
<title>RabbitMQ</title>
<simpara>The "rabbit" sink enables outbound messaging over RabbitMQ.</simpara>
<section xml:id="_options_23">
<title>Options</title>
<simpara>The <emphasis role="strong">rabbit</emphasis> sink has the following options:</simpara>
<simpara>(See the Spring Boot documentation for RabbitMQ connection properties)</simpara>
<variablelist>
<varlistentry>
<term>converterBeanName</term>
<listitem>
<simpara>the bean name of the message converter <emphasis role="strong">(String, default: none)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>persistentDeliveryMode</term>
<listitem>
<simpara>the default delivery mode, true for persistent <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>exchange</term>
<listitem>
<simpara>the Exchange on the RabbitMQ broker to which messages should be sent <emphasis role="strong">(String, default: <literal>""</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>exchangeExpression</term>
<listitem>
<simpara>a SpEL expression that evaluates to the Exchange on the RabbitMQ broker to which messages
should be sent; overrides `exchange` <emphasis role="strong">(String, default: ``)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>mappedRequestHeaders</term>
<listitem>
<simpara>request message header names to be propagated to RabbitMQ, to limit to the set of standard headers plus `bar`, use `STANDARD_REQUEST_HEADERS,bar` <emphasis role="strong">(String, default: <literal>*</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>routingKey</term>
<listitem>
<simpara>the routing key to be passed with the message, as a SpEL expression <emphasis role="strong">(String, default: none)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>routingKeyExpression</term>
<listitem>
<simpara>an expression that evaluates to the routing key to be passed with the message, as a SpEL expression; overrides `routingKey` <emphasis role="strong">(String, default: none)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>By default, the message converter is a <literal>SimpleMessageConverter</literal> which handles <literal>byte[]</literal>, <literal>String</literal> and
<literal>java.io.Serializable</literal>.
A well-known bean name <literal>jsonConverter</literal> will configure a <literal>Jackson2JsonMessageConverter</literal> instead.
In addition, a custom converter bean can be added to the context and referenced by the converterBeanName property.</simpara>
</note>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-redis">
<title>Redis (<literal>redis</literal>)</title>
<simpara>The Redis sink can be used to ingest data into redis store. You can choose <literal>queue</literal>, <literal>topic</literal> or <literal>key</literal> with selcted
collection type to point to a specific data store.</simpara>
<simpara>For example,</simpara>
<screen>dataflow:&gt;stream create store-into-redis --definition "http | redis --queue=myList" --deploy
dataflow:&gt;Created and deployed new stream 'store-into-redis'</screen>
<section xml:id="_options_24">
<title>Options</title>
<simpara>The <emphasis role="strong">redis</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>topicExpression</term>
<listitem>
<simpara>a SpEL expression to use for topic <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>queueExpression</term>
<listitem>
<simpara>a SpEL expression to use for queue <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>keyExpression</term>
<listitem>
<simpara>a SpEL expression to use for keyExpression <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>key</term>
<listitem>
<simpara>name for the key <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>queue</term>
<listitem>
<simpara>name for the queue <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>topic</term>
<listitem>
<simpara>name for the topic <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-sink-router">
<title>Dynamic Router (<literal>router</literal>)</title>
<simpara>The Dynamic Router support allows for routing messages to <emphasis role="strong">named destinations</emphasis> based on the evaluation of a SpEL
expression or Groovy Script.</simpara>
<section xml:id="_spel_based_routing">
<title>SpEL-based Routing</title>
<simpara>The expression evaluates against the message and returns either a channel name, or the key to a map of channel names.</simpara>
<simpara>For more information, please see the "Routers and the Spring Expression Language (SpEL)" subsection in the Spring
Integration Reference manual
<link xlink:href="http://docs.spring.io/spring-integration/reference/html/messaging-routing-chapter.html#router-namespace">Configuring (Generic) Router section</link>.</simpara>
</section>
<section xml:id="_groovy_based_routing">
<title>Groovy-based Routing</title>
<simpara>Instead of SpEL expressions, Groovy scripts can also be used. Let&#8217;s create a Groovy script in the file system at
"file:/my/path/router.groovy", or "classpath:/my/path/router.groovy" :</simpara>
<programlisting language="groovy" linenumbering="unnumbered">println("Groovy processing payload '" + payload + "'");
if (payload.contains('a')) {
    return "foo"
}
else {
    return "bar"
}</programlisting>
<simpara>If you want to pass variable values to your script, you can statically bind values using the <emphasis>variables</emphasis> option or
optionally pass the path to a properties file containing the bindings using the <emphasis>propertiesLocation</emphasis> option.
All properties in the file will be made available to the script as variables. You may specify both <emphasis>variables</emphasis> and
<emphasis>propertiesLocation</emphasis>, in which case any duplicate values provided as <emphasis>variables</emphasis> override values provided in
<emphasis>propertiesLocation</emphasis>.
Note that <emphasis>payload</emphasis> and <emphasis>headers</emphasis> are implicitly bound to give you access to the data contained in a message.</simpara>
<simpara>For more information, see the Spring Integration Reference manual
<link xlink:href="http://docs.spring.io/spring-integration/reference/html/messaging-endpoints-chapter.html#groovy">Groovy Support</link>.</simpara>
</section>
<section xml:id="_options_25">
<title>Options</title>
<simpara>The <emphasis role="strong">router</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>destinations</term>
<listitem>
<simpara>comma-delimited destinations mapped from evaluation results <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>defaultOutputChannel</term>
<listitem>
<simpara>Where to route messages where the channel cannot be resolved <emphasis role="strong">(String, default: <literal>nullChannel</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>expression</term>
<listitem>
<simpara>a SpEL expression used to determine the destination <emphasis role="strong">(String, default: <literal>headers['routeTo']</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>propertiesLocation</term>
<listitem>
<simpara>the path of a properties file containing custom script variable bindings <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>refreshDelay</term>
<listitem>
<simpara>how often to check (in milliseconds) whether the script (if present) has changed; -1 for never <emphasis role="strong">(long, default: <literal>60000</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>script</term>
<listitem>
<simpara>reference to a script used to process messages <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>destinationMappings</term>
<listitem>
<simpara>Destination mappings as a new line delimited string of name-value pairs, e.g. 'foo=bar\n baz=car'. <emphasis role="strong">(String, no default)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>Since this is a dynamic router, destinations are created as needed; therefore, by default the <literal>defaultOutputChannel</literal>
and <literal>resolutionRequired</literal> will only be used if the <literal>Binder</literal> has some problem binding to the destination.</simpara>
</note>
<simpara>You can restrict the creation of dynamic bindings using the <literal>spring.cloud.stream.dynamicDestinations</literal> property.
By default, all resolved destinations will be bound dynamically; if this property has a comma-delimited list of
destination names, only those will be bound.
Messages that resolve to a destination that is not in this list will be routed to the <literal>defaultOutputChannel</literal>, which
must also appear in the list.</simpara>
<simpara><literal>destinationMappings</literal> are used to map the evaluation results to an actual destination name.</simpara>
</section>
</section>
<section xml:id="spring-cloud-stream-modules-sink-tcp">
<title>TCP Sink</title>
<simpara>The TCP Sink provides for outbound messaging over TCP; messages sent to the sink can have <literal>String</literal> or <literal>byte[]</literal> payloads.</simpara>
<simpara>TCP is a streaming protocol and some mechanism is needed to frame messages on the wire. A number of encoders are
available, the default being 'CRLF'.</simpara>
<section xml:id="_options_26">
<title>Options</title>
<simpara>The <emphasis role="strong">tcp</emphasis> sink has the following options:</simpara>
<variablelist>
<varlistentry>
<term>charset</term>
<listitem>
<simpara>the charset used when converting from String to bytes <emphasis role="strong">(String, default: <literal>UTF-8</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>close</term>
<listitem>
<simpara>whether to close the socket after each message <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>encoder</term>
<listitem>
<simpara>the encoder to use when sending messages <emphasis role="strong">(Encoding, default: <literal>CRLF</literal>, possible values: <literal>CRLF,LF,NULL,STXETX,RAW,L1,L2,L4</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>host</term>
<listitem>
<simpara>the remote host to connect to <emphasis role="strong">(String, default: <literal>localhost</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>nio</term>
<listitem>
<simpara>whether or not to use NIO <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>port</term>
<listitem>
<simpara>the port on the remote host to connect to <emphasis role="strong">(int, default: <literal>1234</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>reverseLookup</term>
<listitem>
<simpara>perform a reverse DNS lookup on the remote IP Address <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>socketTimeout</term>
<listitem>
<simpara>the timeout (ms) before closing the socket when no data is received <emphasis role="strong">(int, default: <literal>120000</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>useDirectBuffers</term>
<listitem>
<simpara>whether or not to use direct buffers <emphasis role="strong">(boolean, default: <literal>false</literal>)</emphasis></simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="_available_encoders">
<title>Available Encoders</title>
<variablelist>
<title>Text Data</title>
<varlistentry>
<term>CRLF (default)</term>
<listitem>
<simpara>text terminated by carriage return (0x0d) followed by line feed (0x0a)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>LF</term>
<listitem>
<simpara>text terminated by line feed (0x0a)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>NULL</term>
<listitem>
<simpara>text terminated by a null byte (0x00)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>STXETX</term>
<listitem>
<simpara>text preceded by an STX (0x02) and terminated by an ETX (0x03)</simpara>
</listitem>
</varlistentry>
</variablelist>
<variablelist>
<title>Text and Binary Data</title>
<varlistentry>
<term>RAW</term>
<listitem>
<simpara>no structure - the client indicates a complete message by closing the socket</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>L1</term>
<listitem>
<simpara>data preceded by a one byte (unsigned) length field (supports up to 255 bytes)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>L2</term>
<listitem>
<simpara>data preceded by a two byte (unsigned) length field (up to 2<superscript>16</superscript>-1 bytes)</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>L4</term>
<listitem>
<simpara>data preceded by a four byte (signed) length field (up to 2<superscript>31</superscript>-1 bytes)</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
</chapter>
</part>
<part xml:id="_appendices">
<title>Appendices</title>
<appendix xml:id="building">
<title>Building</title>
<section xml:id="_basic_compile_and_test">
<title>Basic Compile and Test</title>
<simpara>To build the source you will need to install JDK 1.7.</simpara>
<simpara>The build uses the Maven wrapper so you don&#8217;t have to install a specific
version of Maven.  To enable the tests for Redis you should run the server
before bulding.  See below for more information on how run Redis.</simpara>
<simpara>The main build command is</simpara>
<screen>$ ./mvnw clean install</screen>
<simpara>You can also add '-DskipTests' if you like, to avoid running the tests.</simpara>
<note>
<simpara>You can also install Maven (&gt;=3.3.3) yourself and run the <literal>mvn</literal> command
in place of <literal>./mvnw</literal> in the examples below. If you do that you also
might need to add <literal>-P spring</literal> if your local Maven settings do not
contain repository declarations for spring pre-release artifacts.</simpara>
</note>
<note>
<simpara>Be aware that you might need to increase the amount of memory
available to Maven by setting a <literal>MAVEN_OPTS</literal> environment variable with
a value like <literal>-Xmx512m -XX:MaxPermSize=128m</literal>. We try to cover this in
the <literal>.mvn</literal> configuration, so if you find you have to do it to make a
build succeed, please raise a ticket to get the settings added to
source control.</simpara>
</note>
<simpara>The projects that require middleware generally include a
<literal>docker-compose.yml</literal>, so consider using
<link xlink:href="http://compose.docker.io/">Docker Compose</link> to run the middeware servers
in Docker containers. See the README in the
<link xlink:href="https://github.com/spring-cloud-samples/scripts">scripts demo
repository</link> for specific instructions about the common cases of mongo,
rabbit and redis.</simpara>
</section>
<section xml:id="_documentation">
<title>Documentation</title>
<simpara>There is a "full" profile that will generate documentation.  You can build just the documentation by executing</simpara>
<screen>$ ./mvnw package -DskipTests=true -P full -pl spring-cloud-stream-modules-docs -am</screen>
</section>
<section xml:id="_working_with_the_code">
<title>Working with the code</title>
<simpara>If you don&#8217;t have an IDE preference we would recommend that you use
<link xlink:href="http://www.springsource.com/developer/sts">Spring Tools Suite</link> or
<link xlink:href="http://eclipse.org">Eclipse</link> when working with the code. We use the
<link xlink:href="http://eclipse.org/m2e/">m2eclipe</link> eclipse plugin for maven support. Other IDEs and tools
should also work without issue.</simpara>
<section xml:id="_importing_into_eclipse_with_m2eclipse">
<title>Importing into eclipse with m2eclipse</title>
<simpara>We recommend the <link xlink:href="http://eclipse.org/m2e/">m2eclipe</link> eclipse plugin when working with
eclipse. If you don&#8217;t already have m2eclipse installed it is available from the "eclipse
marketplace".</simpara>
<simpara>Unfortunately m2e does not yet support Maven 3.3, so once the projects
are imported into Eclipse you will also need to tell m2eclipse to use
the <literal>.settings.xml</literal> file for the projects.  If you do not do this you
may see many different errors related to the POMs in the
projects.  Open your Eclipse preferences, expand the Maven
preferences, and select User Settings.  In the User Settings field
click Browse and navigate to the Spring Cloud project you imported
selecting the <literal>.settings.xml</literal> file in that project.  Click Apply and
then OK to save the preference changes.</simpara>
<note>
<simpara>Alternatively you can copy the repository settings from <link xlink:href="https://github.com/spring-cloud/spring-cloud-build/blob/master/.settings.xml"><literal>.settings.xml</literal></link> into your own <literal>~/.m2/settings.xml</literal>.</simpara>
</note>
</section>
<section xml:id="_importing_into_eclipse_without_m2eclipse">
<title>Importing into eclipse without m2eclipse</title>
<simpara>If you prefer not to use m2eclipse you can generate eclipse project metadata using the
following command:</simpara>
<screen>$ ./mvnw eclipse:eclipse</screen>
<simpara>The generated eclipse projects can be imported by selecting <literal>import existing projects</literal>
from the <literal>file</literal> menu.</simpara>
</section>
</section>
</appendix>
<chapter xml:id="contributing">
<title>Contributing</title>
<simpara>Spring Cloud is released under the non-restrictive Apache 2.0 license,
and follows a very standard Github development process, using Github
tracker for issues and merging pull requests into master. If you want
to contribute even something trivial please do not hesitate, but
follow the guidelines below.</simpara>
<section xml:id="_sign_the_contributor_license_agreement">
<title>Sign the Contributor License Agreement</title>
<simpara>Before we accept a non-trivial patch or pull request we will need you to sign the
<link xlink:href="https://support.springsource.com/spring_committer_signup">contributor&#8217;s agreement</link>.
Signing the contributor&#8217;s agreement does not grant anyone commit rights to the main
repository, but it does mean that we can accept your contributions, and you will get an
author credit if we do.  Active contributors might be asked to join the core team, and
given the ability to merge pull requests.</simpara>
</section>
<section xml:id="_code_conventions_and_housekeeping">
<title>Code Conventions and Housekeeping</title>
<simpara>None of these is essential for a pull request, but they will all help.  They can also be
added after the original pull request but before a merge.</simpara>
<itemizedlist>
<listitem>
<simpara>Use the Spring Framework code format conventions. If you use Eclipse
you can import formatter settings using the
<literal>eclipse-code-formatter.xml</literal> file from the
<link xlink:href="https://github.com/spring-cloud/build/tree/master/eclipse-coding-conventions.xml">Spring
Cloud Build</link> project. If using IntelliJ, you can use the
<link xlink:href="http://plugins.jetbrains.com/plugin/6546">Eclipse Code Formatter
Plugin</link> to import the same file.</simpara>
</listitem>
<listitem>
<simpara>Make sure all new <literal>.java</literal> files to have a simple Javadoc class comment with at least an
<literal>@author</literal> tag identifying you, and preferably at least a paragraph on what the class is
for.</simpara>
</listitem>
<listitem>
<simpara>Add the ASF license header comment to all new <literal>.java</literal> files (copy from existing files
in the project)</simpara>
</listitem>
<listitem>
<simpara>Add yourself as an <literal>@author</literal> to the .java files that you modify substantially (more
than cosmetic changes).</simpara>
</listitem>
<listitem>
<simpara>Add some Javadocs and, if you change the namespace, some XSD doc elements.</simpara>
</listitem>
<listitem>
<simpara>A few unit tests would help a lot as well&#8201;&#8212;&#8201;someone has to do it.</simpara>
</listitem>
<listitem>
<simpara>If no-one else is using your branch, please rebase it against the current master (or
other target branch in the main project).</simpara>
</listitem>
<listitem>
<simpara>When writing a commit message please follow <link xlink:href="http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html">these conventions</link>,
if you are fixing an existing issue please add <literal>Fixes gh-XXXX</literal> at the end of the commit
message (where XXXX is the issue number).</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
</part>
</book>