<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Spring Cloud Stream Reference Guide</title><link rel="stylesheet" type="text/css" href="css/manual-singlepage.css"><meta name="generator" content="DocBook XSL Stylesheets V1.78.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div lang="en" class="book"><div class="titlepage"><div><div><h1 class="title"><a name="d0e3"></a>Spring Cloud Stream Reference Guide</h1></div><div><span xmlns:d="http://docbook.org/ns/docbook" class="author"><span class="firstname">Sabby Anandan, Artem Bilan, Marius Bogoevici, Eric Bottard, Mark Fisher, Ilayaperumal Gopinathan, Gunnar Hillert, Mark Pollack, Patrick Peralta, Glenn Renfro, Gary Russell, Thomas Risberg, David Turanski, Janne Valkealahti</span></span></div><div><p class="releaseinfo">1.0.0.BUILD-SNAPSHOT</p></div><div><p class="copyright">Copyright &copy; 2013-2016 Pivotal Software, Inc.</p></div><div><div class="legalnotice"><a name="d0e26" href="#d0e26"></a><p>
		Copies of this document may be made for your own use and for distribution to
		others, provided that you do not charge any fee for such copies and further
		provided that each copy contains this Copyright Notice, whether distributed in
		print or electronically.
	</p></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl class="toc"><dt><span class="part"><a href="#_reference_guide">I. Reference Guide</a></span></dt><dd><dl><dt><span class="chapter"><a href="#overview">1. Spring Cloud Stream Modules</a></span></dt><dd><dl><dt><span class="section"><a href="#_overview">1.1. Overview</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#_modules">II. Modules</a></span></dt><dd><dl><dt><span class="chapter"><a href="#sources">2. Sources</a></span></dt><dd><dl><dt><span class="section"><a href="#spring-cloud-stream-modules-file-source">2.1. File Source</a></span></dt><dd><dl><dt><span class="section"><a href="#_options">2.1.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-ftp-source">2.2. FTP Source</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_2">2.2.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-http-source">2.3. Http Source</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_3">2.3.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-jdbc-source">2.4. JDBC Source</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_4">2.4.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#jms">2.5. JMS</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_5">2.5.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-load-generator">2.6. Load Generator (<code class="literal">load-generator</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_6">2.6.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-source-rabbit">2.7. RabbitMQ</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_7">2.7.1. Options</a></span></dt><dt><span class="section"><a href="#rabbitSourceRetry">2.7.2. A Note About Retry</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-sftp">2.8. SFTP (<code class="literal">sftp</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_8">2.8.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-clound-stream-modules-source-syslog">2.9. SYSLOG</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_9">2.9.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#_tcp">2.10. TCP</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_10">2.10.1. Options</a></span></dt><dt><span class="section"><a href="#_available_decoders">2.10.2. Available Decoders</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-time">2.11. Time (<code class="literal">time</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_11">2.11.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-twitterstream">2.12. Twitter Stream (<code class="literal">twitterstream</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_12">2.12.1. Options</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#spring-cloud-stream-modules-processors">3. Processors</a></span></dt><dd><dl><dt><span class="section"><a href="#spring-cloud-stream-modules-filter">3.1. Filter (<code class="literal">filter</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_filter_with_spel_expression">3.1.1. Filter with SpEL expression</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-groovy-filter">3.2. Groovy Filter (<code class="literal">groovy-filter</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_13">3.2.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-httpclient">3.3. Http Client (<code class="literal">httpclient</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_14">3.3.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-bridge">3.4. Bridge (<code class="literal">bridge</code>)</a></span></dt><dt><span class="section"><a href="#spring-cloud-stream-modules-groovy-transform">3.5. Groovy Transform (<code class="literal">groovy-transform</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_15">3.5.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-transform">3.6. Transform (<code class="literal">transform</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_16">3.6.1. Options</a></span></dt><dt><span class="section"><a href="#_transform_with_spel_expression">3.6.2. Transform with SpEL expression</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-splitter">3.7. Splitter</a></span></dt><dd><dl><dt><span class="section"><a href="#_json_example">3.7.1. JSON Example</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#spring-cloud-stream-modules-sinks">4. Sinks</a></span></dt><dd><dl><dt><span class="section"><a href="#spring-cloud-stream-modules-cassandra">4.1. Cassandra (<code class="literal">cassandra</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_17">4.1.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-counter">4.2. Counter (<code class="literal">counter</code>)</a></span></dt><dt><span class="section"><a href="#spring-cloud-stream-modules-field-value-counter">4.3. Field Value Counter (<code class="literal">field-value-counter</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_18">4.3.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-file-sink">4.4. File (<code class="literal">file</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_19">4.4.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#ftp-sink">4.5. FTP Sink (<code class="literal">ftp</code>)</a></span></dt><dt><span class="section"><a href="#spring-cloud-stream-modules-gemfire-sink">4.6. Gemfire (<code class="literal">gemfire</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_20">4.6.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-hdfs">4.7. Hadoop (HDFS) (<code class="literal">hdfs</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_21">4.7.1. Options</a></span></dt><dt><span class="section"><a href="#_partition_path_expression">4.7.2. Partition Path Expression</a></span></dt><dd><dl><dt><span class="section"><a href="#_accessing_properties">Accessing Properties</a></span></dt><dt><span class="section"><a href="#_custom_methods">Custom Methods</a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-jdbc">4.8. JDBC (<code class="literal">jdbc</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_22">4.8.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-log">4.9. Log (<code class="literal">log</code>)</a></span></dt><dt><span class="section"><a href="#spring-cloud-stream-modules-sink-rabbitmq">4.10. RabbitMQ</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_23">4.10.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-redis">4.11. Redis (<code class="literal">redis</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_24">4.11.1. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-sink-router">4.12. Dynamic Router (<code class="literal">router</code>)</a></span></dt><dd><dl><dt><span class="section"><a href="#_spel_based_routing">4.12.1. SpEL-based Routing</a></span></dt><dt><span class="section"><a href="#_groovy_based_routing">4.12.2. Groovy-based Routing</a></span></dt><dt><span class="section"><a href="#_options_25">4.12.3. Options</a></span></dt></dl></dd><dt><span class="section"><a href="#spring-cloud-stream-modules-sink-tcp">4.13. TCP Sink</a></span></dt><dd><dl><dt><span class="section"><a href="#_options_26">4.13.1. Options</a></span></dt><dt><span class="section"><a href="#_available_encoders">4.13.2. Available Encoders</a></span></dt></dl></dd></dl></dd></dl></dd><dt><span class="part"><a href="#_appendices">III. Appendices</a></span></dt><dd><dl><dt><span class="appendix"><a href="#building">A. Building</a></span></dt><dd><dl><dt><span class="section"><a href="#_basic_compile_and_test">A.1. Basic Compile and Test</a></span></dt><dt><span class="section"><a href="#_documentation">A.2. Documentation</a></span></dt><dt><span class="section"><a href="#_working_with_the_code">A.3. Working with the code</a></span></dt><dd><dl><dt><span class="section"><a href="#_importing_into_eclipse_with_m2eclipse">A.3.1. Importing into eclipse with m2eclipse</a></span></dt><dt><span class="section"><a href="#_importing_into_eclipse_without_m2eclipse">A.3.2. Importing into eclipse without m2eclipse</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#contributing">5. Contributing</a></span></dt><dd><dl><dt><span class="section"><a href="#_sign_the_contributor_license_agreement">5.1. Sign the Contributor License Agreement</a></span></dt><dt><span class="section"><a href="#_code_conventions_and_housekeeping">5.2. Code Conventions and Housekeeping</a></span></dt></dl></dd></dl></dd></dl></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="_reference_guide" href="#_reference_guide"></a>Part&nbsp;I.&nbsp;Reference Guide</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="overview" href="#overview"></a>1.&nbsp;Spring Cloud Stream Modules</h2></div></div></div><p>This section goes into more detail about how you can work with Spring Cloud Stream Module as standalone applications or with Spring Cloud Data Flow.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_overview" href="#_overview"></a>1.1&nbsp;Overview</h2></div></div></div><p>TBD</p></div></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="_modules" href="#_modules"></a>Part&nbsp;II.&nbsp;Modules</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="sources" href="#sources"></a>2.&nbsp;Sources</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-file-source" href="#spring-cloud-stream-modules-file-source"></a>2.1&nbsp;File Source</h2></div></div></div><p>This application polls a directory and sends new files or their contents to the output channel.
The file source provides the contents of a File as a byte array by default.
However, this can be customized using the <code class="literal">--mode</code> option:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><span class="strong"><strong>ref</strong></span> Provides a <code class="literal">java.io.File</code> reference</li><li class="listitem"><span class="strong"><strong>lines</strong></span> Will split files line-by-line and emit a new message for each line</li><li class="listitem"><span class="strong"><strong>contents</strong></span> The default. Provides the contents of a file as a byte array</li></ul></div><p>When using <code class="literal">--mode=lines</code>, you can also provide the additional option <code class="literal">--withMarkers=true</code>.
If set to <code class="literal">true</code>, the underlying <code class="literal">FileSplitter</code> will emit additional <span class="emphasis"><em>start-of-file</em></span> and <span class="emphasis"><em>end-of-file</em></span> marker messages before and after the actual data.
The payload of these 2 additional marker messages is of type <code class="literal">FileSplitter.FileMarker</code>. The option <code class="literal">withMarkers</code> defaults to <code class="literal">false</code> if not explicitly set.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options" href="#_options"></a>2.1.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>file</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">dir</span></dt><dd>the absolute path to the directory to monitor for files <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">fixedDelay</span></dt><dd>the fixed delay polling interval specified in seconds <span class="strong"><strong>(int, default: <code class="literal">5</code>)</strong></span></dd><dt><span class="term">initialDelay</span></dt><dd>an initial delay when using a fixed delay trigger, expressed in TimeUnits (seconds by default) <span class="strong"><strong>(int, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">maxMessages</span></dt><dd>the maximum messages per poll; -1 for unlimited <span class="strong"><strong>(long, default: <code class="literal">-1</code>)</strong></span></dd><dt><span class="term">mode</span></dt><dd>specifies how the file is being read. By default the content of a file is provided as byte array <span class="strong"><strong>(FileReadingMode, default: <code class="literal">contents</code>, possible values: <code class="literal">ref,lines,contents</code>)</strong></span></dd><dt><span class="term">pattern</span></dt><dd>a filter expression (Ant style) to accept only files that match the pattern <span class="strong"><strong>(String, default: * )</strong></span></dd><dt><span class="term">preventDuplicates</span></dt><dd>whether to prevent the same file from being processed twice <span class="strong"><strong>(boolean, default: <code class="literal">true</code>)</strong></span></dd><dt><span class="term">timeUnit</span></dt><dd>the time unit for the fixed and initial delays <span class="strong"><strong>(String, default: <code class="literal">SECONDS</code>)</strong></span></dd><dt><span class="term">withMarkers</span></dt><dd>if true emits start of file/end of file marker messages before/after the data. Only valid with FileReadingMode 'lines' <span class="strong"><strong>(Boolean, no default)</strong></span></dd></dl></div><p>The <code class="literal">ref</code> option is useful in some cases in which the file contents are large and it would be more efficient to send the file path.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-ftp-source" href="#spring-cloud-stream-modules-ftp-source"></a>2.2&nbsp;FTP Source</h2></div></div></div><p>This source application supports transfer of files using the FTP protocol.
Files are transferred from the <code class="literal">remote</code> directory to the <code class="literal">local</code> directory where the app is deployed.
Messages emitted by the source are provided as a byte array by default. However, this can be
customized using the <code class="literal">--mode</code> option:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><span class="strong"><strong>ref</strong></span> Provides a <code class="literal">java.io.File</code> reference</li><li class="listitem"><span class="strong"><strong>lines</strong></span> Will split files line-by-line and emit a new message for each line</li><li class="listitem"><span class="strong"><strong>contents</strong></span> The default. Provides the contents of a file as a byte array</li></ul></div><p>When using <code class="literal">--mode=lines</code>, you can also provide the additional option <code class="literal">--withMarkers=true</code>.
If set to <code class="literal">true</code>, the underlying <code class="literal">FileSplitter</code> will emit additional <span class="emphasis"><em>start-of-file</em></span> and <span class="emphasis"><em>end-of-file</em></span> marker messages before and after the actual data.
The payload of these 2 additional marker messages is of type <code class="literal">FileSplitter.FileMarker</code>. The option <code class="literal">withMarkers</code> defaults to <code class="literal">false</code> if not explicitly set.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_2" href="#_options_2"></a>2.2.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>ftp</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">autoCreateLocalDir</span></dt><dd>local directory must be auto created if it does not exist <span class="strong"><strong>(boolean, default: <code class="literal">true</code>)</strong></span></dd><dt><span class="term">clientMode</span></dt><dd>client mode to use : 2 for passive mode and 0 for active mode <span class="strong"><strong>(int, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">deleteRemoteFiles</span></dt><dd>delete remote files after transfer <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">filenamePattern</span></dt><dd>simple filename pattern to apply to the filter <span class="strong"><strong>(String, default: *)</strong></span></dd><dt><span class="term">fixedDelay</span></dt><dd>the rate at which to poll the remote directory <span class="strong"><strong>(int, default: <code class="literal">1</code>)</strong></span></dd><dt><span class="term">host</span></dt><dd>the host name for the FTP server <span class="strong"><strong>(String, default: <code class="literal">localhost</code>)</strong></span></dd><dt><span class="term">initialDelay</span></dt><dd>an initial delay when using a fixed delay trigger, expressed in TimeUnits (seconds by default) <span class="strong"><strong>(int, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">localDir</span></dt><dd>set the local directory the remote files are transferred to <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">maxMessages</span></dt><dd>the maximum messages per poll; -1 for unlimited <span class="strong"><strong>(long, default: <code class="literal">-1</code>)</strong></span></dd><dt><span class="term">mode</span></dt><dd>specifies how the file is being read. By default the content of a file is provided as byte array <span class="strong"><strong>(FileReadingMode, default: <code class="literal">contents</code>, possible values: <code class="literal">ref,lines,contents</code>)</strong></span></dd><dt><span class="term">password</span></dt><dd>the password for the FTP connection <span class="strong"><strong>(Password, no default)</strong></span></dd><dt><span class="term">port</span></dt><dd>the port for the FTP server <span class="strong"><strong>(int, default: <code class="literal">21</code>)</strong></span></dd><dt><span class="term">preserveTimestamp</span></dt><dd>whether to preserve the timestamp of files retrieved <span class="strong"><strong>(boolean, default: <code class="literal">true</code>)</strong></span></dd><dt><span class="term">remoteDir</span></dt><dd>the remote directory to transfer the files from <span class="strong"><strong>(String, default: <code class="literal">/</code>)</strong></span></dd><dt><span class="term">remoteFileSeparator</span></dt><dd>file separator to use on the remote side <span class="strong"><strong>(String, default: <code class="literal">/</code>)</strong></span></dd><dt><span class="term">timeUnit</span></dt><dd>the time unit for the fixed and initial delays <span class="strong"><strong>(String, default: <code class="literal">SECONDS</code>)</strong></span></dd><dt><span class="term">tmpFileSuffix</span></dt><dd>extension to use when downloading files <span class="strong"><strong>(String, default: <code class="literal">.tmp</code>)</strong></span></dd><dt><span class="term">username</span></dt><dd>the username for the FTP connection <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">withMarkers</span></dt><dd>if true emits start of file/end of file marker messages before/after the data. Only valid with FileReadingMode 'lines' <span class="strong"><strong>(Boolean, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-http-source" href="#spring-cloud-stream-modules-http-source"></a>2.3&nbsp;Http Source</h2></div></div></div><p>A source module that listens for HTTP requests and emits the body as a message payload.
If the Content-Type matches <code class="literal">text/*</code> or <code class="literal">application/json</code>, the payload will be a String,
otherwise the payload will be a byte array.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_3" href="#_options_3"></a>2.3.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>http</strong></span> source supports the following configuration properties:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">pathPattern</span></dt><dd>An Ant-Style pattern to determine which http requests will be captured <span class="strong"><strong>(String, default: <code class="literal">/</code>)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-jdbc-source" href="#spring-cloud-stream-modules-jdbc-source"></a>2.4&nbsp;JDBC Source</h2></div></div></div><p>This source polls data from an RDBMS.
This source is fully based on the <code class="literal">DataSourceAutoConfiguration</code>, so refer to the
<a class="link" href="http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-sql.html" target="_top">Spring Boot JDBC Support</a> for more
information.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_4" href="#_options_4"></a>2.4.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>jdbc</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">query</span></dt><dd>the query to use to select data <span class="strong"><strong>(String, no default, required)</strong></span></dd><dt><span class="term">update</span></dt><dd>an SQL update statement to execute for marking polled messages as 'seen' <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">split</span></dt><dd>whether to split the SQL result as individual messages <span class="strong"><strong>(boolean, default: <code class="literal">true</code>)</strong></span></dd><dt><span class="term">$maxRowsPerPoll$$</span></dt><dd>max numbers of rows to process for each poll <span class="strong"><strong>(int, default: <code class="literal">0</code>)</strong></span></dd></dl></div><p>Also see the <a class="link" href="http://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html" target="_top">Spring Boot Documentation</a>
for addition <code class="literal">DataSource</code> properties and <code class="literal">TriggerProperties</code> and <code class="literal">MaxMessagesProperties</code> for polling options.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="jms" href="#jms"></a>2.5&nbsp;JMS</h2></div></div></div><p>The "jms" source enables receiving messages from JMS.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_5" href="#_options_5"></a>2.5.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>jms</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">spring.jms.listener.acknowledgeMode</span></dt><dd>the session acknowledge mode <span class="strong"><strong>(String, default: <code class="literal">AUTO</code>)</strong></span></dd><dt><span class="term">clientId</span></dt><dd>an identifier for the client, to be associated with a durable or shared topic subscription <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">destination</span></dt><dd>the destination name from which messages will be received <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">messageSelector</span></dt><dd>a message selector to be applied to messages <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">subscriptionDurable</span></dt><dd>when true, indicates the subscription to a topic is durable <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">subscriptionShared</span></dt><dd>when true, indicates the subscription to a topic is shared (JMS 2.0) <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">spring.jms.pubSubDomain</span></dt><dd>when true, indicates that the destination is a topic <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">subscriptionName</span></dt><dd>a name that will be assigned to the topic subscription <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">sessionTransacted</span></dt><dd>True to enable transactions and use a `DefaultMessageListenerContainer`, false to select a
`SimpleMessageListenerContainer` <span class="strong"><strong>(String, default: true)</strong></span></dd><dt><span class="term">spring.jms.listener.concurrency</span></dt><dd>The minimum number of consumer threads. *(Integer, default: 1)</dd><dt><span class="term">spring.jms.listener.maxConcurrency</span></dt><dd>The maximum number of consumer threads. Only supported when `sessionTransacted ` is true *(Integer, default: 1)</dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Spring boot broker configuration is used; refer to the
<a class="link" href="http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#boot-features-jms" target="_top">Spring Boot Documentation</a> for more information.
The <code class="literal">spring.jms.*</code> properties above are also handled by the boot JMS support.</p></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-load-generator" href="#spring-cloud-stream-modules-load-generator"></a>2.6&nbsp;Load Generator (<code class="literal">load-generator</code>)</h2></div></div></div><p>A source that sends generated data and dispatches it to the stream. This is to provide a method for users to identify the performance of Spring Cloud Data Flow in different environments and deployment types.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_6" href="#_options_6"></a>2.6.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>load-generator</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">messageCount</span></dt><dd>the number of messages to send <span class="strong"><strong>(Integer, default: <code class="literal">100</code>)</strong></span></dd><dt><span class="term">messageSize</span></dt><dd>the size of message to send <span class="strong"><strong>(Integer, <code class="literal">1000</code>)</strong></span></dd><dt><span class="term">producers</span></dt><dd>the number of producers <span class="strong"><strong>(Integer, <code class="literal">1</code>)</strong></span></dd><dt><span class="term">outputType</span></dt><dd>how this module should emit messages it produces <span class="strong"><strong>(MimeType, default: no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-source-rabbit" href="#spring-cloud-stream-modules-source-rabbit"></a>2.7&nbsp;RabbitMQ</h2></div></div></div><p>The "rabbit" source enables receiving messages from RabbitMQ.</p><p>The queue(s) must exist before the stream is deployed; they are not created automatically.
You can easily create a Queue using the RabbitMQ web UI.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_7" href="#_options_7"></a>2.7.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>rabbit</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">enableRetry</span></dt><dd>enable retry; when retries are exhausted the message will be rejected; message disposition will depend on dead letter configuration <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">initialRetryInterval</span></dt><dd>initial interval between retries <span class="strong"><strong>(int, default: <code class="literal">1000</code>)</strong></span></dd><dt><span class="term">mappedRequestHeaders</span></dt><dd>request message header names to be mapped from the incoming message <span class="strong"><strong>(String, default: <code class="literal">STANDARD_REQUEST_HEADERS</code>)</strong></span></dd><dt><span class="term">maxAttempts</span></dt><dd>maximum delivery attempts <span class="strong"><strong>(int, default: <code class="literal">3</code>)</strong></span></dd><dt><span class="term">maxConcurrency</span></dt><dd>the maximum number of consumers <span class="strong"><strong>(int, default: <code class="literal">1</code>)</strong></span></dd><dt><span class="term">maxRetryInterval</span></dt><dd>maximum retry interval <span class="strong"><strong>(int, default: <code class="literal">30000</code>)</strong></span></dd><dt><span class="term">queues</span></dt><dd>the queue(s) from which messages will be received <span class="strong"><strong>(String, default: no default)</strong></span></dd><dt><span class="term">requeue</span></dt><dd>whether rejected messages will be requeued by default <span class="strong"><strong>(boolean, default: <code class="literal">true</code>)</strong></span></dd><dt><span class="term">retryMultiplier</span></dt><dd>retry interval multiplier <span class="strong"><strong>(double, default: <code class="literal">2.0</code>)</strong></span></dd><dt><span class="term">transacted</span></dt><dd>true if the channel is to be transacted <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd></dl></div><p>Also see the <a class="link" href="http://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html" target="_top">Spring Boot Documentation</a>
for addition properties for the broker connections and listener properties.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="rabbitSourceRetry" href="#rabbitSourceRetry"></a>2.7.2&nbsp;A Note About Retry</h3></div></div></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>With the default <span class="emphasis"><em>ackMode</em></span> (<span class="strong"><strong>AUTO</strong></span>) and <span class="emphasis"><em>requeue</em></span> (<span class="strong"><strong>true</strong></span>) options, failed message deliveries will be retried
indefinitely.
Since there is not much processing in the rabbit source, the risk of failure in the source itself is small, unless
the downstream <code class="literal">Binder</code> is not connected for some reason.
Setting <span class="emphasis"><em>requeue</em></span> to <span class="strong"><strong>false</strong></span> will cause messages to be rejected on the first attempt (and possibly sent to a Dead Letter
Exchange/Queue if the broker is so configured).
The <span class="emphasis"><em>enableRetry</em></span> option allows configuration of retry parameters such that a failed message delivery can be retried and
eventually discarded (or dead-lettered) when retries are exhausted.
The delivery thread is suspended during the retry interval(s).
Retry options are <span class="emphasis"><em>enableRetry</em></span>, <span class="emphasis"><em>maxAttempts</em></span>, <span class="emphasis"><em>initialRetryInterval</em></span>, <span class="emphasis"><em>retryMultiplier</em></span>, and <span class="emphasis"><em>maxRetryInterval</em></span>.
Message deliveries failing with a <span class="emphasis"><em>MessageConversionException</em></span> are never retried; the assumption being that if a message
could not be converted on the first attempt, subsequent attempts will also fail.
Such messages are discarded (or dead-lettered).</p></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-sftp" href="#spring-cloud-stream-modules-sftp"></a>2.8&nbsp;SFTP (<code class="literal">sftp</code>)</h2></div></div></div><p>This source module supports transfer of files using the SFTP protocol.
Files are transferred from the <code class="literal">remote</code> directory to the <code class="literal">local</code> directory where the module is deployed.</p><p>Messages emitted by the source are provided as a byte array by default. However, this can be
customized using the <code class="literal">--mode</code> option:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><span class="strong"><strong>ref</strong></span> Provides a <code class="literal">java.io.File</code> reference</li><li class="listitem"><span class="strong"><strong>lines</strong></span> Will split files line-by-line and emit a new message for each line</li><li class="listitem"><span class="strong"><strong>contents</strong></span> The default. Provides the contents of a file as a byte array</li></ul></div><p>When using <code class="literal">--mode=lines</code>, you can also provide the additional option <code class="literal">--withMarkers=true</code>.
If set to <code class="literal">true</code>, the underlying <code class="literal">FileSplitter</code> will emit additional <span class="emphasis"><em>start-of-file</em></span> and <span class="emphasis"><em>end-of-file</em></span> marker messages before and after the actual data.
The payload of these 2 additional marker messages is of type <code class="literal">FileSplitter.FileMarker</code>. The option <code class="literal">withMarkers</code> defaults to <code class="literal">false</code> if not explicitly set.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_8" href="#_options_8"></a>2.8.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>sftp</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">allowUnknownKeys</span></dt><dd>true to allow connecting to a host with an unknown or changed key <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">autoCreateLocalDir</span></dt><dd>if local directory must be auto created if it does not exist <span class="strong"><strong>(boolean, default: <code class="literal">true</code>)</strong></span></dd><dt><span class="term">deleteRemoteFiles</span></dt><dd>delete remote files after transfer <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">fixedDelay</span></dt><dd>fixed delay in SECONDS to poll the remote directory <span class="strong"><strong>(int, default: <code class="literal">1</code>)</strong></span></dd><dt><span class="term">host</span></dt><dd>the remote host to connect to <span class="strong"><strong>(String, default: <code class="literal">localhost</code>)</strong></span></dd><dt><span class="term">initialDelay</span></dt><dd>an initial delay when using a fixed delay trigger, expressed in TimeUnits (seconds by default) <span class="strong"><strong>(int, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">knownHostsExpression</span></dt><dd>a SpEL expression location of known hosts file; required if 'allowUnknownKeys' is false; examples: systemProperties["user.home"]+"/.ssh/known_hosts", "/foo/bar/known_hosts" <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">localDir</span></dt><dd>set the local directory the remote files are transferred to <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">maxMessages</span></dt><dd>the maximum messages per poll; -1 for unlimited <span class="strong"><strong>(long, default: <code class="literal">-1</code>)</strong></span></dd><dt><span class="term">mode</span></dt><dd>specifies how the file is being read. By default the content of a file is provided as byte array <span class="strong"><strong>(FileReadingMode, default: <code class="literal">contents</code>, possible values: <code class="literal">ref,lines,contents</code>)</strong></span></dd><dt><span class="term">passPhrase</span></dt><dd>the passphrase to use <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">password</span></dt><dd>the password for the provided user <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">pattern</span></dt><dd>simple filename pattern to apply to the filter <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">port</span></dt><dd>the remote port to connect to <span class="strong"><strong>(int, default: <code class="literal">22</code>)</strong></span></dd><dt><span class="term">privateKey</span></dt><dd>the private key location (a valid Spring Resource URL) <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">regexPattern</span></dt><dd>filename regex pattern to apply to the filter <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">remoteDir</span></dt><dd>the remote directory to transfer the files from <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">timeUnit</span></dt><dd>the time unit for the fixed and initial delays <span class="strong"><strong>(String, default: <code class="literal">SECONDS</code>)</strong></span></dd><dt><span class="term">tmpFileSuffix</span></dt><dd>extension to use when downloading files <span class="strong"><strong>(String, default: <code class="literal">.tmp</code>)</strong></span></dd><dt><span class="term">user</span></dt><dd>the username to use <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">withMarkers</span></dt><dd>if true emits start of file/end of file marker messages before/after the data. Only valid with FileReadingMode 'lines' <span class="strong"><strong>(Boolean, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-clound-stream-modules-source-syslog" href="#spring-clound-stream-modules-source-syslog"></a>2.9&nbsp;SYSLOG</h2></div></div></div><p>The syslog source receives SYSLOG packets over UDP, TCP, or both.
RFC3164 (BSD) and RFC5424 formats are supported.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_9" href="#_options_9"></a>2.9.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>syslog</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">protocol</span></dt><dd>`udp`, `tcp`, or `both` <span class="strong"><strong>(String, default <code class="literal">tcp</code>)</strong></span></dd><dt><span class="term">rfc</span></dt><dd>`3164` or `5424` <span class="strong"><strong>(String, default <code class="literal">3164</code>)</strong></span></dd><dt><span class="term">port</span></dt><dd>the port on which to listen  <span class="strong"><strong>(String, default <code class="literal">1514</code>)</strong></span></dd><dt><span class="term">bufferSize</span></dt><dd>the maximum size allowed (TCP) <span class="strong"><strong>(int, default <code class="literal">2048</code>)</strong></span></dd><dt><span class="term">nio</span></dt><dd>`true` to use NIO - only recommended when supporting many connections <span class="strong"><strong>(Boolean, default <code class="literal">false</code>)</strong></span></dd><dt><span class="term">reverseLookup</span></dt><dd>`true` to perform a reverse lookup on the remote IP address <span class="strong"><strong>(Boolean, default <code class="literal">false</code>)</strong></span></dd><dt><span class="term">socketTimeout</span></dt><dd>the socket timeout <span class="strong"><strong>(long, default <code class="literal">none</code>)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_tcp" href="#_tcp"></a>2.10&nbsp;TCP</h2></div></div></div><p>The <code class="literal">tcp</code> source acts as a server and allows a remote party to connect to Spring Cloud Data Flow and submit data over a raw tcp socket.</p><p>TCP is a streaming protocol and some mechanism is needed to frame messages on the wire. A number of decoders are
available, the default being 'CRLF' which is compatible with Telnet.</p><p>Messages produced by the TCP source module have a <code class="literal">byte[]</code> payload.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_10" href="#_options_10"></a>2.10.1&nbsp;Options</h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">bufferSize</span></dt><dd>the size of the buffer (bytes) to use when decoding <span class="strong"><strong>(int, default: <code class="literal">2048</code>)</strong></span></dd><dt><span class="term">decoder</span></dt><dd>the decoder to use when receiving messages <span class="strong"><strong>(Encoding, default: <code class="literal">CRLF</code>, possible values: <code class="literal">CRLF,LF,NULL,STXETX,RAW,L1,L2,L4</code>)</strong></span></dd><dt><span class="term">nio</span></dt><dd>whether or not to use NIO <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">port</span></dt><dd>the port on which to listen <span class="strong"><strong>(int, default: <code class="literal">1234</code>)</strong></span></dd><dt><span class="term">reverseLookup</span></dt><dd>perform a reverse DNS lookup on the remote IP Address <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">socketTimeout</span></dt><dd>the timeout (ms) before closing the socket when no data is received <span class="strong"><strong>(int, default: <code class="literal">120000</code>)</strong></span></dd><dt><span class="term">useDirectBuffers</span></dt><dd>whether or not to use direct buffers <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_available_decoders" href="#_available_decoders"></a>2.10.2&nbsp;Available Decoders</h3></div></div></div><div class="variablelist"><p class="title"><b>Text Data</b></p><dl class="variablelist"><dt><span class="term">CRLF (default)</span></dt><dd>text terminated by carriage return (0x0d) followed by line feed (0x0a)</dd><dt><span class="term">LF</span></dt><dd>text terminated by line feed (0x0a)</dd><dt><span class="term">NULL</span></dt><dd>text terminated by a null byte (0x00)</dd><dt><span class="term">STXETX</span></dt><dd>text preceded by an STX (0x02) and terminated by an ETX (0x03)</dd></dl></div><div class="variablelist"><p class="title"><b>Text and Binary Data</b></p><dl class="variablelist"><dt><span class="term">RAW</span></dt><dd>no structure - the client indicates a complete message by closing the socket</dd><dt><span class="term">L1</span></dt><dd>data preceded by a one byte (unsigned) length field (supports up to 255 bytes)</dd><dt><span class="term">L2</span></dt><dd>data preceded by a two byte (unsigned) length field (up to 2<sup>16</sup>-1 bytes)</dd><dt><span class="term">L4</span></dt><dd>data preceded by a four byte (signed) length field (up to 2<sup>31</sup>-1 bytes)</dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-time" href="#spring-cloud-stream-modules-time"></a>2.11&nbsp;Time (<code class="literal">time</code>)</h2></div></div></div><p>The time source will simply emit a String with the current time every so often.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_11" href="#_options_11"></a>2.11.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>time</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">fixedDelay</span></dt><dd>time delay between messages, expressed in TimeUnits (seconds by default) <span class="strong"><strong>(int, default: <code class="literal">1</code>)</strong></span></dd><dt><span class="term">dateFormat</span></dt><dd>how to render the current time, using SimpleDateFormat <span class="strong"><strong>(String, default: <code class="literal">yyyy-MM-dd HH:mm:ss</code>)</strong></span></dd><dt><span class="term">initialDelay</span></dt><dd>an initial delay when using a fixed delay trigger, expressed in TimeUnits (seconds by default) <span class="strong"><strong>(int, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">timeUnit</span></dt><dd>the time unit for the fixed and initial delays <span class="strong"><strong>(String, default: <code class="literal">SECONDS</code>)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-twitterstream" href="#spring-cloud-stream-modules-twitterstream"></a>2.12&nbsp;Twitter Stream (<code class="literal">twitterstream</code>)</h2></div></div></div><p>This source ingests data from Twitter&#8217;s <a class="link" href="https://dev.twitter.com/docs/streaming-apis/streams/public" target="_top">streaming API v1.1</a>. It uses the <a class="link" href="https://dev.twitter.com/docs/streaming-apis/streams/public" target="_top">sample and filter</a> stream endpoints rather than the full "firehose" which needs special access. The endpoint used will depend on the parameters you supply in the stream definition (some are specific to the filter endpoint).</p><p>You need to supply all keys and secrets (both consumer and accessToken) to authenticate for this source, so it is easiest if you just add these as the following environment variables: CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN and ACCESS_TOKEN_SECRET.</p><p>Stream creation is then straightforward:</p><pre class="literallayout">dataflow:&gt; stream create --name tweets --definition "twitterstream | log" --deploy</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_12" href="#_options_12"></a>2.12.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>twitterstream</strong></span> source has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">accessToken</span></dt><dd>a valid OAuth access token <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">accessTokenSecret</span></dt><dd>an OAuth secret corresponding to the access token <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">consumerKey</span></dt><dd>a consumer key issued by twitter <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">consumerSecret</span></dt><dd>consumer secret corresponding to the consumer key <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">language</span></dt><dd>language code e.g. 'en' <span class="strong"><strong>(String, default: ``)</strong></span></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p><code class="literal">twitterstream</code> emit JSON in the <a class="link" href="https://dev.twitter.com/docs/platform-objects/tweets" target="_top">native Twitter format</a>.</p></td></tr></table></div></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-stream-modules-processors" href="#spring-cloud-stream-modules-processors"></a>3.&nbsp;Processors</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-filter" href="#spring-cloud-stream-modules-filter"></a>3.1&nbsp;Filter (<code class="literal">filter</code>)</h2></div></div></div><p>Use the filter module in a stream to determine whether a Message should be passed to the output channel.</p><p>The <span class="strong"><strong>filter</strong></span> processor has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">expression</span></dt><dd>a SpEL expression used to transform messages <span class="strong"><strong>(String, default: <code class="literal">payload.toString()</code>)</strong></span></dd></dl></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_filter_with_spel_expression" href="#_filter_with_spel_expression"></a>3.1.1&nbsp;Filter with SpEL expression</h3></div></div></div><p>The simplest way to use the filter processor is to pass a SpEL expression when creating the stream. The expression should evaluate the message and return true or false.  For example:</p><pre class="literallayout">dataflow:&gt; stream create --name filtertest --definition "http --server.port=9000 | filter --expression=payload=='good' | log" --deploy</pre><p>This filter will only pass Messages to the log sink if the payload is the word "good". Try sending "good" to the HTTP endpoint and you should see it in the Spring Cloud Data Flow logs:</p><pre class="literallayout">dataflow:&gt; http post --target http://localhost:9000 --data "good"</pre><p>Alternatively, if you send the word "bad" (or anything else), you shouldn&#8217;t see the log entry.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-groovy-filter" href="#spring-cloud-stream-modules-groovy-filter"></a>3.2&nbsp;Groovy Filter (<code class="literal">groovy-filter</code>)</h2></div></div></div><p>A Processor module that retains or discards messages according to a predicate, expressed as a Groovy script.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_13" href="#_options_13"></a>3.2.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>groovy-filter</strong></span> processor has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">script</span></dt><dd>The script resource location <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">variables</span></dt><dd>Variable bindings as a comma delimited string of name-value pairs, e.g. 'foo=bar,baz=car' <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">variablesLocation</span></dt><dd>The location of a properties file containing custom script variable bindings <span class="strong"><strong>(String, default: ``)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-httpclient" href="#spring-cloud-stream-modules-httpclient"></a>3.3&nbsp;Http Client (<code class="literal">httpclient</code>)</h2></div></div></div><p>A processor module that makes requests to an HTTP resource and emits the response body as a message payload. This processor can be combined, e.g., with a time source module to periodically poll results from a HTTP resource.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_14" href="#_options_14"></a>3.3.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>httpclient</strong></span> processor has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">url</span></dt><dd>The URL to issue an http request to, as a static value.</dd><dt><span class="term">urlExpression</span></dt><dd>A SpEL expression against incoming message to determine the URL to use.</dd><dt><span class="term">httpMethod</span></dt><dd>The kind of http method to use.</dd><dt><span class="term">body</span></dt><dd>The (static) body of the request to use.</dd><dt><span class="term">bodyExpression</span></dt><dd>A SpEL expression against incoming message to derive the request body to use.</dd><dt><span class="term">headersExpression</span></dt><dd>A SpEL expression used to derive the http headers map to use.</dd><dt><span class="term">expectedResponseType</span></dt><dd>The type used to interpret the response.</dd><dt><span class="term">replyExpression</span></dt><dd>A SpEL expression used to compute the final result, applied against the whole http response.</dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-bridge" href="#spring-cloud-stream-modules-bridge"></a>3.4&nbsp;Bridge (<code class="literal">bridge</code>)</h2></div></div></div><p>A Processor module that returns messages that is passed by connecting just the input and output channels.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-groovy-transform" href="#spring-cloud-stream-modules-groovy-transform"></a>3.5&nbsp;Groovy Transform (<code class="literal">groovy-transform</code>)</h2></div></div></div><p>A Processor module that transforms messages using a Groovy script.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_15" href="#_options_15"></a>3.5.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>groovy-transform</strong></span> processor has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">script</span></dt><dd>The script resource location <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">variables</span></dt><dd>Variable bindings as a comma delimited string of name-value pairs, e.g. 'foo=bar,baz=car' <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">variablesLocation</span></dt><dd>The location of a properties file containing custom script variable bindings <span class="strong"><strong>(String, default: ``)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-transform" href="#spring-cloud-stream-modules-transform"></a>3.6&nbsp;Transform (<code class="literal">transform</code>)</h2></div></div></div><p>Use the transform module in a stream to convert a Message&#8217;s content or structure.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_16" href="#_options_16"></a>3.6.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>transform</strong></span> processor has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">expression</span></dt><dd>a SpEL expression used to transform messages <span class="strong"><strong>(String, default: <code class="literal">payload.toString()</code>)</strong></span></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_transform_with_spel_expression" href="#_transform_with_spel_expression"></a>3.6.2&nbsp;Transform with SpEL expression</h3></div></div></div><p>The simplest way to use the transform processor is to pass a SpEL expression when creating the stream. The expression should return the modified message or payload.  For example:</p><pre class="literallayout">dataflow:&gt; stream create --name transformtest --definition "http --server.port=9003 | transform --expression=payload.toUpperCase() | log" --deploy</pre><p>This transform will convert all message payloads to upper case. If sending the word "foo" to the HTTP endpoint and you should see "FOO" in the Spring Cloud Data Flow logs:</p><pre class="literallayout">dataflow:&gt; http post --target http://localhost:9003 --data "foo"</pre><p>As part of the SpEL expression you can make use of the pre-registered JSON Path function.  The syntax is #jsonPath(payload,'&lt;json path expression&gt;')</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-splitter" href="#spring-cloud-stream-modules-splitter"></a>3.7&nbsp;Splitter</h2></div></div></div><p>The splitter module builds upon the concept of the same name in Spring Integration and allows the splitting of a single
message into several distinct messages.</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">expression</span></dt><dd>a SpEL expression which would typically evaluate to an array or collection <span class="strong"><strong>(String, default: <code class="literal">null</code>)</strong></span></dd><dt><span class="term">delimiters</span></dt><dd>A list of delimiters to tokenize a String payload ('expression' must be null) <span class="strong"><strong>(String, default: <code class="literal">null</code>)</strong></span></dd><dt><span class="term">fileMarkers</span></dt><dd>Split File payloads, when true, START and END marker messages will be emitted, when false no markers are emitted <span class="strong"><strong>(String, default: <code class="literal">null</code>)</strong></span></dd><dt><span class="term">charset</span></dt><dd>Split File payloads using this charset to convert bytes to String <span class="strong"><strong>(String, default: <code class="literal">null</code>)</strong></span></dd><dt><span class="term">applySequence</span></dt><dd>Add correlation and sequence information to the message headers <span class="strong"><strong>(String, default: <code class="literal">true</code>)</strong></span></dd></dl></div><p>When no <code class="literal">expression</code>, <code class="literal">fileMarkers</code>, or <code class="literal">charset</code> is provided, a <code class="literal">DefaultMessageSplitter</code> is configured with (optional) <code class="literal">delimiters</code>.
When <code class="literal">fileMarkers</code> or <code class="literal">charset</code> is provided, a <code class="literal">FileSplitter</code> is configured (you must provide either a <code class="literal">fileMarkers</code>
or <code class="literal">charset</code> to split files, which must be text-based - they are split into lines).
Otherwise, an <code class="literal">ExpressionEvaluatingMessageSplitter</code> is configured.</p><p>When splitting <code class="literal">File</code> payloads, the <code class="literal">sequenceSize</code> header is zero because the size cannot be determined at the beginning.</p><p><span class="strong"><strong>Ambiguous properties are not allowed.</strong></span></p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_json_example" href="#_json_example"></a>3.7.1&nbsp;JSON Example</h3></div></div></div><p>As part of the SpEL expression you can make use of the pre-registered JSON Path function. The syntax is
<code class="literal">#jsonPath(payload, '&lt;json path expression&gt;')</code>.</p><p>For example, consider the following JSON:</p><pre class="programlisting"><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">{</span> <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"store"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">{</span>
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"book"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">[</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">{</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"category"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"reference"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"author"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"Nigel Rees"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"title"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"Sayings of the Century"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"price"</span>: <span class="hl-number">8.95</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">},</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">{</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"category"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"fiction"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"author"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"Evelyn Waugh"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"title"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"Sword of Honour"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"price"</span>: <span class="hl-number">12.99</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">},</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">{</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"category"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"fiction"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"author"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"Herman Melville"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"title"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"Moby Dick"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"isbn"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"0-553-21311-3"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"price"</span>: <span class="hl-number">8.99</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">},</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">{</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"category"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"fiction"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"author"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"J. R. R. Tolkien"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"title"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"The Lord of the Rings"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"isbn"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"0-395-19395-8"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
            <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"price"</span>: <span class="hl-number">22.99</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">}</span>
    ]<span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"bicycle"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">{</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"color"</span>: <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"red"</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">,</span>
        <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"price"</span>: <span class="hl-number">19.95</span>
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">}</span>
}<span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">}</span></pre><p>and an expression <code class="literal">#jsonPath(payload, '$.store.book')</code>; the result will be 4 messages, each with a <code class="literal">Map</code> payload
containing the properties of a single book.</p></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-stream-modules-sinks" href="#spring-cloud-stream-modules-sinks"></a>4.&nbsp;Sinks</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-cassandra" href="#spring-cloud-stream-modules-cassandra"></a>4.1&nbsp;Cassandra (<code class="literal">cassandra</code>)</h2></div></div></div><p>The Cassandra sink writes into a Cassandra table.  Here is a simple example</p><pre class="literallayout">dataflow:&gt;stream create cassandrastream --definition "http --server.port=8888 --spring.cloud.stream.bindings.output.contentType='application/json' | cassandra --ingestQuery='insert into book (id, isbn, title, author) values (uuid(), ?, ?, ?)' --spring.cassandra.keyspace=clouddata" --deploy</pre><p>Create a keyspace and a <code class="literal">book</code> table in Cassandra using:</p><pre class="programlisting">CREATE KEYSPACE clouddata WITH REPLICATION = { 'class' : 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1' } AND DURABLE_WRITES = true;
USE clouddata;
CREATE TABLE book  (
    id          uuid PRIMARY KEY,
    isbn        text,
    author      text,
    title       text
);</pre><p>You can then send data to this stream via</p><pre class="literallayout">dataflow:&gt;http post --contentType 'application/json' --data '{"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}' --target http://localhost:8888/
&gt; POST (application/json;charset=UTF-8) http://localhost:8888/ {"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}
&gt; 202 ACCEPTED</pre><p>and see the table contents using the CQL</p><pre class="literallayout">SELECT * FROM clouddata.book;</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_17" href="#_options_17"></a>4.1.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>cassandra</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">compressionType</span></dt><dd>the compression to use for the transport <span class="strong"><strong>(CompressionType, default: <code class="literal">NONE</code>, possible values: <code class="literal">NONE,SNAPPY</code>)</strong></span></dd><dt><span class="term">consistencyLevel</span></dt><dd>the consistencyLevel option of WriteOptions <span class="strong"><strong>(ConsistencyLevel, no default, possible values: <code class="literal">ANY,ONE,TWO,THREE,QUOROM,LOCAL_QUOROM,EACH_QUOROM,ALL,LOCAL_ONE,SERIAL,LOCAL_SERIAL</code>)</strong></span></dd><dt><span class="term">spring.cassandra.contactPoints</span></dt><dd>the comma-delimited string of the hosts to connect to Cassandra <span class="strong"><strong>(String, default: <code class="literal">localhost</code>)</strong></span></dd><dt><span class="term">entityBasePackages</span></dt><dd>the base packages to scan for entities annotated with Table annotations <span class="strong"><strong>(String[], default: <code class="literal">[]</code>)</strong></span></dd><dt><span class="term">ingestQuery</span></dt><dd>the ingest Cassandra query <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">spring.cassandra.initScript</span></dt><dd>the path to file with CQL scripts (delimited by ';') to initialize keyspace schema <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">spring.cassandra.keyspace</span></dt><dd>the keyspace name to connect to <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">metricsEnabled</span></dt><dd>enable/disable metrics collection for the created cluster <span class="strong"><strong>(boolean, default: <code class="literal">true</code>)</strong></span></dd><dt><span class="term">spring.cassandra.password</span></dt><dd>the password for connection <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">spring.cassandra.port</span></dt><dd>the port to use to connect to the Cassandra host <span class="strong"><strong>(int, default: <code class="literal">9042</code>)</strong></span></dd><dt><span class="term">queryType</span></dt><dd>the queryType for Cassandra Sink <span class="strong"><strong>(Type, default: <code class="literal">INSERT</code>, possible values: <code class="literal">INSERT,UPDATE,DELETE,STATEMENT</code>)</strong></span></dd><dt><span class="term">retryPolicy</span></dt><dd>the retryPolicy  option of WriteOptions <span class="strong"><strong>(RetryPolicy, no default, possible values: <code class="literal">DEFAULT,DOWNGRADING_CONSISTENCY,FALLTHROUGH,LOGGING</code>)</strong></span></dd><dt><span class="term">statementExpression</span></dt><dd>the expression in Cassandra query DSL style <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">spring.cassandra.schemaAction</span></dt><dd>schema action to perform <span class="strong"><strong>(SchemaAction, default: <code class="literal">NONE</code>, possible values: <code class="literal">CREATE,NONE,RECREATE,RECREATE_DROP_UNUSED</code>)</strong></span></dd><dt><span class="term">ttl</span></dt><dd>the time-to-live option of WriteOptions <span class="strong"><strong>(int, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">spring.cassandra.username</span></dt><dd>the username for connection <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-counter" href="#spring-cloud-stream-modules-counter"></a>4.2&nbsp;Counter (<code class="literal">counter</code>)</h2></div></div></div><p>A simple module that counts messages received, using Spring Boot metrics abstraction.</p><p>The <span class="strong"><strong>counter</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">name</span></dt><dd>The name of the counter to increment. <span class="strong"><strong>(String, default: <code class="literal">counts</code>)</strong></span></dd><dt><span class="term">nameExpression</span></dt><dd>A SpEL expression (against the incoming Message) to derive the name of the counter to increment. <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">store</span></dt><dd>The name of a store used to store the counter. <span class="strong"><strong>(String, default: <code class="literal">memory</code>, possible values: <code class="literal">memory</code>, <code class="literal">redis</code>)</strong></span></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-field-value-counter" href="#spring-cloud-stream-modules-field-value-counter"></a>4.3&nbsp;Field Value Counter (<code class="literal">field-value-counter</code>)</h2></div></div></div><p>A field value counter is a Metric used for counting occurrences of unique values for a named field in a message payload. Spring Cloud Data Flow supports the following payload types out of the box:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">POJO (Java bean)</li><li class="listitem">Tuple</li><li class="listitem">JSON String</li></ul></div><p>For example suppose a message source produces a payload with a field named <span class="emphasis"><em>user</em></span> :</p><pre class="programlisting"><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">class</span> Foo {
   String user;
   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">public</span> Foo(String user) {
       <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">this</span>.user = user;
   }
}</pre><p>If the stream source produces messages with the following objects:</p><pre class="programlisting">   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">new</span> Foo(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"fred"</span>)
   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">new</span> Foo(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"sue"</span>)
   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">new</span> Foo(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"dave"</span>)
   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">new</span> Foo(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"sue"</span>)</pre><p>The field value counter on the field <span class="emphasis"><em>user</em></span> will contain:</p><pre class="literallayout">fred:1, sue:2, dave:1</pre><p>Multi-value fields are also supported. For example, if a field contains a list, each value will be counted once:</p><pre class="literallayout">users:["dave","fred","sue"]
users:["sue","jon"]</pre><p>The field value counter on the field <span class="emphasis"><em>users</em></span> will contain:</p><pre class="literallayout">dave:1, fred:1, sue:2, jon:1</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_18" href="#_options_18"></a>4.3.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>field-value-counter</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">fieldName</span></dt><dd>the name of the field for which values are counted <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">name</span></dt><dd>the name of the metric to contribute to (will be created if necessary) <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">nameExpression</span></dt><dd>a SpEL expression to compute the name of the metric to contribute to <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-file-sink" href="#spring-cloud-stream-modules-file-sink"></a>4.4&nbsp;File (<code class="literal">file</code>)</h2></div></div></div><p>This module writes each message it receives to a file.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_19" href="#_options_19"></a>4.4.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>file</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">binary</span></dt><dd>if false, will append a newline character at the end of each line <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">charset</span></dt><dd>the charset to use when writing a String payload <span class="strong"><strong>(String, default: <code class="literal">UTF-8</code>)</strong></span></dd><dt><span class="term">dir</span></dt><dd>the directory in which files will be created <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">dirExpression</span></dt><dd>spring expression used to define directory name <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">mode</span></dt><dd>what to do if the file already exists <span class="strong"><strong>(Mode, default: <code class="literal">APPEND</code>, possible values: <code class="literal">APPEND,REPLACE,FAIL,IGNORE</code>)</strong></span></dd><dt><span class="term">name</span></dt><dd>filename pattern to use <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">nameExpression</span></dt><dd>spring expression used to define filename <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">suffix</span></dt><dd>filename extension to use <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ftp-sink" href="#ftp-sink"></a>4.5&nbsp;FTP Sink (<code class="literal">ftp</code>)</h2></div></div></div><p>FTP sink is a simple option to push files to an FTP server from incoming messages.</p><p>It uses an <code class="literal">ftp-outbound-adapter</code>, therefore incoming messages could be either a <code class="literal">java.io.File</code> object, a <code class="literal">String</code> (content of the file)
or an array of <code class="literal">bytes</code> (file content as well).</p><p>To use this sink, you need a username and a password to login.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>By default Spring Integration will use <code class="literal">o.s.i.file.DefaultFileNameGenerator</code> if none is specified. <code class="literal">DefaultFileNameGenerator</code> will determine the file name
based on the value of the <code class="literal">file_name</code> header (if it exists) in the <code class="literal">MessageHeaders</code>, or if the payload of the <code class="literal">Message</code> is already a <code class="literal">java.io.File</code>, then it will
use the original name of that file.</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-gemfire-sink" href="#spring-cloud-stream-modules-gemfire-sink"></a>4.6&nbsp;Gemfire (<code class="literal">gemfire</code>)</h2></div></div></div><p>A sink module that allows one to write message payloads to a Gemfire server.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_20" href="#_options_20"></a>4.6.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>gemfire</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">hostAddresses</span></dt><dd>a comma separated list of [host]:[port] specifying either locator or server addresses for the client connection pool <span class="strong"><strong>(String, <code class="literal">localhost:10334</code>)</strong></span></dd><dt><span class="term">keyExpression</span></dt><dd>a SpEL expression which is evaluated to create a cache key <span class="strong"><strong>(String, default: <code class="literal">the value is currently the message payload'</code>)</strong></span></dd><dt><span class="term">port</span></dt><dd>port of the cache server or locator (if useLocator=true). May be a comma delimited list <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">regionName</span></dt><dd>name of the region to use when storing data <span class="strong"><strong>(String, default: <code class="literal">${spring.application.name}</code>)</strong></span></dd><dt><span class="term">connectType</span></dt><dd>'server' or 'locator' <span class="strong"><strong>(String, default: <code class="literal">locator</code>)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-hdfs" href="#spring-cloud-stream-modules-hdfs"></a>4.7&nbsp;Hadoop (HDFS) (<code class="literal">hdfs</code>)</h2></div></div></div><p>If you do not have Hadoop installed, you can install Hadoop as described in our <a class="link" href="Hadoop-Installation.xml#installing-hadoop" target="_top">separate guide</a>.</p><p>Once Hadoop is up and running, you can then use the <code class="literal">hdfs</code> sink when creating a <a class="link" href="Streams.xml#streams" target="_top">stream</a></p><pre class="literallayout">dataflow:&gt; stream create --name myhdfsstream1 --definition "time | hdfs" --deploy</pre><p>In the above example, we&#8217;ve scheduled <code class="literal">time</code> source to automatically send ticks to <code class="literal">hdfs</code> once in every second. If you wait a little while for data to accumuluate you can then list can then list the files in the hadoop filesystem using the shell&#8217;s built in hadoop fs commands.  Before making any access to HDFS in the shell you first need to configure the shell to point to your name node.  This is done using the <code class="literal">hadoop config</code> command.</p><pre class="literallayout">dataflow:&gt;hadoop config fs --namenode hdfs://localhost:8020</pre><p>In this example the hdfs protocol is used but you may also use the webhdfs protocol.  Listing the contents in the output directory (named by default after the stream name) is done by issuing the following command.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream1
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup          0 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt.tmp</pre><p>While the file is being written to it will have the <code class="literal">tmp</code> suffix.  When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the <code class="literal">tmp</code> suffix.  There are several options to control the in use file file naming options.  These are <code class="literal">--inUsePrefix</code> and <code class="literal">--inUseSuffix</code> set the file name prefix and suffix respectfully.</p><p>When you destroy a stream</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream1</pre><p>and list the stream directory again, in use file suffix doesn&#8217;t exist anymore.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream1
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup        380 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt</pre><p>To list the list the contents of a file directly from a shell execute the hadoop cat command.</p><pre class="literallayout">dataflow:&gt; hadoop fs cat /xd/myhdfsstream1/myhdfsstream1-0.txt
2013-12-18 18:10:07
2013-12-18 18:10:08
2013-12-18 18:10:09
...</pre><p>In the above examples we didn&#8217;t yet go through why the file was written in a specific directory and why it was named in this specific way. Default location of a file is defined as <code class="literal">/xd/&lt;stream name&gt;/&lt;stream name&gt;-&lt;rolling part&gt;.txt</code>. These can be changed using options <code class="literal">--directory</code> and <code class="literal">--fileName</code> respectively. Example is shown below.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream2 --definition "time | hdfs --directory=/xd/tmp --fileName=data" --deploy
dataflow:&gt;stream destroy --name myhdfsstream2
dataflow:&gt;hadoop fs ls /xd/tmp
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup        120 2013-12-18 18:31 /xd/tmp/data-0.txt</pre><p>It is also possible to control the size of a files written into HDFS. The <code class="literal">--rollover</code> option can be used to control when file currently being written is rolled over and a new file opened by providing the rollover size in bytes, kilobytes, megatypes, gigabytes, and terabytes.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream3 --definition "time | hdfs --rollover=100" --deploy
dataflow:&gt;stream destroy --name myhdfsstream3
dataflow:&gt;hadoop fs ls /xd/myhdfsstream3
Found 3 items
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-0.txt
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-1.txt
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-2.txt</pre><p>Shortcuts to specify sizes other than bytes are written as <code class="literal">--rollover=64M</code>, <code class="literal">--rollover=512G</code> or <code class="literal">--rollover=1T</code>.</p><p>The stream can also be compressed during the write operation. Example of this is shown below.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream4 --definition "time | hdfs --codec=gzip" --deploy
dataflow:&gt;stream destroy --name myhdfsstream4
dataflow:&gt;hadoop fs ls /xd/myhdfsstream4
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup         80 2013-12-18 18:48 /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip</pre><p>From a native os shell we can use hadoop&#8217;s fs commands and pipe data into gunzip.</p><pre class="literallayout"># bin/hadoop fs -cat /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip | gunzip
2013-12-18 18:48:10
2013-12-18 18:48:11
...</pre><p>Often a stream of data may not have a high enough rate to roll over files frequently, leaving the file in an opened state.  This prevents users from reading a consistent set of data when running mapreduce jobs.  While one can alleviate this problem by using a small rollover value, a better way is to use the <code class="literal">idleTimeout</code>  option that will automatically close the file if there was no writes during the specified period of time.   This feature is also useful in cases where burst of data is written into a stream and you&#8217;d like that data to become visible in HDFS.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The <code class="literal">idleTimeout</code> value should not exceed the timeout values set on the Hadoop cluster. These are typically configured using the <code class="literal">dfs.socket.timeout</code> and/or <code class="literal">dfs.datanode.socket.write.timeout</code> properties in the <code class="literal">hdfs-site.xml</code> configuration file.</p></td></tr></table></div><pre class="literallayout">dataflow:&gt; stream create --name myhdfsstream5 --definition "http --server.port=8000 | hdfs --rollover=20 --idleTimeout=10000" --deploy</pre><p>In the above example we changed a source to <code class="literal">http</code> order to control what we write into a <code class="literal">hdfs</code> sink. We defined a small rollover size and a timeout of 10 seconds. Now we can simply post data into this stream via source end point using a below command.</p><pre class="literallayout">dataflow:&gt; http post --target http://localhost:8000 --data "hello"</pre><p>If we repeat the command very quickly and then wait for the timeout we should be able to see that some files are closed before rollover size was met and some were simply rolled because of a rollover size.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream5
Found 4 items
-rw-r--r--   3 jvalkealahti supergroup         12 2013-12-18 19:02 /xd/myhdfsstream5/myhdfsstream5-0.txt
-rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-1.txt
-rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-2.txt
-rw-r--r--   3 jvalkealahti supergroup         18 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-3.txt</pre><p>Files can be automatically partitioned using a <code class="literal">partitionPath</code> expression. If we create a stream with <code class="literal">idleTimeout</code> and <code class="literal">partitionPath</code> with simple format <code class="literal">yyyy/MM/dd/HH/mm</code> we should see writes ending into its own files within every minute boundary.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream6 --definition "time|hdfs --idleTimeout=10000 --partitionPath=dateFormat('yyyy/MM/dd/HH/mm')" --deploy</pre><p>Let a stream run for a short period of time and list files.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls --recursive true --dir /xd/myhdfsstream6
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42
-rw-r--r--   3 jvalkealahti supergroup        140 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42/myhdfsstream6-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43
-rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43/myhdfsstream6-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44
-rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44/myhdfsstream6-0.txt</pre><p>Partitioning can also be based on defined lists. In a below example we simulate feeding data by using a <code class="literal">time</code> and a <code class="literal">transform</code> elements. Data passed to <code class="literal">hdfs</code> sink has a content <code class="literal">APP0:foobar</code>, <code class="literal">APP1:foobar</code>, <code class="literal">APP2:foobar</code> or <code class="literal">APP3:foobar</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream7 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*3)+':foobar'\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),list(payload.split(':')[0],{{'0TO1','APP0','APP1'},{'2TO3','APP2','APP3'}}))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream7
Destroyed stream 'myhdfsstream7'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list
-rw-r--r--   3 jvalkealahti supergroup        108 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list
-rw-r--r--   3 jvalkealahti supergroup        180 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list/myhdfsstream7-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
APP1:foobar
APP1:foobar
APP0:foobar
APP0:foobar
APP1:foobar</pre><p>Partitioning can also be based on defined ranges. In a below example we simulate feeding data by using a <code class="literal">time</code> and a <code class="literal">transform</code> elements. Data passed to <code class="literal">hdfs</code> sink has a content ranging from <code class="literal">APP0</code> to <code class="literal">APP15</code>. We simple parse the number part and use it to do a partition with ranges <code class="literal">{3,5,10}</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream8 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*15)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),range(T(Integer).parseInt(payload.substring(3)),{3,5,10}))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream8
Destroyed stream 'myhdfsstream8'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range
-rw-r--r--   3 jvalkealahti supergroup         16 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range
-rw-r--r--   3 jvalkealahti supergroup         35 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range
-rw-r--r--   3 jvalkealahti supergroup          5 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
APP3
APP3
APP1
APP0
APP1
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
APP4
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
APP6
APP15
APP7</pre><p>Partition using a <code class="literal">dateFormat</code> can be based on content itself. This is a good use case if old log files needs to be processed where partitioning should happen based on timestamp of a log entry. We create a fake log data with a simple date string ranging from <code class="literal">1970-01-10</code> to <code class="literal">1970-01-13</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream9 --definition "time | transform --expression=\"'1970-01-'+1+T(Math).round(T(Math).random()*3)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH',payload,'yyyy-MM-DD'))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files. If you see the partition paths, those are based on year 1970, not present year.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream9
Destroyed stream 'myhdfsstream9'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/10
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00
-rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/11
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00
-rw-r--r--   3 jvalkealahti supergroup         99 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/12
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00
-rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/13
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00
-rw-r--r--   3 jvalkealahti supergroup         55 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00/myhdfsstream9-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
1970-01-10
1970-01-10
1970-01-10
1970-01-10</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_21" href="#_options_21"></a>4.7.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>hdfs</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">closeTimeout</span></dt><dd>timeout in ms, regardless of activity, after which file will be automatically closed <span class="strong"><strong>(long, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">codec</span></dt><dd>compression codec alias name (gzip, snappy, bzip2, lzo, or slzo) <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">directory</span></dt><dd>where to output the files in the Hadoop FileSystem <span class="strong"><strong>(String, default: <code class="literal">/tmp/hdfs-sink</code>)</strong></span></dd><dt><span class="term">fileExtension</span></dt><dd>the base filename extension to use for the created files <span class="strong"><strong>(String, default: <code class="literal">txt</code>)</strong></span></dd><dt><span class="term">fileName</span></dt><dd>the base filename to use for the created files <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">fileOpenAttempts</span></dt><dd>maximum number of file open attempts to find a path <span class="strong"><strong>(int, default: <code class="literal">10</code>)</strong></span></dd><dt><span class="term">fileUuid</span></dt><dd>whether file name should contain uuid <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">fsUri</span></dt><dd>the URI to use to access the Hadoop FileSystem <span class="strong"><strong>(String, default: <code class="literal">${spring.hadoop.fsUri}</code>)</strong></span></dd><dt><span class="term">idleTimeout</span></dt><dd>inactivity timeout in ms after which file will be automatically closed <span class="strong"><strong>(long, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">inUsePrefix</span></dt><dd>prefix for files currently being written <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">inUseSuffix</span></dt><dd>suffix for files currently being written <span class="strong"><strong>(String, default: <code class="literal">.tmp</code>)</strong></span></dd><dt><span class="term">overwrite</span></dt><dd>whether writer is allowed to overwrite files in Hadoop FileSystem <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">partitionPath</span></dt><dd>a SpEL expression defining the partition path <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">rollover</span></dt><dd>threshold in bytes when file will be automatically rolled over <span class="strong"><strong>(String, default: <code class="literal">1G</code>)</strong></span></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>In the context of the <code class="literal">fileOpenAttempts</code> option, attempt is either one rollover request or failed stream open request for a path (if another writer came up with a same path and already opened it).</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_partition_path_expression" href="#_partition_path_expression"></a>4.7.2&nbsp;Partition Path Expression</h3></div></div></div><p>SpEL expression is evaluated against a Spring Messaging <code class="literal">Message</code> passed internally into a HDFS writer. This allows expression to use <code class="literal">headers</code> and <code class="literal">payload</code> from that message. While you could do a custom processing within a stream and add custom headers, <code class="literal">timestamp</code> is always going to be there. Data to be written is then available in a <code class="literal">payload</code>.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="_accessing_properties" href="#_accessing_properties"></a>Accessing Properties</h4></div></div></div><p>Using a <code class="literal">payload</code> simply returns whatever is currently being written. Access to headers is via <code class="literal">headers</code> property. Any other property is automatically resolved from headers if found. For example <code class="literal">headers.timestamp</code> is equivalent to <code class="literal">timestamp</code>.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="_custom_methods" href="#_custom_methods"></a>Custom Methods</h4></div></div></div><p>Addition to a normal SpEL functionality, few custom methods has been added to make it easier to build partition paths. These custom methods can be used to work with a normal partition concepts like <code class="literal">date formatting</code>, <code class="literal">lists</code>, <code class="literal">ranges</code> and <code class="literal">hashes</code>.</p><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_path" href="#_path"></a>path</h5></div></div></div><pre class="programlisting">path(String... paths)</pre><p>Concatenates paths together with a delimiter <code class="literal">/</code>. This method can be used to make the expression less verbose than using a native SpEL functionality to combine path parts together. To create a path <code class="literal">part1/part2</code>, expression <code class="literal">'part1' + '/' + 'part2'</code> is equivalent to <code class="literal">path('part1','part2')</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">paths</span></dt><dd>Any number of path parts</dd></dl></div><p><b>Return Value.&nbsp;</b>Concatenated value of paths delimited with <code class="literal">/</code>.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_dateformat" href="#_dateformat"></a>dateFormat</h5></div></div></div><pre class="programlisting">dateFormat(String pattern)
dateFormat(String pattern, Long epoch)
dateFormat(String pattern, Date date)
dateFormat(String pattern, String datestring)
dateFormat(String pattern, String datestring, String dateformat)</pre><p>Creates a path using date formatting. Internally this method delegates into <code class="literal">SimpleDateFormat</code> and needs a <code class="literal">Date</code> and a <code class="literal">pattern</code>. On default if no parameter used for conversion is given, <code class="literal">timestamp</code> is expected. Effectively <code class="literal">dateFormat('yyyy')</code> equals to <code class="literal">dateFormat('yyyy', timestamp)</code> or <code class="literal">dateFormat('yyyy', headers.timestamp)</code>.</p><p>Method signature with three parameters can be used to create a custom <code class="literal">Date</code> object which is then passed to <code class="literal">SimpleDateFormat</code> conversion using a <code class="literal">dateformat</code> pattern. This is useful in use cases where partition should be based on a date or time string found from a payload content itself. Default <code class="literal">dateformat</code> pattern if omitted is <code class="literal">yyyy-MM-dd</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">pattern</span></dt><dd>Pattern compatible with <code class="literal">SimpleDateFormat</code> to produce a final output.</dd><dt><span class="term">epoch</span></dt><dd>Timestamp as <code class="literal">Long</code> which is converted into a <code class="literal">Date</code>.</dd><dt><span class="term">date</span></dt><dd>A <code class="literal">Date</code> to be formatted.</dd><dt><span class="term">dateformat</span></dt><dd>Secondary pattern to convert <code class="literal">datestring</code> into a <code class="literal">Date</code>.</dd><dt><span class="term">datestring</span></dt><dd><code class="literal">Date</code> as a <code class="literal">String</code></dd></dl></div><p><b>Return Value.&nbsp;</b>A path part representation which can be a simple file or directory name or a directory structure.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_list" href="#_list"></a>list</h5></div></div></div><pre class="programlisting">list(Object source, List&lt;List&lt;Object&gt;&gt; lists)</pre><p>Creates a partition path part by matching a <code class="literal">source</code> against a lists denoted by <code class="literal">lists</code>.</p><p>Lets assume that data is being written and it&#8217;s possible to extrace an <code class="literal">appid</code> either from headers or payload. We can automatically do a list based partition by using a partition method <code class="literal">list(headers.appid,{{'1TO3','APP1','APP2','APP3'},{'4TO6','APP4','APP5','APP6'}})</code>. This method would create three partitions, <code class="literal">1TO3_list</code>, <code class="literal">4TO6_list</code> and <code class="literal">list</code>. Latter is used if no match is found from partition lists passed to <code class="literal">lists</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> to be matched against <code class="literal">lists</code>.</dd><dt><span class="term">lists</span></dt><dd>A definition of list of lists.</dd></dl></div><p><b>Return Value.&nbsp;</b>A path part prefixed with a matched key i.e. <code class="literal">XXX_list</code> or <code class="literal">list</code> if no match.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_range" href="#_range"></a>range</h5></div></div></div><pre class="programlisting">range(Object source, List&lt;Object&gt; list)</pre><p>Creates a partition path part by matching a <code class="literal">source</code> against a list denoted by <code class="literal">list</code> using a simple binary search.</p><p>The partition method takes a <code class="literal">source</code> as first argument and <code class="literal">list</code> as a second argument. Behind the scenes this is using jvm&#8217;s <code class="literal">binarySearch</code> which works on an <code class="literal">Object</code> level so we can pass in anything. Remember that meaningful range match only works if passed in <code class="literal">Object</code> and types in list are of same type like <code class="literal">Integer</code>. Range is defined by a binarySearch itself so mostly it is to match against an upper bound except the last range in a list. Having a list of <code class="literal">{1000,3000,5000}</code> means that everything above 3000 will be matched with 5000. If that is an issue then simply adding <code class="literal">Integer.MAX_VALUE</code> as last range would overflow everything above 5000 into a new partition. Created partitions would then be <code class="literal">1000_range</code>, <code class="literal">3000_range</code> and <code class="literal">5000_range</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> to be matched against <code class="literal">list</code>.</dd><dt><span class="term">list</span></dt><dd>A definition of list.</dd></dl></div><p><b>Return Value.&nbsp;</b>A path part prefixed with a matched key i.e. <code class="literal">XXX_range</code>.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_hash" href="#_hash"></a>hash</h5></div></div></div><pre class="programlisting">hash(Object source, int bucketcount)</pre><p>Creates a partition path part by calculating hashkey using <code class="literal">source`s</code> <code class="literal">hashCode</code> and <code class="literal">bucketcount</code>. Using a partition method <code class="literal">hash(timestamp,2)</code> would then create partitions named <code class="literal">0_hash</code>, <code class="literal">1_hash</code> and <code class="literal">2_hash</code>. Number suffixed with <code class="literal">_hash</code> is simply calculated using <code class="literal">Object.hashCode() % bucketcount</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> which <code class="literal">hashCode</code> will be used.</dd><dt><span class="term">bucketcount</span></dt><dd>A number of buckets</dd></dl></div><p><b>Return Value.&nbsp;</b>A path part prefixed with a hash key i.e. <code class="literal">XXX_hash</code>.</p></div></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-jdbc" href="#spring-cloud-stream-modules-jdbc"></a>4.8&nbsp;JDBC (<code class="literal">jdbc</code>)</h2></div></div></div><p>A module that writes its incoming payload to an RDBMS using JDBC.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_22" href="#_options_22"></a>4.8.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>jdbc</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">expression</span></dt><dd>a SpEL expression used to transform messages <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">tableName</span></dt><dd>String <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name</code>)</strong></span></dd><dt><span class="term">columns</span></dt><dd>the names of the columns that shall receive data, as a set of column[:SpEL] mappings, also used at initialization time to issue the DDL <span class="strong"><strong>(String, default: <code class="literal">payload</code>)</strong></span></dd><dt><span class="term">initialize</span></dt><dd>String <span class="strong"><strong>(Boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">batchSize</span></dt><dd>String <span class="strong"><strong>(long, default: <code class="literal">10000</code>)</strong></span></dd></dl></div><p>The module also uses Spring Boot&#8217;s <a class="link" href="http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-sql.html#boot-features-configure-datasource" target="_top">DataSource support</a>
for configuring the database connection, so properties like <code class="literal">spring.datasource.url</code> <span class="emphasis"><em>etc.</em></span> apply.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-log" href="#spring-cloud-stream-modules-log"></a>4.9&nbsp;Log (<code class="literal">log</code>)</h2></div></div></div><p>Probably the simplest option for a sink is just to log the data. The <code class="literal">log</code> sink uses the application logger to output the data for inspection. The log level is set to <code class="literal">WARN</code> and the logger name is created from the stream name. To create a stream using a <code class="literal">log</code> sink you would use a command like</p><pre class="literallayout">dataflow:&gt; stream create --name mylogstream --definition "http --server.port=8000 | log" --deploy</pre><p>You can then try adding some data. We&#8217;ve used the <code class="literal">http</code> source on port 8000 here, so run the following command to send a message</p><pre class="literallayout">dataflow:&gt; http post --target http://localhost:8000 --data "hello"</pre><p>and you should see the following output in the Spring Cloud Data Flow console.</p><pre class="literallayout">13/06/07 16:12:18 INFO Received: hello</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-sink-rabbitmq" href="#spring-cloud-stream-modules-sink-rabbitmq"></a>4.10&nbsp;RabbitMQ</h2></div></div></div><p>The "rabbit" sink enables outbound messaging over RabbitMQ.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_23" href="#_options_23"></a>4.10.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>rabbit</strong></span> sink has the following options:</p><p>(See the Spring Boot documentation for RabbitMQ connection properties)</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">converterBeanName</span></dt><dd>the bean name of the message converter <span class="strong"><strong>(String, default: none)</strong></span></dd><dt><span class="term">persistentDeliveryMode</span></dt><dd>the default delivery mode, true for persistent <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">exchange</span></dt><dd>the Exchange on the RabbitMQ broker to which messages should be sent <span class="strong"><strong>(String, default: <code class="literal">""</code>)</strong></span></dd><dt><span class="term">exchangeExpression</span></dt><dd>a SpEL expression that evaluates to the Exchange on the RabbitMQ broker to which messages
should be sent; overrides `exchange` <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">mappedRequestHeaders</span></dt><dd>request message header names to be propagated to RabbitMQ, to limit to the set of standard headers plus `bar`, use `STANDARD_REQUEST_HEADERS,bar` <span class="strong"><strong>(String, default: <code class="literal">*</code>)</strong></span></dd><dt><span class="term">routingKey</span></dt><dd>the routing key to be passed with the message, as a SpEL expression <span class="strong"><strong>(String, default: none)</strong></span></dd><dt><span class="term">routingKeyExpression</span></dt><dd>an expression that evaluates to the routing key to be passed with the message, as a SpEL expression; overrides `routingKey` <span class="strong"><strong>(String, default: none)</strong></span></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>By default, the message converter is a <code class="literal">SimpleMessageConverter</code> which handles <code class="literal">byte[]</code>, <code class="literal">String</code> and
<code class="literal">java.io.Serializable</code>.
A well-known bean name <code class="literal">jsonConverter</code> will configure a <code class="literal">Jackson2JsonMessageConverter</code> instead.
In addition, a custom converter bean can be added to the context and referenced by the converterBeanName property.</p></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-redis" href="#spring-cloud-stream-modules-redis"></a>4.11&nbsp;Redis (<code class="literal">redis</code>)</h2></div></div></div><p>The Redis sink can be used to ingest data into redis store. You can choose <code class="literal">queue</code>, <code class="literal">topic</code> or <code class="literal">key</code> with selcted
collection type to point to a specific data store.</p><p>For example,</p><pre class="screen">dataflow:&gt;stream create store-into-redis --definition "http | redis --queue=myList" --deploy
dataflow:&gt;Created and deployed new stream 'store-into-redis'</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_24" href="#_options_24"></a>4.11.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>redis</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">topicExpression</span></dt><dd>a SpEL expression to use for topic <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">queueExpression</span></dt><dd>a SpEL expression to use for queue <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">keyExpression</span></dt><dd>a SpEL expression to use for keyExpression <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">key</span></dt><dd>name for the key <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">queue</span></dt><dd>name for the queue <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">topic</span></dt><dd>name for the topic <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-sink-router" href="#spring-cloud-stream-modules-sink-router"></a>4.12&nbsp;Dynamic Router (<code class="literal">router</code>)</h2></div></div></div><p>The Dynamic Router support allows for routing messages to <span class="strong"><strong>named destinations</strong></span> based on the evaluation of a SpEL
expression or Groovy Script.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_spel_based_routing" href="#_spel_based_routing"></a>4.12.1&nbsp;SpEL-based Routing</h3></div></div></div><p>The expression evaluates against the message and returns either a channel name, or the key to a map of channel names.</p><p>For more information, please see the "Routers and the Spring Expression Language (SpEL)" subsection in the Spring
Integration Reference manual
<a class="link" href="http://docs.spring.io/spring-integration/reference/html/messaging-routing-chapter.html#router-namespace" target="_top">Configuring (Generic) Router section</a>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_groovy_based_routing" href="#_groovy_based_routing"></a>4.12.2&nbsp;Groovy-based Routing</h3></div></div></div><p>Instead of SpEL expressions, Groovy scripts can also be used. Let&#8217;s create a Groovy script in the file system at
"file:/my/path/router.groovy", or "classpath:/my/path/router.groovy" :</p><pre class="programlisting">println(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"Groovy processing payload '"</span> + payload + <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"'"</span>);
<span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">if</span> (payload.contains(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">'a'</span>)) {
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">return</span> <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"foo"</span>
}
<span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">else</span> {
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">return</span> <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"bar"</span>
}</pre><p>If you want to pass variable values to your script, you can statically bind values using the <span class="emphasis"><em>variables</em></span> option or
optionally pass the path to a properties file containing the bindings using the <span class="emphasis"><em>propertiesLocation</em></span> option.
All properties in the file will be made available to the script as variables. You may specify both <span class="emphasis"><em>variables</em></span> and
<span class="emphasis"><em>propertiesLocation</em></span>, in which case any duplicate values provided as <span class="emphasis"><em>variables</em></span> override values provided in
<span class="emphasis"><em>propertiesLocation</em></span>.
Note that <span class="emphasis"><em>payload</em></span> and <span class="emphasis"><em>headers</em></span> are implicitly bound to give you access to the data contained in a message.</p><p>For more information, see the Spring Integration Reference manual
<a class="link" href="http://docs.spring.io/spring-integration/reference/html/messaging-endpoints-chapter.html#groovy" target="_top">Groovy Support</a>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_25" href="#_options_25"></a>4.12.3&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>router</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">destinations</span></dt><dd>comma-delimited destinations mapped from evaluation results <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">defaultOutputChannel</span></dt><dd>Where to route messages where the channel cannot be resolved <span class="strong"><strong>(String, default: <code class="literal">nullChannel</code>)</strong></span></dd><dt><span class="term">expression</span></dt><dd>a SpEL expression used to determine the destination <span class="strong"><strong>(String, default: <code class="literal">headers['routeTo']</code>)</strong></span></dd><dt><span class="term">propertiesLocation</span></dt><dd>the path of a properties file containing custom script variable bindings <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">refreshDelay</span></dt><dd>how often to check (in milliseconds) whether the script (if present) has changed; -1 for never <span class="strong"><strong>(long, default: <code class="literal">60000</code>)</strong></span></dd><dt><span class="term">script</span></dt><dd>reference to a script used to process messages <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">destinationMappings</span></dt><dd>Destination mappings as a new line delimited string of name-value pairs, e.g. 'foo=bar\n baz=car'. <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Since this is a dynamic router, destinations are created as needed; therefore, by default the <code class="literal">defaultOutputChannel</code>
and <code class="literal">resolutionRequired</code> will only be used if the <code class="literal">Binder</code> has some problem binding to the destination.</p></td></tr></table></div><p>You can restrict the creation of dynamic bindings using the <code class="literal">spring.cloud.stream.dynamicDestinations</code> property.
By default, all resolved destinations will be bound dynamically; if this property has a comma-delimited list of
destination names, only those will be bound.
Messages that resolve to a destination that is not in this list will be routed to the <code class="literal">defaultOutputChannel</code>, which
must also appear in the list.</p><p><code class="literal">destinationMappings</code> are used to map the evaluation results to an actual destination name.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-sink-tcp" href="#spring-cloud-stream-modules-sink-tcp"></a>4.13&nbsp;TCP Sink</h2></div></div></div><p>The TCP Sink provides for outbound messaging over TCP; messages sent to the sink can have <code class="literal">String</code> or <code class="literal">byte[]</code> payloads.</p><p>TCP is a streaming protocol and some mechanism is needed to frame messages on the wire. A number of encoders are
available, the default being 'CRLF'.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_26" href="#_options_26"></a>4.13.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>tcp</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">charset</span></dt><dd>the charset used when converting from String to bytes <span class="strong"><strong>(String, default: <code class="literal">UTF-8</code>)</strong></span></dd><dt><span class="term">close</span></dt><dd>whether to close the socket after each message <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">encoder</span></dt><dd>the encoder to use when sending messages <span class="strong"><strong>(Encoding, default: <code class="literal">CRLF</code>, possible values: <code class="literal">CRLF,LF,NULL,STXETX,RAW,L1,L2,L4</code>)</strong></span></dd><dt><span class="term">host</span></dt><dd>the remote host to connect to <span class="strong"><strong>(String, default: <code class="literal">localhost</code>)</strong></span></dd><dt><span class="term">nio</span></dt><dd>whether or not to use NIO <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">port</span></dt><dd>the port on the remote host to connect to <span class="strong"><strong>(int, default: <code class="literal">1234</code>)</strong></span></dd><dt><span class="term">reverseLookup</span></dt><dd>perform a reverse DNS lookup on the remote IP Address <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">socketTimeout</span></dt><dd>the timeout (ms) before closing the socket when no data is received <span class="strong"><strong>(int, default: <code class="literal">120000</code>)</strong></span></dd><dt><span class="term">useDirectBuffers</span></dt><dd>whether or not to use direct buffers <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_available_encoders" href="#_available_encoders"></a>4.13.2&nbsp;Available Encoders</h3></div></div></div><div class="variablelist"><p class="title"><b>Text Data</b></p><dl class="variablelist"><dt><span class="term">CRLF (default)</span></dt><dd>text terminated by carriage return (0x0d) followed by line feed (0x0a)</dd><dt><span class="term">LF</span></dt><dd>text terminated by line feed (0x0a)</dd><dt><span class="term">NULL</span></dt><dd>text terminated by a null byte (0x00)</dd><dt><span class="term">STXETX</span></dt><dd>text preceded by an STX (0x02) and terminated by an ETX (0x03)</dd></dl></div><div class="variablelist"><p class="title"><b>Text and Binary Data</b></p><dl class="variablelist"><dt><span class="term">RAW</span></dt><dd>no structure - the client indicates a complete message by closing the socket</dd><dt><span class="term">L1</span></dt><dd>data preceded by a one byte (unsigned) length field (supports up to 255 bytes)</dd><dt><span class="term">L2</span></dt><dd>data preceded by a two byte (unsigned) length field (up to 2<sup>16</sup>-1 bytes)</dd><dt><span class="term">L4</span></dt><dd>data preceded by a four byte (signed) length field (up to 2<sup>31</sup>-1 bytes)</dd></dl></div></div></div></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="_appendices" href="#_appendices"></a>Part&nbsp;III.&nbsp;Appendices</h1></div></div></div><div class="appendix"><div class="titlepage"><div><div><h2 class="title"><a name="building" href="#building"></a>Appendix&nbsp;A.&nbsp;Building</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_basic_compile_and_test" href="#_basic_compile_and_test"></a>A.1&nbsp;Basic Compile and Test</h2></div></div></div><p>To build the source you will need to install JDK 1.7.</p><p>The build uses the Maven wrapper so you don&#8217;t have to install a specific
version of Maven.  To enable the tests for Redis you should run the server
before bulding.  See below for more information on how run Redis.</p><p>The main build command is</p><pre class="screen">$ ./mvnw clean install</pre><p>You can also add '-DskipTests' if you like, to avoid running the tests.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>You can also install Maven (&gt;=3.3.3) yourself and run the <code class="literal">mvn</code> command
in place of <code class="literal">./mvnw</code> in the examples below. If you do that you also
might need to add <code class="literal">-P spring</code> if your local Maven settings do not
contain repository declarations for spring pre-release artifacts.</p></td></tr></table></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Be aware that you might need to increase the amount of memory
available to Maven by setting a <code class="literal">MAVEN_OPTS</code> environment variable with
a value like <code class="literal">-Xmx512m -XX:MaxPermSize=128m</code>. We try to cover this in
the <code class="literal">.mvn</code> configuration, so if you find you have to do it to make a
build succeed, please raise a ticket to get the settings added to
source control.</p></td></tr></table></div><p>The projects that require middleware generally include a
<code class="literal">docker-compose.yml</code>, so consider using
<a class="link" href="http://compose.docker.io/" target="_top">Docker Compose</a> to run the middeware servers
in Docker containers. See the README in the
<a class="link" href="https://github.com/spring-cloud-samples/scripts" target="_top">scripts demo
repository</a> for specific instructions about the common cases of mongo,
rabbit and redis.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_documentation" href="#_documentation"></a>A.2&nbsp;Documentation</h2></div></div></div><p>There is a "full" profile that will generate documentation.  You can build just the documentation by executing</p><pre class="screen">$ ./mvnw package -DskipTests=true -P full -pl spring-cloud-stream-modules-docs -am</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_working_with_the_code" href="#_working_with_the_code"></a>A.3&nbsp;Working with the code</h2></div></div></div><p>If you don&#8217;t have an IDE preference we would recommend that you use
<a class="link" href="http://www.springsource.com/developer/sts" target="_top">Spring Tools Suite</a> or
<a class="link" href="http://eclipse.org" target="_top">Eclipse</a> when working with the code. We use the
<a class="link" href="http://eclipse.org/m2e/" target="_top">m2eclipe</a> eclipse plugin for maven support. Other IDEs and tools
should also work without issue.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_importing_into_eclipse_with_m2eclipse" href="#_importing_into_eclipse_with_m2eclipse"></a>A.3.1&nbsp;Importing into eclipse with m2eclipse</h3></div></div></div><p>We recommend the <a class="link" href="http://eclipse.org/m2e/" target="_top">m2eclipe</a> eclipse plugin when working with
eclipse. If you don&#8217;t already have m2eclipse installed it is available from the "eclipse
marketplace".</p><p>Unfortunately m2e does not yet support Maven 3.3, so once the projects
are imported into Eclipse you will also need to tell m2eclipse to use
the <code class="literal">.settings.xml</code> file for the projects.  If you do not do this you
may see many different errors related to the POMs in the
projects.  Open your Eclipse preferences, expand the Maven
preferences, and select User Settings.  In the User Settings field
click Browse and navigate to the Spring Cloud project you imported
selecting the <code class="literal">.settings.xml</code> file in that project.  Click Apply and
then OK to save the preference changes.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Alternatively you can copy the repository settings from <a class="link" href="https://github.com/spring-cloud/spring-cloud-build/blob/master/.settings.xml" target="_top"><code class="literal">.settings.xml</code></a> into your own <code class="literal">~/.m2/settings.xml</code>.</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_importing_into_eclipse_without_m2eclipse" href="#_importing_into_eclipse_without_m2eclipse"></a>A.3.2&nbsp;Importing into eclipse without m2eclipse</h3></div></div></div><p>If you prefer not to use m2eclipse you can generate eclipse project metadata using the
following command:</p><pre class="screen">$ ./mvnw eclipse:eclipse</pre><p>The generated eclipse projects can be imported by selecting <code class="literal">import existing projects</code>
from the <code class="literal">file</code> menu.</p></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="contributing" href="#contributing"></a>5.&nbsp;Contributing</h2></div></div></div><p>Spring Cloud is released under the non-restrictive Apache 2.0 license,
and follows a very standard Github development process, using Github
tracker for issues and merging pull requests into master. If you want
to contribute even something trivial please do not hesitate, but
follow the guidelines below.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_sign_the_contributor_license_agreement" href="#_sign_the_contributor_license_agreement"></a>5.1&nbsp;Sign the Contributor License Agreement</h2></div></div></div><p>Before we accept a non-trivial patch or pull request we will need you to sign the
<a class="link" href="https://support.springsource.com/spring_committer_signup" target="_top">contributor&#8217;s agreement</a>.
Signing the contributor&#8217;s agreement does not grant anyone commit rights to the main
repository, but it does mean that we can accept your contributions, and you will get an
author credit if we do.  Active contributors might be asked to join the core team, and
given the ability to merge pull requests.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_code_conventions_and_housekeeping" href="#_code_conventions_and_housekeeping"></a>5.2&nbsp;Code Conventions and Housekeeping</h2></div></div></div><p>None of these is essential for a pull request, but they will all help.  They can also be
added after the original pull request but before a merge.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Use the Spring Framework code format conventions. If you use Eclipse
you can import formatter settings using the
<code class="literal">eclipse-code-formatter.xml</code> file from the
<a class="link" href="https://github.com/spring-cloud/build/tree/master/eclipse-coding-conventions.xml" target="_top">Spring
Cloud Build</a> project. If using IntelliJ, you can use the
<a class="link" href="http://plugins.jetbrains.com/plugin/6546" target="_top">Eclipse Code Formatter
Plugin</a> to import the same file.</li><li class="listitem">Make sure all new <code class="literal">.java</code> files to have a simple Javadoc class comment with at least an
<code class="literal">@author</code> tag identifying you, and preferably at least a paragraph on what the class is
for.</li><li class="listitem">Add the ASF license header comment to all new <code class="literal">.java</code> files (copy from existing files
in the project)</li><li class="listitem">Add yourself as an <code class="literal">@author</code> to the .java files that you modify substantially (more
than cosmetic changes).</li><li class="listitem">Add some Javadocs and, if you change the namespace, some XSD doc elements.</li><li class="listitem">A few unit tests would help a lot as well&#8201;&#8212;&#8201;someone has to do it.</li><li class="listitem">If no-one else is using your branch, please rebase it against the current master (or
other target branch in the main project).</li><li class="listitem">When writing a commit message please follow <a class="link" href="http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html" target="_top">these conventions</a>,
if you are fixing an existing issue please add <code class="literal">Fixes gh-XXXX</code> at the end of the commit
message (where XXXX is the issue number).</li></ul></div></div></div></div></div></body></html>