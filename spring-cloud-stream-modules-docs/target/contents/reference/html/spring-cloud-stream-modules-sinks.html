<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>4.&nbsp;Sinks</title><link rel="stylesheet" type="text/css" href="css/manual-multipage.css"><meta name="generator" content="DocBook XSL Stylesheets V1.78.1"><link rel="home" href="index.html" title="Spring Cloud Stream Reference Guide"><link rel="up" href="_modules.html" title="Part&nbsp;II.&nbsp;Modules"><link rel="prev" href="spring-cloud-stream-modules-processors.html" title="3.&nbsp;Processors"><link rel="next" href="_appendices.html" title="Part&nbsp;III.&nbsp;Appendices"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">4.&nbsp;Sinks</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="spring-cloud-stream-modules-processors.html">Prev</a>&nbsp;</td><th width="60%" align="center">Part&nbsp;II.&nbsp;Modules</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="_appendices.html">Next</a></td></tr></table><hr></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-stream-modules-sinks" href="#spring-cloud-stream-modules-sinks"></a>4.&nbsp;Sinks</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-cassandra" href="#spring-cloud-stream-modules-cassandra"></a>4.1&nbsp;Cassandra (<code class="literal">cassandra</code>)</h2></div></div></div><p>The Cassandra sink writes into a Cassandra table.  Here is a simple example</p><pre class="literallayout">dataflow:&gt;stream create cassandrastream --definition "http --server.port=8888 --spring.cloud.stream.bindings.output.contentType='application/json' | cassandra --ingestQuery='insert into book (id, isbn, title, author) values (uuid(), ?, ?, ?)' --spring.cassandra.keyspace=clouddata" --deploy</pre><p>Create a keyspace and a <code class="literal">book</code> table in Cassandra using:</p><pre class="programlisting">CREATE KEYSPACE clouddata WITH REPLICATION = { 'class' : 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1' } AND DURABLE_WRITES = true;
USE clouddata;
CREATE TABLE book  (
    id          uuid PRIMARY KEY,
    isbn        text,
    author      text,
    title       text
);</pre><p>You can then send data to this stream via</p><pre class="literallayout">dataflow:&gt;http post --contentType 'application/json' --data '{"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}' --target http://localhost:8888/
&gt; POST (application/json;charset=UTF-8) http://localhost:8888/ {"isbn": "1599869772", "title": "The Art of War", "author": "Sun Tzu"}
&gt; 202 ACCEPTED</pre><p>and see the table contents using the CQL</p><pre class="literallayout">SELECT * FROM clouddata.book;</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_17" href="#_options_17"></a>4.1.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>cassandra</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">compressionType</span></dt><dd>the compression to use for the transport <span class="strong"><strong>(CompressionType, default: <code class="literal">NONE</code>, possible values: <code class="literal">NONE,SNAPPY</code>)</strong></span></dd><dt><span class="term">consistencyLevel</span></dt><dd>the consistencyLevel option of WriteOptions <span class="strong"><strong>(ConsistencyLevel, no default, possible values: <code class="literal">ANY,ONE,TWO,THREE,QUOROM,LOCAL_QUOROM,EACH_QUOROM,ALL,LOCAL_ONE,SERIAL,LOCAL_SERIAL</code>)</strong></span></dd><dt><span class="term">spring.cassandra.contactPoints</span></dt><dd>the comma-delimited string of the hosts to connect to Cassandra <span class="strong"><strong>(String, default: <code class="literal">localhost</code>)</strong></span></dd><dt><span class="term">entityBasePackages</span></dt><dd>the base packages to scan for entities annotated with Table annotations <span class="strong"><strong>(String[], default: <code class="literal">[]</code>)</strong></span></dd><dt><span class="term">ingestQuery</span></dt><dd>the ingest Cassandra query <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">spring.cassandra.initScript</span></dt><dd>the path to file with CQL scripts (delimited by ';') to initialize keyspace schema <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">spring.cassandra.keyspace</span></dt><dd>the keyspace name to connect to <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">metricsEnabled</span></dt><dd>enable/disable metrics collection for the created cluster <span class="strong"><strong>(boolean, default: <code class="literal">true</code>)</strong></span></dd><dt><span class="term">spring.cassandra.password</span></dt><dd>the password for connection <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">spring.cassandra.port</span></dt><dd>the port to use to connect to the Cassandra host <span class="strong"><strong>(int, default: <code class="literal">9042</code>)</strong></span></dd><dt><span class="term">queryType</span></dt><dd>the queryType for Cassandra Sink <span class="strong"><strong>(Type, default: <code class="literal">INSERT</code>, possible values: <code class="literal">INSERT,UPDATE,DELETE,STATEMENT</code>)</strong></span></dd><dt><span class="term">retryPolicy</span></dt><dd>the retryPolicy  option of WriteOptions <span class="strong"><strong>(RetryPolicy, no default, possible values: <code class="literal">DEFAULT,DOWNGRADING_CONSISTENCY,FALLTHROUGH,LOGGING</code>)</strong></span></dd><dt><span class="term">statementExpression</span></dt><dd>the expression in Cassandra query DSL style <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">spring.cassandra.schemaAction</span></dt><dd>schema action to perform <span class="strong"><strong>(SchemaAction, default: <code class="literal">NONE</code>, possible values: <code class="literal">CREATE,NONE,RECREATE,RECREATE_DROP_UNUSED</code>)</strong></span></dd><dt><span class="term">ttl</span></dt><dd>the time-to-live option of WriteOptions <span class="strong"><strong>(int, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">spring.cassandra.username</span></dt><dd>the username for connection <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-counter" href="#spring-cloud-stream-modules-counter"></a>4.2&nbsp;Counter (<code class="literal">counter</code>)</h2></div></div></div><p>A simple module that counts messages received, using Spring Boot metrics abstraction.</p><p>The <span class="strong"><strong>counter</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">name</span></dt><dd>The name of the counter to increment. <span class="strong"><strong>(String, default: <code class="literal">counts</code>)</strong></span></dd><dt><span class="term">nameExpression</span></dt><dd>A SpEL expression (against the incoming Message) to derive the name of the counter to increment. <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">store</span></dt><dd>The name of a store used to store the counter. <span class="strong"><strong>(String, default: <code class="literal">memory</code>, possible values: <code class="literal">memory</code>, <code class="literal">redis</code>)</strong></span></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-field-value-counter" href="#spring-cloud-stream-modules-field-value-counter"></a>4.3&nbsp;Field Value Counter (<code class="literal">field-value-counter</code>)</h2></div></div></div><p>A field value counter is a Metric used for counting occurrences of unique values for a named field in a message payload. Spring Cloud Data Flow supports the following payload types out of the box:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">POJO (Java bean)</li><li class="listitem">Tuple</li><li class="listitem">JSON String</li></ul></div><p>For example suppose a message source produces a payload with a field named <span class="emphasis"><em>user</em></span> :</p><pre class="programlisting"><span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">class</span> Foo {
   String user;
   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">public</span> Foo(String user) {
       <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">this</span>.user = user;
   }
}</pre><p>If the stream source produces messages with the following objects:</p><pre class="programlisting">   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">new</span> Foo(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"fred"</span>)
   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">new</span> Foo(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"sue"</span>)
   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">new</span> Foo(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"dave"</span>)
   <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">new</span> Foo(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"sue"</span>)</pre><p>The field value counter on the field <span class="emphasis"><em>user</em></span> will contain:</p><pre class="literallayout">fred:1, sue:2, dave:1</pre><p>Multi-value fields are also supported. For example, if a field contains a list, each value will be counted once:</p><pre class="literallayout">users:["dave","fred","sue"]
users:["sue","jon"]</pre><p>The field value counter on the field <span class="emphasis"><em>users</em></span> will contain:</p><pre class="literallayout">dave:1, fred:1, sue:2, jon:1</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_18" href="#_options_18"></a>4.3.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>field-value-counter</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">fieldName</span></dt><dd>the name of the field for which values are counted <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">name</span></dt><dd>the name of the metric to contribute to (will be created if necessary) <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">nameExpression</span></dt><dd>a SpEL expression to compute the name of the metric to contribute to <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-file-sink" href="#spring-cloud-stream-modules-file-sink"></a>4.4&nbsp;File (<code class="literal">file</code>)</h2></div></div></div><p>This module writes each message it receives to a file.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_19" href="#_options_19"></a>4.4.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>file</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">binary</span></dt><dd>if false, will append a newline character at the end of each line <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">charset</span></dt><dd>the charset to use when writing a String payload <span class="strong"><strong>(String, default: <code class="literal">UTF-8</code>)</strong></span></dd><dt><span class="term">dir</span></dt><dd>the directory in which files will be created <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">dirExpression</span></dt><dd>spring expression used to define directory name <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">mode</span></dt><dd>what to do if the file already exists <span class="strong"><strong>(Mode, default: <code class="literal">APPEND</code>, possible values: <code class="literal">APPEND,REPLACE,FAIL,IGNORE</code>)</strong></span></dd><dt><span class="term">name</span></dt><dd>filename pattern to use <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">nameExpression</span></dt><dd>spring expression used to define filename <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">suffix</span></dt><dd>filename extension to use <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ftp-sink" href="#ftp-sink"></a>4.5&nbsp;FTP Sink (<code class="literal">ftp</code>)</h2></div></div></div><p>FTP sink is a simple option to push files to an FTP server from incoming messages.</p><p>It uses an <code class="literal">ftp-outbound-adapter</code>, therefore incoming messages could be either a <code class="literal">java.io.File</code> object, a <code class="literal">String</code> (content of the file)
or an array of <code class="literal">bytes</code> (file content as well).</p><p>To use this sink, you need a username and a password to login.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>By default Spring Integration will use <code class="literal">o.s.i.file.DefaultFileNameGenerator</code> if none is specified. <code class="literal">DefaultFileNameGenerator</code> will determine the file name
based on the value of the <code class="literal">file_name</code> header (if it exists) in the <code class="literal">MessageHeaders</code>, or if the payload of the <code class="literal">Message</code> is already a <code class="literal">java.io.File</code>, then it will
use the original name of that file.</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-gemfire-sink" href="#spring-cloud-stream-modules-gemfire-sink"></a>4.6&nbsp;Gemfire (<code class="literal">gemfire</code>)</h2></div></div></div><p>A sink module that allows one to write message payloads to a Gemfire server.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_20" href="#_options_20"></a>4.6.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>gemfire</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">hostAddresses</span></dt><dd>a comma separated list of [host]:[port] specifying either locator or server addresses for the client connection pool <span class="strong"><strong>(String, <code class="literal">localhost:10334</code>)</strong></span></dd><dt><span class="term">keyExpression</span></dt><dd>a SpEL expression which is evaluated to create a cache key <span class="strong"><strong>(String, default: <code class="literal">the value is currently the message payload'</code>)</strong></span></dd><dt><span class="term">port</span></dt><dd>port of the cache server or locator (if useLocator=true). May be a comma delimited list <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">regionName</span></dt><dd>name of the region to use when storing data <span class="strong"><strong>(String, default: <code class="literal">${spring.application.name}</code>)</strong></span></dd><dt><span class="term">connectType</span></dt><dd>'server' or 'locator' <span class="strong"><strong>(String, default: <code class="literal">locator</code>)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-hdfs" href="#spring-cloud-stream-modules-hdfs"></a>4.7&nbsp;Hadoop (HDFS) (<code class="literal">hdfs</code>)</h2></div></div></div><p>If you do not have Hadoop installed, you can install Hadoop as described in our <a class="link" href="Hadoop-Installation.xml#installing-hadoop" target="_top">separate guide</a>.</p><p>Once Hadoop is up and running, you can then use the <code class="literal">hdfs</code> sink when creating a <a class="link" href="Streams.xml#streams" target="_top">stream</a></p><pre class="literallayout">dataflow:&gt; stream create --name myhdfsstream1 --definition "time | hdfs" --deploy</pre><p>In the above example, we&#8217;ve scheduled <code class="literal">time</code> source to automatically send ticks to <code class="literal">hdfs</code> once in every second. If you wait a little while for data to accumuluate you can then list can then list the files in the hadoop filesystem using the shell&#8217;s built in hadoop fs commands.  Before making any access to HDFS in the shell you first need to configure the shell to point to your name node.  This is done using the <code class="literal">hadoop config</code> command.</p><pre class="literallayout">dataflow:&gt;hadoop config fs --namenode hdfs://localhost:8020</pre><p>In this example the hdfs protocol is used but you may also use the webhdfs protocol.  Listing the contents in the output directory (named by default after the stream name) is done by issuing the following command.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream1
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup          0 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt.tmp</pre><p>While the file is being written to it will have the <code class="literal">tmp</code> suffix.  When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the <code class="literal">tmp</code> suffix.  There are several options to control the in use file file naming options.  These are <code class="literal">--inUsePrefix</code> and <code class="literal">--inUseSuffix</code> set the file name prefix and suffix respectfully.</p><p>When you destroy a stream</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream1</pre><p>and list the stream directory again, in use file suffix doesn&#8217;t exist anymore.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream1
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup        380 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt</pre><p>To list the list the contents of a file directly from a shell execute the hadoop cat command.</p><pre class="literallayout">dataflow:&gt; hadoop fs cat /xd/myhdfsstream1/myhdfsstream1-0.txt
2013-12-18 18:10:07
2013-12-18 18:10:08
2013-12-18 18:10:09
...</pre><p>In the above examples we didn&#8217;t yet go through why the file was written in a specific directory and why it was named in this specific way. Default location of a file is defined as <code class="literal">/xd/&lt;stream name&gt;/&lt;stream name&gt;-&lt;rolling part&gt;.txt</code>. These can be changed using options <code class="literal">--directory</code> and <code class="literal">--fileName</code> respectively. Example is shown below.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream2 --definition "time | hdfs --directory=/xd/tmp --fileName=data" --deploy
dataflow:&gt;stream destroy --name myhdfsstream2
dataflow:&gt;hadoop fs ls /xd/tmp
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup        120 2013-12-18 18:31 /xd/tmp/data-0.txt</pre><p>It is also possible to control the size of a files written into HDFS. The <code class="literal">--rollover</code> option can be used to control when file currently being written is rolled over and a new file opened by providing the rollover size in bytes, kilobytes, megatypes, gigabytes, and terabytes.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream3 --definition "time | hdfs --rollover=100" --deploy
dataflow:&gt;stream destroy --name myhdfsstream3
dataflow:&gt;hadoop fs ls /xd/myhdfsstream3
Found 3 items
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-0.txt
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-1.txt
-rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-2.txt</pre><p>Shortcuts to specify sizes other than bytes are written as <code class="literal">--rollover=64M</code>, <code class="literal">--rollover=512G</code> or <code class="literal">--rollover=1T</code>.</p><p>The stream can also be compressed during the write operation. Example of this is shown below.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream4 --definition "time | hdfs --codec=gzip" --deploy
dataflow:&gt;stream destroy --name myhdfsstream4
dataflow:&gt;hadoop fs ls /xd/myhdfsstream4
Found 1 items
-rw-r--r--   3 jvalkealahti supergroup         80 2013-12-18 18:48 /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip</pre><p>From a native os shell we can use hadoop&#8217;s fs commands and pipe data into gunzip.</p><pre class="literallayout"># bin/hadoop fs -cat /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip | gunzip
2013-12-18 18:48:10
2013-12-18 18:48:11
...</pre><p>Often a stream of data may not have a high enough rate to roll over files frequently, leaving the file in an opened state.  This prevents users from reading a consistent set of data when running mapreduce jobs.  While one can alleviate this problem by using a small rollover value, a better way is to use the <code class="literal">idleTimeout</code>  option that will automatically close the file if there was no writes during the specified period of time.   This feature is also useful in cases where burst of data is written into a stream and you&#8217;d like that data to become visible in HDFS.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The <code class="literal">idleTimeout</code> value should not exceed the timeout values set on the Hadoop cluster. These are typically configured using the <code class="literal">dfs.socket.timeout</code> and/or <code class="literal">dfs.datanode.socket.write.timeout</code> properties in the <code class="literal">hdfs-site.xml</code> configuration file.</p></td></tr></table></div><pre class="literallayout">dataflow:&gt; stream create --name myhdfsstream5 --definition "http --server.port=8000 | hdfs --rollover=20 --idleTimeout=10000" --deploy</pre><p>In the above example we changed a source to <code class="literal">http</code> order to control what we write into a <code class="literal">hdfs</code> sink. We defined a small rollover size and a timeout of 10 seconds. Now we can simply post data into this stream via source end point using a below command.</p><pre class="literallayout">dataflow:&gt; http post --target http://localhost:8000 --data "hello"</pre><p>If we repeat the command very quickly and then wait for the timeout we should be able to see that some files are closed before rollover size was met and some were simply rolled because of a rollover size.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls /xd/myhdfsstream5
Found 4 items
-rw-r--r--   3 jvalkealahti supergroup         12 2013-12-18 19:02 /xd/myhdfsstream5/myhdfsstream5-0.txt
-rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-1.txt
-rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-2.txt
-rw-r--r--   3 jvalkealahti supergroup         18 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-3.txt</pre><p>Files can be automatically partitioned using a <code class="literal">partitionPath</code> expression. If we create a stream with <code class="literal">idleTimeout</code> and <code class="literal">partitionPath</code> with simple format <code class="literal">yyyy/MM/dd/HH/mm</code> we should see writes ending into its own files within every minute boundary.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream6 --definition "time|hdfs --idleTimeout=10000 --partitionPath=dateFormat('yyyy/MM/dd/HH/mm')" --deploy</pre><p>Let a stream run for a short period of time and list files.</p><pre class="literallayout">dataflow:&gt;hadoop fs ls --recursive true --dir /xd/myhdfsstream6
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:42 /xd/myhdfsstream6/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42
-rw-r--r--   3 jvalkealahti supergroup        140 2014-05-28 09:43 /xd/myhdfsstream6/2014/05/28/09/42/myhdfsstream6-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43
-rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:44 /xd/myhdfsstream6/2014/05/28/09/43/myhdfsstream6-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44
-rw-r--r--   3 jvalkealahti supergroup       1200 2014-05-28 09:45 /xd/myhdfsstream6/2014/05/28/09/44/myhdfsstream6-0.txt</pre><p>Partitioning can also be based on defined lists. In a below example we simulate feeding data by using a <code class="literal">time</code> and a <code class="literal">transform</code> elements. Data passed to <code class="literal">hdfs</code> sink has a content <code class="literal">APP0:foobar</code>, <code class="literal">APP1:foobar</code>, <code class="literal">APP2:foobar</code> or <code class="literal">APP3:foobar</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream7 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*3)+':foobar'\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),list(payload.split(':')[0],{{'0TO1','APP0','APP1'},{'2TO3','APP2','APP3'}}))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream7
Destroyed stream 'myhdfsstream7'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list
-rw-r--r--   3 jvalkealahti supergroup        108 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list
-rw-r--r--   3 jvalkealahti supergroup        180 2014-05-28 19:24 /xd/myhdfsstream7/2014/05/28/19/2TO3_list/myhdfsstream7-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream7/2014/05/28/19/0TO1_list/myhdfsstream7-0.txt
APP1:foobar
APP1:foobar
APP0:foobar
APP0:foobar
APP1:foobar</pre><p>Partitioning can also be based on defined ranges. In a below example we simulate feeding data by using a <code class="literal">time</code> and a <code class="literal">transform</code> elements. Data passed to <code class="literal">hdfs</code> sink has a content ranging from <code class="literal">APP0</code> to <code class="literal">APP15</code>. We simple parse the number part and use it to do a partition with ranges <code class="literal">{3,5,10}</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream8 --definition "time | transform --expression=\"'APP'+T(Math).round(T(Math).random()*15)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH'),range(T(Integer).parseInt(payload.substring(3)),{3,5,10}))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream8
Destroyed stream 'myhdfsstream8'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range
-rw-r--r--   3 jvalkealahti supergroup         16 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range
-rw-r--r--   3 jvalkealahti supergroup         35 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range
-rw-r--r--   3 jvalkealahti supergroup          5 2014-05-28 19:34 /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/3_range/myhdfsstream8-0.txt
APP3
APP3
APP1
APP0
APP1
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/5_range/myhdfsstream8-0.txt
APP4
dataflow:&gt;hadoop fs cat /xd/myhdfsstream8/2014/05/28/19/10_range/myhdfsstream8-0.txt
APP6
APP15
APP7</pre><p>Partition using a <code class="literal">dateFormat</code> can be based on content itself. This is a good use case if old log files needs to be processed where partitioning should happen based on timestamp of a log entry. We create a fake log data with a simple date string ranging from <code class="literal">1970-01-10</code> to <code class="literal">1970-01-13</code>.</p><pre class="literallayout">dataflow:&gt;stream create --name myhdfsstream9 --definition "time | transform --expression=\"'1970-01-'+1+T(Math).round(T(Math).random()*3)\" | hdfs --idleTimeout=10000 --partitionPath=path(dateFormat('yyyy/MM/dd/HH',payload,'yyyy-MM-DD'))" --deploy</pre><p>Let the stream run few seconds, destroy it and check what got written in those partitioned files. If you see the partition paths, those are based on year 1970, not present year.</p><pre class="literallayout">dataflow:&gt;stream destroy --name myhdfsstream9
Destroyed stream 'myhdfsstream9'
dataflow:&gt;hadoop fs ls --recursive true --dir /xd
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/10
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00
-rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/11
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00
-rw-r--r--   3 jvalkealahti supergroup         99 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/11/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/12
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00
-rw-r--r--   3 jvalkealahti supergroup         44 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/12/00/myhdfsstream9-0.txt
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:56 /xd/myhdfsstream9/1970/01/13
drwxr-xr-x   - jvalkealahti supergroup          0 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00
-rw-r--r--   3 jvalkealahti supergroup         55 2014-05-28 19:57 /xd/myhdfsstream9/1970/01/13/00/myhdfsstream9-0.txt
dataflow:&gt;hadoop fs cat /xd/myhdfsstream9/1970/01/10/00/myhdfsstream9-0.txt
1970-01-10
1970-01-10
1970-01-10
1970-01-10</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_21" href="#_options_21"></a>4.7.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>hdfs</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">closeTimeout</span></dt><dd>timeout in ms, regardless of activity, after which file will be automatically closed <span class="strong"><strong>(long, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">codec</span></dt><dd>compression codec alias name (gzip, snappy, bzip2, lzo, or slzo) <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">directory</span></dt><dd>where to output the files in the Hadoop FileSystem <span class="strong"><strong>(String, default: <code class="literal">/tmp/hdfs-sink</code>)</strong></span></dd><dt><span class="term">fileExtension</span></dt><dd>the base filename extension to use for the created files <span class="strong"><strong>(String, default: <code class="literal">txt</code>)</strong></span></dd><dt><span class="term">fileName</span></dt><dd>the base filename to use for the created files <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name&gt;</code>)</strong></span></dd><dt><span class="term">fileOpenAttempts</span></dt><dd>maximum number of file open attempts to find a path <span class="strong"><strong>(int, default: <code class="literal">10</code>)</strong></span></dd><dt><span class="term">fileUuid</span></dt><dd>whether file name should contain uuid <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">fsUri</span></dt><dd>the URI to use to access the Hadoop FileSystem <span class="strong"><strong>(String, default: <code class="literal">${spring.hadoop.fsUri}</code>)</strong></span></dd><dt><span class="term">idleTimeout</span></dt><dd>inactivity timeout in ms after which file will be automatically closed <span class="strong"><strong>(long, default: <code class="literal">0</code>)</strong></span></dd><dt><span class="term">inUsePrefix</span></dt><dd>prefix for files currently being written <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">inUseSuffix</span></dt><dd>suffix for files currently being written <span class="strong"><strong>(String, default: <code class="literal">.tmp</code>)</strong></span></dd><dt><span class="term">overwrite</span></dt><dd>whether writer is allowed to overwrite files in Hadoop FileSystem <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">partitionPath</span></dt><dd>a SpEL expression defining the partition path <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">rollover</span></dt><dd>threshold in bytes when file will be automatically rolled over <span class="strong"><strong>(String, default: <code class="literal">1G</code>)</strong></span></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>In the context of the <code class="literal">fileOpenAttempts</code> option, attempt is either one rollover request or failed stream open request for a path (if another writer came up with a same path and already opened it).</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_partition_path_expression" href="#_partition_path_expression"></a>4.7.2&nbsp;Partition Path Expression</h3></div></div></div><p>SpEL expression is evaluated against a Spring Messaging <code class="literal">Message</code> passed internally into a HDFS writer. This allows expression to use <code class="literal">headers</code> and <code class="literal">payload</code> from that message. While you could do a custom processing within a stream and add custom headers, <code class="literal">timestamp</code> is always going to be there. Data to be written is then available in a <code class="literal">payload</code>.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="_accessing_properties" href="#_accessing_properties"></a>Accessing Properties</h4></div></div></div><p>Using a <code class="literal">payload</code> simply returns whatever is currently being written. Access to headers is via <code class="literal">headers</code> property. Any other property is automatically resolved from headers if found. For example <code class="literal">headers.timestamp</code> is equivalent to <code class="literal">timestamp</code>.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="_custom_methods" href="#_custom_methods"></a>Custom Methods</h4></div></div></div><p>Addition to a normal SpEL functionality, few custom methods has been added to make it easier to build partition paths. These custom methods can be used to work with a normal partition concepts like <code class="literal">date formatting</code>, <code class="literal">lists</code>, <code class="literal">ranges</code> and <code class="literal">hashes</code>.</p><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_path" href="#_path"></a>path</h5></div></div></div><pre class="programlisting">path(String... paths)</pre><p>Concatenates paths together with a delimiter <code class="literal">/</code>. This method can be used to make the expression less verbose than using a native SpEL functionality to combine path parts together. To create a path <code class="literal">part1/part2</code>, expression <code class="literal">'part1' + '/' + 'part2'</code> is equivalent to <code class="literal">path('part1','part2')</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">paths</span></dt><dd>Any number of path parts</dd></dl></div><p><b>Return Value.&nbsp;</b>Concatenated value of paths delimited with <code class="literal">/</code>.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_dateformat" href="#_dateformat"></a>dateFormat</h5></div></div></div><pre class="programlisting">dateFormat(String pattern)
dateFormat(String pattern, Long epoch)
dateFormat(String pattern, Date date)
dateFormat(String pattern, String datestring)
dateFormat(String pattern, String datestring, String dateformat)</pre><p>Creates a path using date formatting. Internally this method delegates into <code class="literal">SimpleDateFormat</code> and needs a <code class="literal">Date</code> and a <code class="literal">pattern</code>. On default if no parameter used for conversion is given, <code class="literal">timestamp</code> is expected. Effectively <code class="literal">dateFormat('yyyy')</code> equals to <code class="literal">dateFormat('yyyy', timestamp)</code> or <code class="literal">dateFormat('yyyy', headers.timestamp)</code>.</p><p>Method signature with three parameters can be used to create a custom <code class="literal">Date</code> object which is then passed to <code class="literal">SimpleDateFormat</code> conversion using a <code class="literal">dateformat</code> pattern. This is useful in use cases where partition should be based on a date or time string found from a payload content itself. Default <code class="literal">dateformat</code> pattern if omitted is <code class="literal">yyyy-MM-dd</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">pattern</span></dt><dd>Pattern compatible with <code class="literal">SimpleDateFormat</code> to produce a final output.</dd><dt><span class="term">epoch</span></dt><dd>Timestamp as <code class="literal">Long</code> which is converted into a <code class="literal">Date</code>.</dd><dt><span class="term">date</span></dt><dd>A <code class="literal">Date</code> to be formatted.</dd><dt><span class="term">dateformat</span></dt><dd>Secondary pattern to convert <code class="literal">datestring</code> into a <code class="literal">Date</code>.</dd><dt><span class="term">datestring</span></dt><dd><code class="literal">Date</code> as a <code class="literal">String</code></dd></dl></div><p><b>Return Value.&nbsp;</b>A path part representation which can be a simple file or directory name or a directory structure.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_list" href="#_list"></a>list</h5></div></div></div><pre class="programlisting">list(Object source, List&lt;List&lt;Object&gt;&gt; lists)</pre><p>Creates a partition path part by matching a <code class="literal">source</code> against a lists denoted by <code class="literal">lists</code>.</p><p>Lets assume that data is being written and it&#8217;s possible to extrace an <code class="literal">appid</code> either from headers or payload. We can automatically do a list based partition by using a partition method <code class="literal">list(headers.appid,{{'1TO3','APP1','APP2','APP3'},{'4TO6','APP4','APP5','APP6'}})</code>. This method would create three partitions, <code class="literal">1TO3_list</code>, <code class="literal">4TO6_list</code> and <code class="literal">list</code>. Latter is used if no match is found from partition lists passed to <code class="literal">lists</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> to be matched against <code class="literal">lists</code>.</dd><dt><span class="term">lists</span></dt><dd>A definition of list of lists.</dd></dl></div><p><b>Return Value.&nbsp;</b>A path part prefixed with a matched key i.e. <code class="literal">XXX_list</code> or <code class="literal">list</code> if no match.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_range" href="#_range"></a>range</h5></div></div></div><pre class="programlisting">range(Object source, List&lt;Object&gt; list)</pre><p>Creates a partition path part by matching a <code class="literal">source</code> against a list denoted by <code class="literal">list</code> using a simple binary search.</p><p>The partition method takes a <code class="literal">source</code> as first argument and <code class="literal">list</code> as a second argument. Behind the scenes this is using jvm&#8217;s <code class="literal">binarySearch</code> which works on an <code class="literal">Object</code> level so we can pass in anything. Remember that meaningful range match only works if passed in <code class="literal">Object</code> and types in list are of same type like <code class="literal">Integer</code>. Range is defined by a binarySearch itself so mostly it is to match against an upper bound except the last range in a list. Having a list of <code class="literal">{1000,3000,5000}</code> means that everything above 3000 will be matched with 5000. If that is an issue then simply adding <code class="literal">Integer.MAX_VALUE</code> as last range would overflow everything above 5000 into a new partition. Created partitions would then be <code class="literal">1000_range</code>, <code class="literal">3000_range</code> and <code class="literal">5000_range</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> to be matched against <code class="literal">list</code>.</dd><dt><span class="term">list</span></dt><dd>A definition of list.</dd></dl></div><p><b>Return Value.&nbsp;</b>A path part prefixed with a matched key i.e. <code class="literal">XXX_range</code>.</p></div><div class="section"><div class="titlepage"><div><div><h5 class="title"><a name="_hash" href="#_hash"></a>hash</h5></div></div></div><pre class="programlisting">hash(Object source, int bucketcount)</pre><p>Creates a partition path part by calculating hashkey using <code class="literal">source`s</code> <code class="literal">hashCode</code> and <code class="literal">bucketcount</code>. Using a partition method <code class="literal">hash(timestamp,2)</code> would then create partitions named <code class="literal">0_hash</code>, <code class="literal">1_hash</code> and <code class="literal">2_hash</code>. Number suffixed with <code class="literal">_hash</code> is simply calculated using <code class="literal">Object.hashCode() % bucketcount</code>.</p><div class="variablelist"><p class="title"><b>Parameters</b></p><dl class="variablelist"><dt><span class="term">source</span></dt><dd>An <code class="literal">Object</code> which <code class="literal">hashCode</code> will be used.</dd><dt><span class="term">bucketcount</span></dt><dd>A number of buckets</dd></dl></div><p><b>Return Value.&nbsp;</b>A path part prefixed with a hash key i.e. <code class="literal">XXX_hash</code>.</p></div></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-jdbc" href="#spring-cloud-stream-modules-jdbc"></a>4.8&nbsp;JDBC (<code class="literal">jdbc</code>)</h2></div></div></div><p>A module that writes its incoming payload to an RDBMS using JDBC.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_22" href="#_options_22"></a>4.8.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>jdbc</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">expression</span></dt><dd>a SpEL expression used to transform messages <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">tableName</span></dt><dd>String <span class="strong"><strong>(String, default: <code class="literal">&lt;stream name</code>)</strong></span></dd><dt><span class="term">columns</span></dt><dd>the names of the columns that shall receive data, as a set of column[:SpEL] mappings, also used at initialization time to issue the DDL <span class="strong"><strong>(String, default: <code class="literal">payload</code>)</strong></span></dd><dt><span class="term">initialize</span></dt><dd>String <span class="strong"><strong>(Boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">batchSize</span></dt><dd>String <span class="strong"><strong>(long, default: <code class="literal">10000</code>)</strong></span></dd></dl></div><p>The module also uses Spring Boot&#8217;s <a class="link" href="http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-sql.html#boot-features-configure-datasource" target="_top">DataSource support</a>
for configuring the database connection, so properties like <code class="literal">spring.datasource.url</code> <span class="emphasis"><em>etc.</em></span> apply.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-log" href="#spring-cloud-stream-modules-log"></a>4.9&nbsp;Log (<code class="literal">log</code>)</h2></div></div></div><p>Probably the simplest option for a sink is just to log the data. The <code class="literal">log</code> sink uses the application logger to output the data for inspection. The log level is set to <code class="literal">WARN</code> and the logger name is created from the stream name. To create a stream using a <code class="literal">log</code> sink you would use a command like</p><pre class="literallayout">dataflow:&gt; stream create --name mylogstream --definition "http --server.port=8000 | log" --deploy</pre><p>You can then try adding some data. We&#8217;ve used the <code class="literal">http</code> source on port 8000 here, so run the following command to send a message</p><pre class="literallayout">dataflow:&gt; http post --target http://localhost:8000 --data "hello"</pre><p>and you should see the following output in the Spring Cloud Data Flow console.</p><pre class="literallayout">13/06/07 16:12:18 INFO Received: hello</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-sink-rabbitmq" href="#spring-cloud-stream-modules-sink-rabbitmq"></a>4.10&nbsp;RabbitMQ</h2></div></div></div><p>The "rabbit" sink enables outbound messaging over RabbitMQ.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_23" href="#_options_23"></a>4.10.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>rabbit</strong></span> sink has the following options:</p><p>(See the Spring Boot documentation for RabbitMQ connection properties)</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">converterBeanName</span></dt><dd>the bean name of the message converter <span class="strong"><strong>(String, default: none)</strong></span></dd><dt><span class="term">persistentDeliveryMode</span></dt><dd>the default delivery mode, true for persistent <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">exchange</span></dt><dd>the Exchange on the RabbitMQ broker to which messages should be sent <span class="strong"><strong>(String, default: <code class="literal">""</code>)</strong></span></dd><dt><span class="term">exchangeExpression</span></dt><dd>a SpEL expression that evaluates to the Exchange on the RabbitMQ broker to which messages
should be sent; overrides `exchange` <span class="strong"><strong>(String, default: ``)</strong></span></dd><dt><span class="term">mappedRequestHeaders</span></dt><dd>request message header names to be propagated to RabbitMQ, to limit to the set of standard headers plus `bar`, use `STANDARD_REQUEST_HEADERS,bar` <span class="strong"><strong>(String, default: <code class="literal">*</code>)</strong></span></dd><dt><span class="term">routingKey</span></dt><dd>the routing key to be passed with the message, as a SpEL expression <span class="strong"><strong>(String, default: none)</strong></span></dd><dt><span class="term">routingKeyExpression</span></dt><dd>an expression that evaluates to the routing key to be passed with the message, as a SpEL expression; overrides `routingKey` <span class="strong"><strong>(String, default: none)</strong></span></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>By default, the message converter is a <code class="literal">SimpleMessageConverter</code> which handles <code class="literal">byte[]</code>, <code class="literal">String</code> and
<code class="literal">java.io.Serializable</code>.
A well-known bean name <code class="literal">jsonConverter</code> will configure a <code class="literal">Jackson2JsonMessageConverter</code> instead.
In addition, a custom converter bean can be added to the context and referenced by the converterBeanName property.</p></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-redis" href="#spring-cloud-stream-modules-redis"></a>4.11&nbsp;Redis (<code class="literal">redis</code>)</h2></div></div></div><p>The Redis sink can be used to ingest data into redis store. You can choose <code class="literal">queue</code>, <code class="literal">topic</code> or <code class="literal">key</code> with selcted
collection type to point to a specific data store.</p><p>For example,</p><pre class="screen">dataflow:&gt;stream create store-into-redis --definition "http | redis --queue=myList" --deploy
dataflow:&gt;Created and deployed new stream 'store-into-redis'</pre><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_24" href="#_options_24"></a>4.11.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>redis</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">topicExpression</span></dt><dd>a SpEL expression to use for topic <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">queueExpression</span></dt><dd>a SpEL expression to use for queue <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">keyExpression</span></dt><dd>a SpEL expression to use for keyExpression <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">key</span></dt><dd>name for the key <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">queue</span></dt><dd>name for the queue <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">topic</span></dt><dd>name for the topic <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-sink-router" href="#spring-cloud-stream-modules-sink-router"></a>4.12&nbsp;Dynamic Router (<code class="literal">router</code>)</h2></div></div></div><p>The Dynamic Router support allows for routing messages to <span class="strong"><strong>named destinations</strong></span> based on the evaluation of a SpEL
expression or Groovy Script.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_spel_based_routing" href="#_spel_based_routing"></a>4.12.1&nbsp;SpEL-based Routing</h3></div></div></div><p>The expression evaluates against the message and returns either a channel name, or the key to a map of channel names.</p><p>For more information, please see the "Routers and the Spring Expression Language (SpEL)" subsection in the Spring
Integration Reference manual
<a class="link" href="http://docs.spring.io/spring-integration/reference/html/messaging-routing-chapter.html#router-namespace" target="_top">Configuring (Generic) Router section</a>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_groovy_based_routing" href="#_groovy_based_routing"></a>4.12.2&nbsp;Groovy-based Routing</h3></div></div></div><p>Instead of SpEL expressions, Groovy scripts can also be used. Let&#8217;s create a Groovy script in the file system at
"file:/my/path/router.groovy", or "classpath:/my/path/router.groovy" :</p><pre class="programlisting">println(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"Groovy processing payload '"</span> + payload + <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"'"</span>);
<span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">if</span> (payload.contains(<span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">'a'</span>)) {
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">return</span> <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"foo"</span>
}
<span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">else</span> {
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">return</span> <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"bar"</span>
}</pre><p>If you want to pass variable values to your script, you can statically bind values using the <span class="emphasis"><em>variables</em></span> option or
optionally pass the path to a properties file containing the bindings using the <span class="emphasis"><em>propertiesLocation</em></span> option.
All properties in the file will be made available to the script as variables. You may specify both <span class="emphasis"><em>variables</em></span> and
<span class="emphasis"><em>propertiesLocation</em></span>, in which case any duplicate values provided as <span class="emphasis"><em>variables</em></span> override values provided in
<span class="emphasis"><em>propertiesLocation</em></span>.
Note that <span class="emphasis"><em>payload</em></span> and <span class="emphasis"><em>headers</em></span> are implicitly bound to give you access to the data contained in a message.</p><p>For more information, see the Spring Integration Reference manual
<a class="link" href="http://docs.spring.io/spring-integration/reference/html/messaging-endpoints-chapter.html#groovy" target="_top">Groovy Support</a>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_25" href="#_options_25"></a>4.12.3&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>router</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">destinations</span></dt><dd>comma-delimited destinations mapped from evaluation results <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">defaultOutputChannel</span></dt><dd>Where to route messages where the channel cannot be resolved <span class="strong"><strong>(String, default: <code class="literal">nullChannel</code>)</strong></span></dd><dt><span class="term">expression</span></dt><dd>a SpEL expression used to determine the destination <span class="strong"><strong>(String, default: <code class="literal">headers['routeTo']</code>)</strong></span></dd><dt><span class="term">propertiesLocation</span></dt><dd>the path of a properties file containing custom script variable bindings <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">refreshDelay</span></dt><dd>how often to check (in milliseconds) whether the script (if present) has changed; -1 for never <span class="strong"><strong>(long, default: <code class="literal">60000</code>)</strong></span></dd><dt><span class="term">script</span></dt><dd>reference to a script used to process messages <span class="strong"><strong>(String, no default)</strong></span></dd><dt><span class="term">destinationMappings</span></dt><dd>Destination mappings as a new line delimited string of name-value pairs, e.g. 'foo=bar\n baz=car'. <span class="strong"><strong>(String, no default)</strong></span></dd></dl></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Since this is a dynamic router, destinations are created as needed; therefore, by default the <code class="literal">defaultOutputChannel</code>
and <code class="literal">resolutionRequired</code> will only be used if the <code class="literal">Binder</code> has some problem binding to the destination.</p></td></tr></table></div><p>You can restrict the creation of dynamic bindings using the <code class="literal">spring.cloud.stream.dynamicDestinations</code> property.
By default, all resolved destinations will be bound dynamically; if this property has a comma-delimited list of
destination names, only those will be bound.
Messages that resolve to a destination that is not in this list will be routed to the <code class="literal">defaultOutputChannel</code>, which
must also appear in the list.</p><p><code class="literal">destinationMappings</code> are used to map the evaluation results to an actual destination name.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-stream-modules-sink-tcp" href="#spring-cloud-stream-modules-sink-tcp"></a>4.13&nbsp;TCP Sink</h2></div></div></div><p>The TCP Sink provides for outbound messaging over TCP; messages sent to the sink can have <code class="literal">String</code> or <code class="literal">byte[]</code> payloads.</p><p>TCP is a streaming protocol and some mechanism is needed to frame messages on the wire. A number of encoders are
available, the default being 'CRLF'.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_options_26" href="#_options_26"></a>4.13.1&nbsp;Options</h3></div></div></div><p>The <span class="strong"><strong>tcp</strong></span> sink has the following options:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">charset</span></dt><dd>the charset used when converting from String to bytes <span class="strong"><strong>(String, default: <code class="literal">UTF-8</code>)</strong></span></dd><dt><span class="term">close</span></dt><dd>whether to close the socket after each message <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">encoder</span></dt><dd>the encoder to use when sending messages <span class="strong"><strong>(Encoding, default: <code class="literal">CRLF</code>, possible values: <code class="literal">CRLF,LF,NULL,STXETX,RAW,L1,L2,L4</code>)</strong></span></dd><dt><span class="term">host</span></dt><dd>the remote host to connect to <span class="strong"><strong>(String, default: <code class="literal">localhost</code>)</strong></span></dd><dt><span class="term">nio</span></dt><dd>whether or not to use NIO <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">port</span></dt><dd>the port on the remote host to connect to <span class="strong"><strong>(int, default: <code class="literal">1234</code>)</strong></span></dd><dt><span class="term">reverseLookup</span></dt><dd>perform a reverse DNS lookup on the remote IP Address <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd><dt><span class="term">socketTimeout</span></dt><dd>the timeout (ms) before closing the socket when no data is received <span class="strong"><strong>(int, default: <code class="literal">120000</code>)</strong></span></dd><dt><span class="term">useDirectBuffers</span></dt><dd>whether or not to use direct buffers <span class="strong"><strong>(boolean, default: <code class="literal">false</code>)</strong></span></dd></dl></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_available_encoders" href="#_available_encoders"></a>4.13.2&nbsp;Available Encoders</h3></div></div></div><div class="variablelist"><p class="title"><b>Text Data</b></p><dl class="variablelist"><dt><span class="term">CRLF (default)</span></dt><dd>text terminated by carriage return (0x0d) followed by line feed (0x0a)</dd><dt><span class="term">LF</span></dt><dd>text terminated by line feed (0x0a)</dd><dt><span class="term">NULL</span></dt><dd>text terminated by a null byte (0x00)</dd><dt><span class="term">STXETX</span></dt><dd>text preceded by an STX (0x02) and terminated by an ETX (0x03)</dd></dl></div><div class="variablelist"><p class="title"><b>Text and Binary Data</b></p><dl class="variablelist"><dt><span class="term">RAW</span></dt><dd>no structure - the client indicates a complete message by closing the socket</dd><dt><span class="term">L1</span></dt><dd>data preceded by a one byte (unsigned) length field (supports up to 255 bytes)</dd><dt><span class="term">L2</span></dt><dd>data preceded by a two byte (unsigned) length field (up to 2<sup>16</sup>-1 bytes)</dd><dt><span class="term">L4</span></dt><dd>data preceded by a four byte (signed) length field (up to 2<sup>31</sup>-1 bytes)</dd></dl></div></div></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="spring-cloud-stream-modules-processors.html">Prev</a>&nbsp;</td><td width="20%" align="center"><a accesskey="u" href="_modules.html">Up</a></td><td width="40%" align="right">&nbsp;<a accesskey="n" href="_appendices.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">3.&nbsp;Processors&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;Part&nbsp;III.&nbsp;Appendices</td></tr></table></div></body></html>